{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Plot model performance\n",
    "\n",
    "Visualize performance on different models on data `2023-09-05` from the wandb API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import itertools\n",
    "import sys\n",
    "sys.path.append(str(pathlib.Path().absolute().parent))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.stats import wilcoxon\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "\n",
    "from utils import BodeColorPalette, get_runs_as_list\n",
    "from src.util.definitions import DATA_ROOT\n",
    "bode_palette = BodeColorPalette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "sns.set_theme(context=\"paper\", \n",
    "              style=\"white\", \n",
    "              font_scale=1, #0.7,\n",
    "              rc={\"savefig.transparent\": True, \n",
    "                  \"axes.grid\": False, \n",
    "                  \"axes.spines.bottom\": True,\n",
    "                  \"axes.spines.left\": False,\n",
    "                  \"axes.spines.right\": False,\n",
    "                  \"axes.spines.top\": False,\n",
    "                  \"font.family\":'sans-serif',\n",
    "                  \"font.sans-serif\":[\"Helvetica\", \"Arial\"],\n",
    "                  \"xtick.major.pad\": 0.0,\n",
    "                  \"xtick.minor.pad\": 0.0,\n",
    "                  \"ytick.major.pad\": 0.0,\n",
    "                  \"ytick.minor.pad\": 0.0,\n",
    "                  \"axes.labelweight\": \"bold\",\n",
    "                  \"axes.labelpad\": 2.5,  # standard is 4.0\n",
    "                  \"axes.xmargin\": .05,\n",
    "                 }, \n",
    "             )\n",
    "\n",
    "# more settings for all plots\n",
    "errorbar = \"se\"  # standard error of the mean\n",
    "errwidth = .9\n",
    "errcolor = \"black\"\n",
    "capsize = .1  # size of the end of the errorbar\n",
    "linewidth = 1.  # width of the outline of barplot\n",
    "\n",
    "palette = [\n",
    "    bode_palette.blues[0], \n",
    "    bode_palette.oranges[0],\n",
    "    bode_palette.blues[2],\n",
    "    bode_palette.oranges[2],\n",
    "    bode_palette.blues[3],\n",
    "    bode_palette.oranges[3]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get data from wandb API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_list, config_list, tag_list, name_list  = get_runs_as_list(filters={\"$or\":\n",
    "                                                             [{\"jobType\": \"training\"},\n",
    "                                                              {\"jobType\": \"hparam_best\"}\n",
    "                                                             ]})\n",
    "df_all = pd.json_normalize(config_list).merge(pd.json_normalize(summary_list), left_index=True, right_index=True)\n",
    "df_all[\"tags\"] = tag_list\n",
    "df_all[\"run_id\"] = name_list\n",
    "df_all[\"run_group\"] = [s.rsplit(\"_\", maxsplit=1)[0] for s in name_list]\n",
    "df_all[\"Model+Features\"] = df_all[\"name\"] + \"/\" + df_all[\"decoder.global_features\"].str.join(\"+\").str.replace(\"None\", \"CGR\")\n",
    "df_all[\"fold\"] = df_all[\"run_id\"].str[-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check available experiments by split\n",
    "for tag, row in df_all.groupby(\"tags\")[[\"experiment_id\"]].agg(set).iterrows():\n",
    "    print(tag, \"-->\", row[\"experiment_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dir where we will save plots\n",
    "analysis_dir = pathlib.Path(\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "# choose the task (0D, 1D, or 2D)\n",
    "tag = \"0D\"\n",
    "# choose the colorscheme\n",
    "fill_color = '0.4'  # grey\n",
    "stroke_color = '0.1'  # almost black\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "    \"JG1100\",\n",
    "    \"JG1160\",\n",
    "    #\"JG1135\",  # GCN is very low compared to others\n",
    "    \"JG1185\",\n",
    "    \"JG1106\",\n",
    "    \"JG1109\",\n",
    "    \"JG1128\",\n",
    "    \"JG1131\",\n",
    "    \"JG1115\",\n",
    "    \"JG1116\",\n",
    "    \"JG1117\",\n",
    "    \n",
    "]\n",
    "\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(experiment_ids) & df_all['tags'].apply(lambda x: tag in x)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(experiment_ids, itertools.count()))\n",
    "df_plot = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict))\n",
    "ticklabels = df_plot[\"Model+Features\"].unique().tolist()\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(2.8,2))\n",
    "sns.barplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "    x=\"Model+Features\",\n",
    "    y=metric,\n",
    "    errorbar=errorbar,\n",
    "    errwidth=errwidth,\n",
    "    errcolor=errcolor,\n",
    "    capsize=capsize,\n",
    "    color=fill_color,\n",
    "    edgecolor=stroke_color,\n",
    "    linewidth=linewidth,\n",
    "    alpha=.8,\n",
    "    width=.7,\n",
    ")\n",
    "\n",
    "sns.stripplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "              x=\"Model+Features\",\n",
    "              y=metric,\n",
    "              dodge=True,\n",
    "              edgecolor=stroke_color,\n",
    "              color=fill_color,\n",
    "              linewidth=.5,\n",
    "              legend=False,\n",
    "              marker=\"o\",\n",
    "              size=2.5,\n",
    "              alpha=.8\n",
    "             )\n",
    "\n",
    "ax.axhline(0.555, ls=\"--\", color=\"black\", linewidth=.7)  # chance level for val/0D\n",
    "\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.xaxis.set_tick_params(labelrotation=90)\n",
    "ax.set_ylim((0, 1))\n",
    "ax.xaxis.set_ticklabels(df_plot[\"Model+Features\"].drop_duplicates().str.replace(\"LogisticRegression\", \"LogReg\"))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_barplot.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_barplot.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the same with a boxplot\n",
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "# choose the task (0D, 1D, or 2D)\n",
    "tag = \"0D\"\n",
    "# choose the colorscheme\n",
    "fill_color = '0.6'  # grey\n",
    "stroke_color = '0.1'  # almost black\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "    \"JG1100\",\n",
    "    \"JG1160\",\n",
    "    #\"JG1135\",  # GCN is very low compared to others\n",
    "    \"JG1185\",\n",
    "    \"JG1106\",\n",
    "    \"JG1109\",\n",
    "    \"JG1128\",\n",
    "    \"JG1131\",\n",
    "    \"JG1115\",\n",
    "    \"JG1116\",\n",
    "    \"JG1117\",\n",
    "    \n",
    "]\n",
    "\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(experiment_ids) & df_all['tags'].apply(lambda x: tag in x)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(experiment_ids, itertools.count()))\n",
    "df_plot = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict))\n",
    "ticklabels = df_plot[\"Model+Features\"].unique().tolist()\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(5.6,4))\n",
    "sns.boxplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "    x=\"Model+Features\",\n",
    "    y=metric,\n",
    "    boxprops={\"edgecolor\": stroke_color},\n",
    "    medianprops={\"color\": stroke_color},\n",
    "    color=fill_color,\n",
    "    linewidth=linewidth,\n",
    "    fliersize=2.5,\n",
    "    saturation=.8,\n",
    "    width=.7,\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.set_xlabel(None)\n",
    "ax.xaxis.set_tick_params(labelrotation=90)\n",
    "ax.set_ylim((0.915, 0.975))\n",
    "ax.xaxis.set_ticklabels(df_plot[\"Model+Features\"].drop_duplicates().str.replace(\"LogisticRegression\", \"LogReg\"))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_boxplot.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_boxplot.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the same with a data points only\n",
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "# choose the task (0D, 1D, or 2D)\n",
    "tag = \"0D\"\n",
    "# choose the colorscheme\n",
    "fill_color = '0.4'  # grey\n",
    "stroke_color = '0.1'  # almost black\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "    \"JG1100\",\n",
    "    \"JG1160\",\n",
    "    #\"JG1135\",  # GCN is very low compared to others\n",
    "    \"JG1185\",\n",
    "    \"JG1106\",\n",
    "    \"JG1109\",\n",
    "    \"JG1128\",\n",
    "    \"JG1131\",\n",
    "    \"JG1115\",\n",
    "    \"JG1116\",\n",
    "    \"JG1117\",\n",
    "    \n",
    "]\n",
    "\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(experiment_ids) & df_all['tags'].apply(lambda x: tag in x)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(experiment_ids, itertools.count()))\n",
    "df_plot = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict))\n",
    "ticklabels = df_plot[\"Model+Features\"].unique().tolist()\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(5.6,4))\n",
    "\n",
    "sns.stripplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "              x=\"Model+Features\",\n",
    "              y=metric,\n",
    "              dodge=True,\n",
    "              jitter=.15,\n",
    "              edgecolor=stroke_color,\n",
    "              color=fill_color,\n",
    "              linewidth=.5,\n",
    "              legend=False,\n",
    "              marker=\"o\",\n",
    "              size=3.5,\n",
    "              alpha=.8\n",
    "             )\n",
    "\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.xaxis.set_tick_params(labelrotation=90)\n",
    "ax.set_ylim((0.915, 0.975))\n",
    "ax.xaxis.set_ticklabels(df_plot[\"Model+Features\"].drop_duplicates().str.replace(\"LogisticRegression\", \"LogReg\"))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_scatter.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_scatter.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.groupby(\"Model+Features\")[\"val/avgPrecision_macro\"].agg([np.mean, np.std]).sort_values(by=\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the best model significantly better than the rest?\n",
    "best = \"FFN/OHE\"\n",
    "x = df_plot.loc[df_plot[\"Model+Features\"] == best].sort_values(by=\"run_id\")[\"val/avgPrecision_macro\"].to_numpy()\n",
    "for i in df_plot[\"Model+Features\"].drop_duplicates():\n",
    "    if i != best:\n",
    "        y = df_plot.loc[df_plot[\"Model+Features\"] == i].sort_values(by=\"run_id\")[\"val/avgPrecision_macro\"].to_numpy()\n",
    "        print(i, \":\\t\", wilcoxon(x, y, alternative=\"two-sided\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot[\"Model+Features\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "# choose the task (0D, 1D, or 2D)\n",
    "tag = \"1D\"\n",
    "# choose the colorscheme\n",
    "fill_color = (189/256, 87/256, 213/256)\n",
    "stroke_color = (140/256, 67/256, 158/256)\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "    \"JG1101\",\n",
    "    #\"JG1104\",  # GCN is very low compared to others\n",
    "    \"JG1121\",\n",
    "    \"JG1123\",\n",
    "    \"JG1125\",\n",
    "    \"JG1126\",\n",
    "    \"JG1186\",\n",
    "    \"JG1129\",\n",
    "    \"JG1132\",\n",
    "    \"JG1118\",\n",
    "\n",
    "]\n",
    "\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(experiment_ids) & df_all['tags'].apply(lambda x: tag in x)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(experiment_ids, itertools.count()))\n",
    "df_plot = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict))\n",
    "ticklabels = df_plot[\"Model+Features\"].unique().tolist()\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(2.8,2))\n",
    "sns.barplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "    x=\"Model+Features\",\n",
    "    y=metric,\n",
    "    errorbar=errorbar,\n",
    "    errwidth=errwidth,\n",
    "    errcolor=errcolor,\n",
    "    capsize=capsize,\n",
    "    color=fill_color,\n",
    "    edgecolor=stroke_color,\n",
    "    linewidth=linewidth,\n",
    "    alpha=.8,\n",
    "    width=.7,\n",
    ")\n",
    "\n",
    "sns.stripplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "              x=\"Model+Features\",\n",
    "              y=metric,\n",
    "              dodge=True,\n",
    "              edgecolor=stroke_color,\n",
    "              color=fill_color,\n",
    "              linewidth=.5,\n",
    "              legend=False,\n",
    "              marker=\"o\",\n",
    "              size=2.5,\n",
    "              alpha=.8\n",
    "             )\n",
    "\n",
    "ax.axhline(0.537, ls=\"--\", color=\"black\", linewidth=.7)  # chance level for val/1D\n",
    "\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.xaxis.set_tick_params(labelrotation=90)\n",
    "ax.set_ylim((0, 1))\n",
    "ax.xaxis.set_ticklabels(df_plot[\"Model+Features\"].drop_duplicates().str.replace(\"LogisticRegression\", \"LogReg\"))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_barplot.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_barplot.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a boxplot\n",
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "# choose the task (0D, 1D, or 2D)\n",
    "tag = \"1D\"\n",
    "# choose the colorscheme\n",
    "fill_color = (189/256, 87/256, 213/256)\n",
    "stroke_color = (140/256, 67/256, 158/256)\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "    \"JG1101\",\n",
    "    #\"JG1104\",  # GCN is very low compared to others\n",
    "    \"JG1121\",\n",
    "    \"JG1123\",\n",
    "    \"JG1125\",\n",
    "    \"JG1126\",\n",
    "    \"JG1186\",\n",
    "    \"JG1129\",\n",
    "    \"JG1132\",\n",
    "    \"JG1118\",\n",
    "\n",
    "]\n",
    "\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(experiment_ids) & df_all['tags'].apply(lambda x: tag in x)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(experiment_ids, itertools.count()))\n",
    "df_plot = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict))\n",
    "ticklabels = df_plot[\"Model+Features\"].unique().tolist()\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(5.6, 4))\n",
    "sns.boxplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "    x=\"Model+Features\",\n",
    "    y=metric,\n",
    "    color=fill_color,\n",
    "    boxprops={\"edgecolor\": stroke_color},\n",
    "    medianprops={\"color\": stroke_color},\n",
    "    linewidth=linewidth,\n",
    "    fliersize=2.5,\n",
    "    saturation=.8,\n",
    "    width=.7,\n",
    ")\n",
    "\n",
    "ax.axhline(0.537, ls=\"--\", color=\"black\", linewidth=.7)  # chance level for val/1D\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.xaxis.set_tick_params(labelrotation=90)\n",
    "ax.set_ylim((0.55, 0.95))\n",
    "ax.xaxis.set_ticklabels(df_plot[\"Model+Features\"].drop_duplicates().str.replace(\"LogisticRegression\", \"LogReg\"))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_boxplot.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_boxplot.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a boxplot\n",
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "# choose the task (0D, 1D, or 2D)\n",
    "tag = \"1D\"\n",
    "# choose the colorscheme\n",
    "fill_color = (189/256, 87/256, 213/256)\n",
    "stroke_color = (140/256, 67/256, 158/256)\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "    \"JG1101\",\n",
    "    #\"JG1104\",  # GCN is very low compared to others\n",
    "    \"JG1121\",\n",
    "    \"JG1123\",\n",
    "    \"JG1125\",\n",
    "    \"JG1126\",\n",
    "    \"JG1186\",\n",
    "    \"JG1129\",\n",
    "    \"JG1132\",\n",
    "    \"JG1118\",\n",
    "\n",
    "]\n",
    "\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(experiment_ids) & df_all['tags'].apply(lambda x: tag in x)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(experiment_ids, itertools.count()))\n",
    "df_plot = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict))\n",
    "ticklabels = df_plot[\"Model+Features\"].unique().tolist()\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(5.6, 4))\n",
    "sns.stripplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "              x=\"Model+Features\",\n",
    "              y=metric,\n",
    "              dodge=True,\n",
    "              jitter=.15,\n",
    "              edgecolor=stroke_color,\n",
    "              color=fill_color,\n",
    "              linewidth=.5,\n",
    "              legend=False,\n",
    "              marker=\"o\",\n",
    "              size=3.5,\n",
    "              alpha=.8\n",
    "             )\n",
    "\n",
    "ax.axhline(0.537, ls=\"--\", color=\"black\", linewidth=.7)  # chance level for val/1D\n",
    "\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.xaxis.set_tick_params(labelrotation=90)\n",
    "ax.set_ylim((0.55, 0.95))\n",
    "ax.xaxis.set_ticklabels(df_plot[\"Model+Features\"].drop_duplicates().str.replace(\"LogisticRegression\", \"LogReg\"))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_scatter.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_scatter.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.groupby(\"Model+Features\")[\"val/avgPrecision_macro\"].agg([np.mean, np.std]).sort_values(by=\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the best model significantly better than the rest?\n",
    "best = \"D-MPNN/CGR\"\n",
    "x = df_plot.loc[df_plot[\"Model+Features\"] == best].sort_values(by=\"run_id\")[\"val/avgPrecision_macro\"].to_numpy()\n",
    "for i in df_plot[\"Model+Features\"].drop_duplicates():\n",
    "    if i != best:\n",
    "        y = df_plot.loc[df_plot[\"Model+Features\"] == i].sort_values(by=\"run_id\")[\"val/avgPrecision_macro\"].to_numpy()\n",
    "        print(i, \":\\t\", wilcoxon(x, y, alternative=\"two-sided\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "# choose the task (0D, 1D, or 2D)\n",
    "tag = \"2D\"\n",
    "# choose the colorscheme\n",
    "fill_color = (\n",
    "    135/256,\n",
    "    186/256,\n",
    "    112/256,\n",
    ")\n",
    "\n",
    "stroke_color = (92/256, 124/256, 76/256)\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "    \"JG1102\",\n",
    "    #\"JG1105\",  # GCN is very low compared to others\n",
    "    \"JG1122\",\n",
    "    \"JG1124\",\n",
    "    \"JG1127\",\n",
    "    \"JG1112\",\n",
    "    \"JG1130\",\n",
    "    \"JG1133\",\n",
    "    \"JG1119\",\n",
    "\n",
    "]\n",
    "\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(experiment_ids) & df_all['tags'].apply(lambda x: tag in x)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(experiment_ids, itertools.count()))\n",
    "df_plot = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict))\n",
    "ticklabels = df_plot[\"Model+Features\"].unique().tolist()\n",
    "\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(2.8,2))\n",
    "sns.barplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "    x=\"Model+Features\",\n",
    "    y=metric,\n",
    "    errorbar=errorbar,\n",
    "    errwidth=errwidth,\n",
    "    errcolor=errcolor,\n",
    "    capsize=capsize,\n",
    "    color=fill_color,\n",
    "    edgecolor=stroke_color,\n",
    "    linewidth=linewidth,\n",
    "    alpha=.8,\n",
    "    width=.7,\n",
    ")\n",
    "\n",
    "sns.stripplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "              x=\"Model+Features\",\n",
    "              y=metric,\n",
    "              dodge=True,\n",
    "              edgecolor=stroke_color,\n",
    "              color=fill_color,\n",
    "              linewidth=.5,\n",
    "              legend=False,\n",
    "              marker=\"o\",\n",
    "              size=2.5,\n",
    "              alpha=.8\n",
    "             )\n",
    "\n",
    "ax.axhline(0.569, ls=\"--\", color=\"black\", linewidth=.7)  # chance level for val/2D\n",
    "\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.xaxis.set_tick_params(labelrotation=90)\n",
    "ax.set_ylim((0, 1))\n",
    "ax.xaxis.set_ticklabels(df_plot[\"Model+Features\"].drop_duplicates().str.replace(\"LogisticRegression\", \"LogReg\"))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_barplot.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_barplot.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make boxplot\n",
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "# choose the task (0D, 1D, or 2D)\n",
    "tag = \"2D\"\n",
    "# choose the colorscheme\n",
    "fill_color = (\n",
    "    135/256,\n",
    "    186/256,\n",
    "    112/256,\n",
    ")\n",
    "\n",
    "stroke_color = (92/256, 124/256, 76/256)\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "    \"JG1102\",\n",
    "    #\"JG1105\",  # GCN is very low compared to others\n",
    "    \"JG1122\",\n",
    "    \"JG1124\",\n",
    "    \"JG1127\",\n",
    "    \"JG1112\",\n",
    "    \"JG1130\",\n",
    "    \"JG1133\",\n",
    "    \"JG1119\",\n",
    "\n",
    "]\n",
    "\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(experiment_ids) & df_all['tags'].apply(lambda x: tag in x)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(experiment_ids, itertools.count()))\n",
    "df_plot = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict))\n",
    "ticklabels = df_plot[\"Model+Features\"].unique().tolist()\n",
    "\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(5.6, 4))\n",
    "sns.boxplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "    x=\"Model+Features\",\n",
    "    y=metric,\n",
    "    color=fill_color,\n",
    "    boxprops={\"edgecolor\": stroke_color},\n",
    "    medianprops={\"color\": stroke_color},\n",
    "    linewidth=linewidth,\n",
    "    fliersize=2.5,\n",
    "    saturation=.8,\n",
    "    width=.7,\n",
    ")\n",
    "\n",
    "ax.axhline(0.569, ls=\"--\", color=\"black\", linewidth=.7)  # chance level for val/2D\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.xaxis.set_tick_params(labelrotation=90)\n",
    "ax.set_ylim((0.45, 1))\n",
    "ax.xaxis.set_ticklabels(df_plot[\"Model+Features\"].drop_duplicates().str.replace(\"LogisticRegression\", \"LogReg\"))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_boxplot.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_boxplot.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make boxplot\n",
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "# choose the task (0D, 1D, or 2D)\n",
    "tag = \"2D\"\n",
    "# choose the colorscheme\n",
    "fill_color = (\n",
    "    135/256,\n",
    "    186/256,\n",
    "    112/256,\n",
    ")\n",
    "\n",
    "stroke_color = (92/256, 124/256, 76/256)\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "    \"JG1102\",\n",
    "    #\"JG1105\",  # GCN is very low compared to others\n",
    "    \"JG1122\",\n",
    "    \"JG1124\",\n",
    "    \"JG1127\",\n",
    "    \"JG1112\",\n",
    "    \"JG1130\",\n",
    "    \"JG1133\",\n",
    "    \"JG1119\",\n",
    "\n",
    "]\n",
    "\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(experiment_ids) & df_all['tags'].apply(lambda x: tag in x)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(experiment_ids, itertools.count()))\n",
    "df_plot = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict))\n",
    "ticklabels = df_plot[\"Model+Features\"].unique().tolist()\n",
    "\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(5.6, 4))\n",
    "sns.stripplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "              x=\"Model+Features\",\n",
    "              y=metric,\n",
    "              dodge=True,\n",
    "              jitter=.15,\n",
    "              edgecolor=stroke_color,\n",
    "              color=fill_color,\n",
    "              linewidth=.5,\n",
    "              legend=False,\n",
    "              marker=\"o\",\n",
    "              size=3.5,\n",
    "              alpha=.8\n",
    "             )\n",
    "\n",
    "ax.axhline(0.569, ls=\"--\", color=\"black\", linewidth=.7)  # chance level for val/2D\n",
    "\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.xaxis.set_tick_params(labelrotation=90)\n",
    "ax.set_ylim((0.45, 1))\n",
    "ax.xaxis.set_ticklabels(df_plot[\"Model+Features\"].drop_duplicates().str.replace(\"LogisticRegression\", \"LogReg\"))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_scatter.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_scatter.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.groupby(\"Model+Features\")[\"val/avgPrecision_macro\"].agg([np.mean, np.std]).sort_values(by=\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the best model significantly better than the rest?\n",
    "best = \"D-MPNN/CGR\"\n",
    "x = df_plot.loc[df_plot[\"Model+Features\"] == best].sort_values(by=\"run_id\")[\"val/avgPrecision_macro\"].to_numpy()\n",
    "for i in df_plot[\"Model+Features\"].drop_duplicates():\n",
    "    if i != best:\n",
    "        y = df_plot.loc[df_plot[\"Model+Features\"] == i].sort_values(by=\"run_id\")[\"val/avgPrecision_macro\"].to_numpy()\n",
    "        print(i, \":\\t\", wilcoxon(x, y, alternative=\"two-sided\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barplot\n",
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "# choose the task (0D, 1D, or 2D)\n",
    "tag = \"3D\"\n",
    "# choose the colorscheme\n",
    "fill_color = (198/256, 193/256, 80/256)\n",
    "\n",
    "stroke_color = (142/256, 136/256, 58/256)\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "    \"JG1103\",\n",
    "    \"JG1107\",\n",
    "    \"JG1108\",\n",
    "    \"JG1110\",\n",
    "    \"JG1111\",\n",
    "    \"JG1134\",\n",
    "    \"JG1113\",\n",
    "    \"JG1114\",\n",
    "    \"JG1120\",\n",
    "]\n",
    "\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(experiment_ids) & df_all['tags'].apply(lambda x: tag in x)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(experiment_ids, itertools.count()))\n",
    "df_plot = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict))\n",
    "ticklabels = df_plot[\"Model+Features\"].unique().tolist()\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(2.8,2))\n",
    "sns.barplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "    x=\"Model+Features\",\n",
    "    y=metric,\n",
    "    errorbar=errorbar,\n",
    "    errwidth=errwidth,\n",
    "    errcolor=errcolor,\n",
    "    capsize=capsize,\n",
    "    color=fill_color,\n",
    "    edgecolor=stroke_color,\n",
    "    linewidth=linewidth,\n",
    "    alpha=.8,\n",
    "    width=.7,\n",
    ")\n",
    "\n",
    "sns.stripplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "              x=\"Model+Features\",\n",
    "              y=metric,\n",
    "              dodge=True,\n",
    "              edgecolor=stroke_color,\n",
    "              color=fill_color,\n",
    "              linewidth=.5,\n",
    "              legend=False,\n",
    "              marker=\"o\",\n",
    "              size=2.5,\n",
    "              alpha=.8\n",
    "             )\n",
    "\n",
    "ax.axhline(0.578, ls=\"--\", color=\"black\", linewidth=.7)  # chance level for val/3D\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.xaxis.set_tick_params(labelrotation=90)\n",
    "ax.set_ylim((0, 1))\n",
    "ax.xaxis.set_ticklabels(df_plot[\"Model+Features\"].drop_duplicates().str.replace(\"LogisticRegression\", \"LogReg\"))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_barplot.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_barplot.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot\n",
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "# choose the task (0D, 1D, or 2D)\n",
    "tag = \"3D\"\n",
    "# choose the colorscheme\n",
    "fill_color = (198/256, 193/256, 80/256)\n",
    "\n",
    "stroke_color = (142/256, 136/256, 58/256)\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "    \"JG1103\",\n",
    "    \"JG1107\",\n",
    "    \"JG1108\",\n",
    "    \"JG1110\",\n",
    "    \"JG1111\",\n",
    "    \"JG1134\",\n",
    "    \"JG1113\",\n",
    "    \"JG1114\",\n",
    "    \"JG1120\",\n",
    "]\n",
    "\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(experiment_ids) & df_all['tags'].apply(lambda x: tag in x)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(experiment_ids, itertools.count()))\n",
    "df_plot = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict))\n",
    "ticklabels = df_plot[\"Model+Features\"].unique().tolist()\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(5.6, 4))\n",
    "sns.boxplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "    x=\"Model+Features\",\n",
    "    y=metric,\n",
    "    color=fill_color,\n",
    "    boxprops={\"edgecolor\": stroke_color},\n",
    "    medianprops={\"color\": stroke_color},\n",
    "    linewidth=linewidth,\n",
    "    fliersize=2.5,\n",
    "    saturation=.8,\n",
    "    width=.7,\n",
    ")\n",
    "\n",
    "ax.axhline(0.578, ls=\"--\", color=\"black\", linewidth=.7)  # chance level for val/3D\n",
    "\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.xaxis.set_tick_params(labelrotation=90)\n",
    "ax.set_ylim((0.4, 0.88))\n",
    "ax.xaxis.set_ticklabels(df_plot[\"Model+Features\"].drop_duplicates().str.replace(\"LogisticRegression\", \"LogReg\"))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_boxplot.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_boxplot.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter\n",
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "# choose the task (0D, 1D, or 2D)\n",
    "tag = \"3D\"\n",
    "# choose the colorscheme\n",
    "fill_color = (198/256, 193/256, 80/256)\n",
    "\n",
    "stroke_color = (142/256, 136/256, 58/256)\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "    \"JG1103\",\n",
    "    \"JG1107\",\n",
    "    \"JG1108\",\n",
    "    \"JG1110\",\n",
    "    \"JG1111\",\n",
    "    \"JG1134\",\n",
    "    \"JG1113\",\n",
    "    \"JG1114\",\n",
    "    \"JG1120\",\n",
    "]\n",
    "\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(experiment_ids) & df_all['tags'].apply(lambda x: tag in x)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(experiment_ids, itertools.count()))\n",
    "df_plot = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict))\n",
    "ticklabels = df_plot[\"Model+Features\"].unique().tolist()\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(5.6, 4))\n",
    "sns.stripplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "              x=\"Model+Features\",\n",
    "              y=metric,\n",
    "              dodge=True,\n",
    "              jitter=.15,\n",
    "              edgecolor=stroke_color,\n",
    "              color=fill_color,\n",
    "              linewidth=.5,\n",
    "              legend=False,\n",
    "              marker=\"o\",\n",
    "              size=3.5,\n",
    "              alpha=.8\n",
    ")\n",
    "\n",
    "ax.axhline(0.578, ls=\"--\", color=\"black\", linewidth=.7)  # chance level for val/3D\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.xaxis.set_tick_params(labelrotation=90)\n",
    "ax.set_ylim((0.4, 0.88))\n",
    "ax.xaxis.set_ticklabels(df_plot[\"Model+Features\"].drop_duplicates().str.replace(\"LogisticRegression\", \"LogReg\"))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_scatter.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_scatter.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a lineplot\n",
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "# choose the task (0D, 1D, or 2D)\n",
    "tag = \"3D\"\n",
    "# choose the colorscheme\n",
    "fill_color = (198/256, 193/256, 80/256)\n",
    "\n",
    "stroke_color = (142/256, 136/256, 58/256)\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "    \"JG1103\",\n",
    "    \"JG1107\",\n",
    "    \"JG1108\",\n",
    "    \"JG1110\",\n",
    "    \"JG1111\",\n",
    "    \"JG1134\",\n",
    "    \"JG1113\",\n",
    "    \"JG1114\",\n",
    "    \"JG1120\",\n",
    "]\n",
    "\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(experiment_ids) & df_all['tags'].apply(lambda x: tag in x)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(experiment_ids, itertools.count()))\n",
    "df_plot = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict))\n",
    "ticklabels = df_plot[\"Model+Features\"].unique().tolist()\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(5.6, 4))\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "              x=\"Model+Features\",\n",
    "              y=metric,\n",
    "              hue=\"fold\", \n",
    "              palette=sns.color_palette(\"husl\", 9),\n",
    "              #edgecolor=stroke_color,\n",
    "              #color=fill_color,\n",
    "              #linewidth=.5,\n",
    "              #legend=False,\n",
    "              #marker=\"o\",\n",
    "              #size=3.5,\n",
    "              #alpha=.8\n",
    ")\n",
    "\n",
    "ax.set_xlabel(None)\n",
    "ax.xaxis.set_tick_params(labelrotation=90)\n",
    "ax.set_ylim((0.4, 0.88))\n",
    "ax.xaxis.set_ticklabels(df_plot[\"Model+Features\"].drop_duplicates().str.replace(\"LogisticRegression\", \"LogReg\"))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_line.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_line.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same, but only best models\n",
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "# choose the task (0D, 1D, or 2D)\n",
    "tag = \"3D\"\n",
    "# choose the colorscheme\n",
    "fill_color = (198/256, 193/256, 80/256)\n",
    "\n",
    "stroke_color = (142/256, 136/256, 58/256)\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "        \"JG1111\",\n",
    "    \"JG1134\",\n",
    "        \"JG1108\",\n",
    "    \"JG1103\",\n",
    "\n",
    "\n",
    "            \"JG1110\",\n",
    "]\n",
    "\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(experiment_ids) & df_all['tags'].apply(lambda x: tag in x)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(experiment_ids, itertools.count()))\n",
    "df_plot = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict))\n",
    "ticklabels = df_plot[\"Model+Features\"].unique().tolist()\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(5.6, 4))\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    data=df_plot, \n",
    "              x=\"Model+Features\",\n",
    "              y=metric,\n",
    "              hue=\"fold\", \n",
    "              palette=sns.color_palette(\"husl\", 9),\n",
    "              #edgecolor=stroke_color,\n",
    "              #color=fill_color,\n",
    "              #linewidth=.5,\n",
    "              #legend=False,\n",
    "              #marker=\"o\",\n",
    "              #size=3.5,\n",
    "              #alpha=.8\n",
    ")\n",
    "\n",
    "ax.set_xlabel(None)\n",
    "ax.xaxis.set_tick_params(labelrotation=90)\n",
    "ax.set_ylim((0.4, 0.88))\n",
    "ax.xaxis.set_ticklabels(df_plot[\"Model+Features\"].drop_duplicates().str.replace(\"LogisticRegression\", \"LogReg\"))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_line.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_{tag}_models_{metric.replace('/', '_')}_line.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.groupby(\"Model+Features\")[\"val/avgPrecision_macro\"].agg([np.mean, np.std]).sort_values(by=\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the best model significantly better than the rest?\n",
    "best = \"XGB/FP\"\n",
    "x = df_plot.loc[df_plot[\"Model+Features\"] == best].sort_values(by=\"run_id\")[\"val/avgPrecision_macro\"].to_numpy()\n",
    "for i in df_plot[\"Model+Features\"].drop_duplicates():\n",
    "    if i != best:\n",
    "        y = df_plot.loc[df_plot[\"Model+Features\"] == i].sort_values(by=\"run_id\")[\"val/avgPrecision_macro\"].to_numpy()\n",
    "        print(i, \":\\t\", wilcoxon(x, y, alternative=\"two-sided\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0D restricted data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette([\"#5790fc\", \"#f89c20\", \"#964a8b\", \"#e42536\"])  # works for colorblind\n",
    "palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some settings for all following plots\n",
    "\n",
    "# order used for hue/style\n",
    "order = ['FFN/OHE', 'XGB/FP', 'XGB/FP+RDKit', 'D-MPNN/CGR', 'D-MPNN/RDKit']\n",
    "\n",
    "# linestyle\n",
    "dashes=[(3, 3), (3, 3), (1, 1), (3, 3), (1, 1)]\n",
    "\n",
    "# alternative palette where two colors are reused with less saturation to show derivative categories\n",
    "palette = sns.color_palette([\"#5790fc\", \"#f89c20\", \"#a66611\", \"#a0228d\", \"#783b6e\"])  # works for colorblind\n",
    "palette = sns.color_palette([\"#5790fc\", \"#f89c20\", \"#a66611\", \"#e42536\", \"#a1212c\"])  # works for colorblind\n",
    "palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chance_ap(split_name, fold=None, set_type=\"test\"):\n",
    "    df = pd.read_csv(DATA_ROOT / \"splits\" / \"split_statistics.csv\")\n",
    "    if fold is not None:\n",
    "        return df.loc[(df[\"split_name\"] == split_name ) & (df[\"fold\"] == fold), f\"Chance level average precision macro on {set_type} set\"].item()\n",
    "    else:\n",
    "        return df.loc[(df[\"split_name\"] == split_name ), f\"Chance level average precision macro on {set_type} set\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "        [  # _0.625\n",
    "            \"JG1266\",  # D-MPNN+CGR\n",
    "            \"JG1265\",  # XGB+FP+RDKit\n",
    "            \"JG1264\",  # FFN+OHE\n",
    "            \"JG1267\",  # D-MPNN+CGR+RDKit \n",
    "            \"JG1308\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _1.25\n",
    "            \"JG1147\",  # D-MPNN+CGR\n",
    "            \"JG1153\",  # XGB+FP+RDKit\n",
    "            \"JG1141\",  # FFN+OHE\n",
    "            \"JG1159\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1307\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _2.5\n",
    "            \"JG1146\",  # D-MPNN+CGR\n",
    "            \"JG1152\",  # XGB+FP+RDKit\n",
    "            \"JG1140\",  # FFN+OHE\n",
    "            \"JG1158\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1306\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _5\n",
    "            \"JG1145\",  # D-MPNN+CGR\n",
    "            \"JG1151\",  # XGB+FP+RDKit\n",
    "            \"JG1139\",  # FFN+OHE\n",
    "            \"JG1157\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1305\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _10\n",
    "            \"JG1144\",  # D-MPNN+CGR\n",
    "            \"JG1150\",  # XGB+FP+RDKit\n",
    "            \"JG1138\",  # FFN+OHE\n",
    "            \"JG1156\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1304\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _20\n",
    "            \"JG1143\",  # D-MPNN+CGR\n",
    "            \"JG1149\",  # XGB+FP+RDKit\n",
    "            \"JG1137\",  # FFN+OHE\n",
    "            \"JG1155\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1303\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _40\n",
    "            \"JG1142\",  # D-MPNN+CGR\n",
    "            \"JG1148\",  # XGB+FP+RDKit\n",
    "            \"JG1136\",  # FFN+OHE\n",
    "            \"JG1154\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1302\",  # XGB/FP\n",
    "    ],\n",
    "        [  # full\n",
    "            \"JG1100\",  # D-MPNN+CGR\n",
    "            \"JG1131\",  # XGB+FP+RDKit\n",
    "            \"JG1117\",  # FFN+OHE\n",
    "            \"JG1160\",  # D-MPNN+CGR+RDKit n/a for full 0D split\n",
    "            \"JG1128\",  # XGB/FP\n",
    "    ],\n",
    "]\n",
    "\n",
    "sample_counts = {  # mean number of training samples for each split \n",
    "    \"0D_0.625\": 250,\n",
    "    \"0D_1.25\": 500,\n",
    "    \"0D_2.5\": 1000,\n",
    "    \"0D_5\": 2000,\n",
    "    \"0D_10\": 4001,\n",
    "    \"0D_20\": 8003,\n",
    "    \"0D_40\": 16006,\n",
    "    \"0D\": 32014,\n",
    "}\n",
    "\n",
    "chance_level = [0.555, 0.555, 0.555, 0.555, 0.554, 0.555, 0.556, 0.555]  # same order as sample_counts\n",
    "\n",
    "exps = [i for exp in experiment_ids for i in exp]\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(exps)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(exps, itertools.count()))\n",
    "df_plot_x = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict)).copy()\n",
    "df_plot_x[\"x\"] = df_plot_x[\"tags\"].apply(lambda x: sample_counts[x[0]])\n",
    "\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(4.75,4))\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    data=df_plot_x, \n",
    "    x=\"x\",\n",
    "    y=metric,\n",
    "    palette=palette,\n",
    "    hue_order=order,\n",
    "    style_order=order,\n",
    "    style=\"Model+Features\",\n",
    "    hue=\"Model+Features\",\n",
    "    errorbar=errorbar,\n",
    "    err_style=\"bars\",\n",
    "    dashes=dashes,\n",
    "    linewidth=linewidth,\n",
    "    markers=[\"o\", \"^\", \"s\", \"<\", \"p\"],\n",
    ")\n",
    "# don't plot chance level b/c it is much lower\n",
    "#sns.lineplot(x=sample_counts.values(), y=chance_level, c=\"black\", label=\"Chance level\")\n",
    "\n",
    "ax.set_xlabel(\"Training data size\")\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.xaxis.set_tick_params(labelrotation=0)\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(0.05))\n",
    "ax.set_ylim((0.7, 1))\n",
    "ax.set_xticks(\n",
    "    [250, 500, 1000, 2000, 4000, 8000, 16000, 32000], \n",
    "    ['250', '500', '1000', '2000', '4000', '8000', '16000', '32000']\n",
    ")\n",
    "ax.legend(title=None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_0D_restricted-data_models_{metric.replace('/', '_')}.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_0D_restricted-data_models_{metric.replace('/', '_')}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"test/avgPrecision_macro\"\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "        [  # _0.625\n",
    "            \"JG1266\",  # D-MPNN+CGR\n",
    "            \"JG1265\",  # XGB+FP+RDKit\n",
    "            \"JG1264\",  # FFN+OHE\n",
    "            \"JG1267\",  # D-MPNN+CGR+RDKit \n",
    "            \"JG1308\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _1.25\n",
    "            \"JG1147\",  # D-MPNN+CGR\n",
    "            \"JG1153\",  # XGB+FP+RDKit\n",
    "            \"JG1141\",  # FFN+OHE\n",
    "            \"JG1159\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1307\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _2.5\n",
    "            \"JG1146\",  # D-MPNN+CGR\n",
    "            \"JG1152\",  # XGB+FP+RDKit\n",
    "            \"JG1140\",  # FFN+OHE\n",
    "            \"JG1158\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1306\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _5\n",
    "            \"JG1145\",  # D-MPNN+CGR\n",
    "            \"JG1151\",  # XGB+FP+RDKit\n",
    "            \"JG1139\",  # FFN+OHE\n",
    "            \"JG1157\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1305\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _10\n",
    "            \"JG1144\",  # D-MPNN+CGR\n",
    "            \"JG1150\",  # XGB+FP+RDKit\n",
    "            \"JG1138\",  # FFN+OHE\n",
    "            \"JG1156\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1304\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _20\n",
    "            \"JG1143\",  # D-MPNN+CGR\n",
    "            \"JG1149\",  # XGB+FP+RDKit\n",
    "            \"JG1137\",  # FFN+OHE\n",
    "            \"JG1155\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1303\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _40\n",
    "            \"JG1142\",  # D-MPNN+CGR\n",
    "            \"JG1148\",  # XGB+FP+RDKit\n",
    "            \"JG1136\",  # FFN+OHE\n",
    "            \"JG1154\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1302\",  # XGB/FP\n",
    "    ],\n",
    "        [  # full\n",
    "            \"JG1100\",  # D-MPNN+CGR\n",
    "            \"JG1131\",  # XGB+FP+RDKit\n",
    "            \"JG1117\",  # FFN+OHE\n",
    "            \"JG1160\",  # D-MPNN+CGR+RDKit n/a for full 0D split\n",
    "            \"JG1128\",  # XGB/FP\n",
    "    ],\n",
    "]\n",
    "sample_counts = {  # mean number of training samples for each split\n",
    "    \"0D_0.625\": 250,\n",
    "    \"0D_1.25\": 500,\n",
    "    \"0D_2.5\": 1000,\n",
    "    \"0D_5\": 2000,\n",
    "    \"0D_10\": 4001,\n",
    "    \"0D_20\": 8003,\n",
    "    \"0D_40\": 16006,\n",
    "    \"0D\": 32014,\n",
    "}\n",
    "\n",
    "chance_level = [0.555, 0.556, 0.556, 0.556, 0.556, 0.556, 0.555, 0.555]  # same order as sample_counts\n",
    "\n",
    "\n",
    "exps = [i for exp in experiment_ids for i in exp]\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(exps)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(exps, itertools.count()))\n",
    "df_plot_x = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict)).copy()\n",
    "df_plot_x[\"x\"] = df_plot_x[\"tags\"].apply(lambda x: sample_counts[x[0]])\n",
    "\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(4.75,4))\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    data=df_plot_x, \n",
    "    x=\"x\",\n",
    "    y=metric,\n",
    "    palette=palette,\n",
    "    hue_order=order,\n",
    "    style_order=order,\n",
    "    style=\"Model+Features\",\n",
    "    hue=\"Model+Features\",\n",
    "    errorbar=errorbar,\n",
    "    err_style=\"bars\",\n",
    "    dashes=dashes,\n",
    "    linewidth=linewidth,\n",
    "    markers=[\"o\", \"^\", \"s\", \"<\", \"p\"],\n",
    ")\n",
    "\n",
    "# don't plot chance level b/c it is much lower\n",
    "#sns.lineplot(x=sample_counts.values(), y=chance_level, c=\"black\", label=\"Chance level\")\n",
    "\n",
    "ax.set_xlabel(\"Training data size\")\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.xaxis.set_tick_params(labelrotation=0)\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(0.05))\n",
    "ax.set_ylim((0.7, 1))\n",
    "ax.set_xticks(\n",
    "    [250, 500, 1000, 2000, 4000, 8000, 16000, 32000], \n",
    "    ['250', '500', '1000', '2000', '4000', '8000', '16000', '32000']\n",
    ")\n",
    "ax.legend(loc=\"upper left\", title=None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_0D_restricted-data_models_{metric.replace('/', '_')}.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_0D_restricted-data_models_{metric.replace('/', '_')}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scaled using chance level for better comparability across sizes\n",
    "# i.e. we divide by the chance level so that the chance result is now 0 and best is still 1\n",
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"test/avgPrecision_macro\"\n",
    "\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "        [  # _0.625\n",
    "            \"JG1266\",  # D-MPNN+CGR\n",
    "            \"JG1265\",  # XGB+FP+RDKit\n",
    "            \"JG1264\",  # FFN+OHE\n",
    "            \"JG1267\",  # D-MPNN+CGR+RDKit \n",
    "            \"JG1308\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _1.25\n",
    "            \"JG1147\",  # D-MPNN+CGR\n",
    "            \"JG1153\",  # XGB+FP+RDKit\n",
    "            \"JG1141\",  # FFN+OHE\n",
    "            \"JG1159\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1307\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _2.5\n",
    "            \"JG1146\",  # D-MPNN+CGR\n",
    "            \"JG1152\",  # XGB+FP+RDKit\n",
    "            \"JG1140\",  # FFN+OHE\n",
    "            \"JG1158\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1306\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _5\n",
    "            \"JG1145\",  # D-MPNN+CGR\n",
    "            \"JG1151\",  # XGB+FP+RDKit\n",
    "            \"JG1139\",  # FFN+OHE\n",
    "            \"JG1157\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1305\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _10\n",
    "            \"JG1144\",  # D-MPNN+CGR\n",
    "            \"JG1150\",  # XGB+FP+RDKit\n",
    "            \"JG1138\",  # FFN+OHE\n",
    "            \"JG1156\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1304\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _20\n",
    "            \"JG1143\",  # D-MPNN+CGR\n",
    "            \"JG1149\",  # XGB+FP+RDKit\n",
    "            \"JG1137\",  # FFN+OHE\n",
    "            \"JG1155\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1303\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _40\n",
    "            \"JG1142\",  # D-MPNN+CGR\n",
    "            \"JG1148\",  # XGB+FP+RDKit\n",
    "            \"JG1136\",  # FFN+OHE\n",
    "            \"JG1154\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1302\",  # XGB/FP\n",
    "    ],\n",
    "        [  # full\n",
    "            \"JG1100\",  # D-MPNN+CGR\n",
    "            \"JG1131\",  # XGB+FP+RDKit\n",
    "            \"JG1117\",  # FFN+OHE\n",
    "            \"JG1160\",  # D-MPNN+CGR+RDKit n/a for full 0D split\n",
    "            \"JG1128\",  # XGB/FP\n",
    "    ],\n",
    "]\n",
    "sample_counts = {  # mean number of training samples for each split\n",
    "    \"0D_0.625\": 250,\n",
    "    \"0D_1.25\": 500,\n",
    "    \"0D_2.5\": 1000,\n",
    "    \"0D_5\": 2000,\n",
    "    \"0D_10\": 4001,\n",
    "    \"0D_20\": 8003,\n",
    "    \"0D_40\": 16006,\n",
    "    \"0D\": 32014,\n",
    "}\n",
    "\n",
    "exps = [i for exp in experiment_ids for i in exp]\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(exps)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(exps, itertools.count()))\n",
    "df_plot_x = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict)).copy()\n",
    "df_plot_x[\"x\"] = df_plot_x[\"tags\"].apply(lambda x: sample_counts[x[0]])\n",
    "\n",
    "# divide by chance level. Note that we obtain the chance level per individual fold so that the SEM still makes sense after scaling.\n",
    "chance_dict = {k: v for k, v in zip(sample_counts.keys(), chance_level)}\n",
    "df_plot_x[f\"{metric}_scaled\"] = (df_plot_x[metric] - df_plot_x.apply(lambda x: get_chance_ap(x[\"tags\"][0], x[\"fold\"], \"test\"), axis=1)) / (1 - df_plot_x.apply(lambda x: get_chance_ap(x[\"tags\"][0], x[\"fold\"], \"test\"), axis=1))\n",
    "\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(4.75,4))\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    data=df_plot_x, \n",
    "    x=\"x\",\n",
    "    y=f\"{metric}_scaled\",\n",
    "    palette=palette,\n",
    "    hue_order=order,\n",
    "    style_order=order,\n",
    "    style=\"Model+Features\",\n",
    "    hue=\"Model+Features\",\n",
    "    errorbar=errorbar,\n",
    "    err_style=\"bars\",\n",
    "    dashes=dashes,\n",
    "    linewidth=linewidth,\n",
    "    markers=[\"o\", \"^\", \"s\", \"<\", \"p\"],\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Training data size\")\n",
    "ax.set_ylabel(\"AUPRC (relative improvement over chance)\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.xaxis.set_tick_params(labelrotation=0)\n",
    "ax.set_ylim((0.3, 1))\n",
    "ax.set_xticks(\n",
    "    [250, 500, 1000, 2000, 4000, 8000, 16000, 32000], \n",
    "    ['250', '500', '1000', '2000', '4000', '8000', '16000', '32000']\n",
    ")\n",
    "ax.legend(loc=\"lower right\", title=None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_0D_restricted-data_models_{metric.replace('/', '_')}_relative.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_0D_restricted-data_models_{metric.replace('/', '_')}_relative.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D restricted data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "tags = [\"1D_2.5\", \"1D_5\", \"1D_10\", \"1D_20\", \"1D_40\", \"1D_80\"]\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "        [  # _2.5\n",
    "            \"JG1178\",  # D-MPNN+CGR\n",
    "            \"JG1172\",  # XGB+FP+RDKit\n",
    "            \"JG1166\",  # FFN+OHE\n",
    "            \"JG1184\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1301\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _5\n",
    "            \"JG1177\",  # D-MPNN+CGR\n",
    "            \"JG1171\",  # XGB+FP+RDKit\n",
    "            \"JG1165\",  # FFN+OHE\n",
    "            \"JG1183\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1300\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _10\n",
    "            \"JG1176\",  # D-MPNN+CGR\n",
    "            \"JG1170\",  # XGB+FP+RDKit\n",
    "            \"JG1164\",  # FFN+OHE\n",
    "            \"JG1182\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1299\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _20\n",
    "            \"JG1175\",  # D-MPNN+CGR\n",
    "            \"JG1169\",  # XGB+FP+RDKit\n",
    "            \"JG1163\",  # FFN+OHE\n",
    "            \"JG1181\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1298\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _40\n",
    "            \"JG1174\",  # D-MPNN+CGR\n",
    "            \"JG1168\",  # XGB+FP+RDKit\n",
    "            \"JG1162\",  # FFN+OHE\n",
    "            \"JG1180\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1297\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _80\n",
    "            \"JG1173\",  # D-MPNN+CGR\n",
    "            \"JG1167\",  # XGB+FP+RDKit\n",
    "            \"JG1161\",  # FFN+OHE\n",
    "            \"JG1179\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1296\",  # XGB+FP\n",
    "    ],\n",
    "]\n",
    "\n",
    "sample_counts = {  # mean number of training samples for each split \n",
    "    \"1D_2.5\": 487,\n",
    "    \"1D_5\": 1668,\n",
    "    \"1D_10\": 3680,\n",
    "    \"1D_20\": 8107,\n",
    "    \"1D_40\": 15648,\n",
    "    \"1D_80\": 31250,\n",
    "}\n",
    "\n",
    "chance_level = [0.562, 0.565, 0.581, 0.551, 0.578, 0.570]  # same order as sample_counts\n",
    "\n",
    "\n",
    "exps = [i for exp in experiment_ids for i in exp]\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(exps)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(exps, itertools.count()))\n",
    "df_plot_x = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict)).copy()\n",
    "df_plot_x[\"x\"] = df_plot_x[\"tags\"].apply(lambda x: sample_counts[x[0]])\n",
    "\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(4.75,4))\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    data=df_plot_x, \n",
    "    x=\"x\",\n",
    "    y=metric,\n",
    "    palette=palette,\n",
    "    hue_order=order,\n",
    "    style_order=order,\n",
    "    style=\"Model+Features\",\n",
    "    hue=\"Model+Features\",\n",
    "    errorbar=errorbar,\n",
    "    err_style=\"bars\",\n",
    "    dashes=dashes,\n",
    "    linewidth=linewidth,\n",
    "    markers=[\"o\", \"^\", \"s\", \"<\", \"p\"],\n",
    ")\n",
    "\n",
    "#sns.lineplot(x=sample_counts.values(), y=chance_level, c=\"black\", label=\"Chance level\")\n",
    "\n",
    "ax.set_xlabel(\"Training data size\")\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.xaxis.set_tick_params(labelrotation=0)\n",
    "ax.set_ylim((0.70, 0.95))\n",
    "ax.set_xticks(\n",
    "    [500, 1000, 2000, 4000, 8000, 16000, 32000], \n",
    "    ['500', '1000', '2000', '4000', '8000', '16000', '32000']\n",
    ")\n",
    "ax.legend(loc=\"upper left\", title=None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_1D_restricted-data_models_{metric.replace('/', '_')}.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_1D_restricted-data_models_{metric.replace('/', '_')}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"test/avgPrecision_macro\"\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "        [  # _2.5\n",
    "            \"JG1178\",  # D-MPNN+CGR\n",
    "            \"JG1172\",  # XGB+FP+RDKit\n",
    "            \"JG1166\",  # FFN+OHE\n",
    "            \"JG1184\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1301\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _5\n",
    "            \"JG1177\",  # D-MPNN+CGR\n",
    "            \"JG1171\",  # XGB+FP+RDKit\n",
    "            \"JG1165\",  # FFN+OHE\n",
    "            \"JG1183\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1300\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _10\n",
    "            \"JG1176\",  # D-MPNN+CGR\n",
    "            \"JG1170\",  # XGB+FP+RDKit\n",
    "            \"JG1164\",  # FFN+OHE\n",
    "            \"JG1182\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1299\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _20\n",
    "            \"JG1175\",  # D-MPNN+CGR\n",
    "            \"JG1169\",  # XGB+FP+RDKit\n",
    "            \"JG1163\",  # FFN+OHE\n",
    "            \"JG1181\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1298\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _40\n",
    "            \"JG1174\",  # D-MPNN+CGR\n",
    "            \"JG1168\",  # XGB+FP+RDKit\n",
    "            \"JG1162\",  # FFN+OHE\n",
    "            \"JG1180\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1297\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _80\n",
    "            \"JG1173\",  # D-MPNN+CGR\n",
    "            \"JG1167\",  # XGB+FP+RDKit\n",
    "            \"JG1161\",  # FFN+OHE\n",
    "            \"JG1179\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1296\",  # XGB+FP\n",
    "    ],\n",
    "]\n",
    "\n",
    "sample_counts = {  # mean number of training samples for each split \n",
    "    \"1D_2.5\": 487,\n",
    "    \"1D_5\": 1668,\n",
    "    \"1D_10\": 3680,\n",
    "    \"1D_20\": 8107,\n",
    "    \"1D_40\": 15648,\n",
    "    \"1D_80\": 31250,\n",
    "}\n",
    "\n",
    "chance_level = [0.562, 0.565, 0.581, 0.551, 0.578, 0.570]  # same order as sample_counts\n",
    "\n",
    "\n",
    "exps = [i for exp in experiment_ids for i in exp]\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(exps)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(exps, itertools.count()))\n",
    "df_plot_x = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict)).copy()\n",
    "df_plot_x[\"x\"] = df_plot_x[\"tags\"].apply(lambda x: sample_counts[x[0]])\n",
    "\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(4.75,4))\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    data=df_plot_x, \n",
    "    x=\"x\",\n",
    "    y=metric,\n",
    "    palette=palette,\n",
    "    hue_order=order,\n",
    "    style_order=order,\n",
    "    style=\"Model+Features\",\n",
    "    hue=\"Model+Features\",\n",
    "    errorbar=errorbar,\n",
    "    err_style=\"bars\",\n",
    "    dashes=dashes,\n",
    "    linewidth=linewidth,\n",
    "    markers=[\"o\", \"^\", \"s\", \"<\", \"p\"],\n",
    ")\n",
    "\n",
    "#sns.lineplot(x=sample_counts.values(), y=chance_level, c=\"black\", label=\"Chance level\")\n",
    "\n",
    "ax.set_xlabel(\"Training data size\")\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.xaxis.set_tick_params(labelrotation=0)\n",
    "ax.set_ylim((0.70, 0.95))\n",
    "ax.set_xticks(\n",
    "    [500, 1000, 2000, 4000, 8000, 16000, 32000], \n",
    "    ['500', '1000', '2000', '4000', '8000', '16000', '32000']\n",
    ")\n",
    "\n",
    "ax.legend(loc=\"upper left\", title=None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_1D_restricted-data_models_{metric.replace('/', '_')}.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_1D_restricted-data_models_{metric.replace('/', '_')}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scaled using chance level for better comparability across sizes\n",
    "# i.e. we divide by the chance level so that the chance result is now 0 and best is still 1\n",
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"test/avgPrecision_macro\"\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "        [  # _2.5\n",
    "            \"JG1178\",  # D-MPNN+CGR\n",
    "            \"JG1172\",  # XGB+FP+RDKit\n",
    "            \"JG1166\",  # FFN+OHE\n",
    "            \"JG1184\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1301\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _5\n",
    "            \"JG1177\",  # D-MPNN+CGR\n",
    "            \"JG1171\",  # XGB+FP+RDKit\n",
    "            \"JG1165\",  # FFN+OHE\n",
    "            \"JG1183\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1300\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _10\n",
    "            \"JG1176\",  # D-MPNN+CGR\n",
    "            \"JG1170\",  # XGB+FP+RDKit\n",
    "            \"JG1164\",  # FFN+OHE\n",
    "            \"JG1182\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1299\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _20\n",
    "            \"JG1175\",  # D-MPNN+CGR\n",
    "            \"JG1169\",  # XGB+FP+RDKit\n",
    "            \"JG1163\",  # FFN+OHE\n",
    "            \"JG1181\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1298\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _40\n",
    "            \"JG1174\",  # D-MPNN+CGR\n",
    "            \"JG1168\",  # XGB+FP+RDKit\n",
    "            \"JG1162\",  # FFN+OHE\n",
    "            \"JG1180\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1297\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _80\n",
    "            \"JG1173\",  # D-MPNN+CGR\n",
    "            \"JG1167\",  # XGB+FP+RDKit\n",
    "            \"JG1161\",  # FFN+OHE\n",
    "            \"JG1179\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1296\",  # XGB+FP\n",
    "    ],\n",
    "]\n",
    "\n",
    "sample_counts = {  # mean number of training samples for each split \n",
    "    \"1D_2.5\": 487,\n",
    "    \"1D_5\": 1668,\n",
    "    \"1D_10\": 3680,\n",
    "    \"1D_20\": 8107,\n",
    "    \"1D_40\": 15648,\n",
    "    \"1D_80\": 31250,\n",
    "}\n",
    "\n",
    "exps = [i for exp in experiment_ids for i in exp]\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(exps)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(exps, itertools.count()))\n",
    "df_plot_x = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict)).copy()\n",
    "df_plot_x[\"x\"] = df_plot_x[\"tags\"].apply(lambda x: sample_counts[x[0]])\n",
    "\n",
    "# divide by chance level. Note that we obtain the chance level per individual fold so that the SEM still makes sense after scaling.\n",
    "chance_dict = {k: v for k, v in zip(sample_counts.keys(), chance_level)}\n",
    "df_plot_x[f\"{metric}_scaled\"] = (df_plot_x[metric] - df_plot_x.apply(lambda x: get_chance_ap(x[\"tags\"][0], x[\"fold\"], \"test\"), axis=1)) / (1 - df_plot_x.apply(lambda x: get_chance_ap(x[\"tags\"][0], x[\"fold\"], \"test\"), axis=1))\n",
    "\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(4.75,4))\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    data=df_plot_x, \n",
    "    x=\"x\",\n",
    "    y=f\"{metric}_scaled\",\n",
    "    palette=palette,\n",
    "    hue_order=order,\n",
    "    style_order=order,\n",
    "    style=\"Model+Features\",\n",
    "    hue=\"Model+Features\",\n",
    "    errorbar=errorbar,\n",
    "    err_style=\"bars\",\n",
    "    dashes=dashes,\n",
    "    linewidth=linewidth,\n",
    "    markers=[\"o\", \"^\", \"s\", \"<\", \"p\"],\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Training data size\")\n",
    "ax.set_ylabel(\"AUPRC (relative improvement over chance)\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.xaxis.set_tick_params(labelrotation=0)\n",
    "ax.set_ylim((0.3, 1))\n",
    "ax.set_xticks(\n",
    "    [500, 1000, 2000, 4000, 8000, 16000, 32000], \n",
    "    ['500', '1000', '2000', '4000', '8000', '16000', '32000']\n",
    ")\n",
    "ax.legend(loc=\"lower right\", title=None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_1D_restricted-data_models_{metric.replace('/', '_')}_relative.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_1D_restricted-data_models_{metric.replace('/', '_')}_relative.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D restricted data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [   \n",
    "        [  # _2\n",
    "            \"JG1246\",  # D-MPNN+CGR\n",
    "            \"JG1245\",  # XGB+FP+RDKit\n",
    "            \"JG1244\",  # FFN+OHE\n",
    "            \"JG1247\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1295\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _5\n",
    "            \"JG1254\",  # D-MPNN+CGR\n",
    "            \"JG1253\",  # XGB+FP+RDKit\n",
    "            \"JG1252\",  # FFN+OHE\n",
    "            \"JG1255\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1294\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _7.5\n",
    "            \"JG1270\",  # D-MPNN+CGR\n",
    "            \"JG1269\",  # XGB+FP+RDKit\n",
    "            \"JG1268\",  # FFN+OHE\n",
    "            \"JG1271\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1293\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _10\n",
    "            \"JG1238\",  # D-MPNN+CGR\n",
    "            \"JG1237\",  # XGB+FP+RDKit\n",
    "            \"JG1236\",  # FFN+OHE\n",
    "            \"JG1239\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1292\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _15\n",
    "            \"JG1205\",  # D-MPNN+CGR\n",
    "            \"JG1199\",  # XGB+FP+RDKit\n",
    "            \"JG1193\",  # FFN+OHE\n",
    "            \"JG1211\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1291\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _20\n",
    "            \"JG1204\",  # D-MPNN+CGR\n",
    "            \"JG1198\",  # XGB+FP+RDKit\n",
    "            \"JG1192\",  # FFN+OHE\n",
    "            \"JG1210\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1290\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _30\n",
    "            \"JG1203\",  # D-MPNN+CGR\n",
    "            \"JG1197\",  # XGB+FP+RDKit\n",
    "            \"JG1191\",  # FFN+OHE\n",
    "            \"JG1209\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1289\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _40\n",
    "            \"JG1202\",  # D-MPNN+CGR\n",
    "            \"JG1196\",  # XGB+FP+RDKit\n",
    "            \"JG1190\",  # FFN+OHE\n",
    "            \"JG1208\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1288\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _60\n",
    "            \"JG1201\",  # D-MPNN+CGR\n",
    "            \"JG1195\",  # XGB+FP+RDKit\n",
    "            \"JG1189\",  # FFN+OHE\n",
    "            \"JG1207\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1287\",  # XGB+FP\n",
    "    ],\n",
    "        # note: we do not use the results from _80 due to high variance (too small test set)\n",
    "        [  # _80\n",
    "            \"JG1200\",  # D-MPNN+CGR\n",
    "            \"JG1194\",  # XGB+FP+RDKit\n",
    "            \"JG1188\",  # FFN+OHE\n",
    "            \"JG1206\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1286\",  # XGB+FP\n",
    "    ],\n",
    "]\n",
    "\n",
    "sample_counts = {  # mean number of training samples for each split \n",
    "    \"2D_2\": 38,\n",
    "    \"2D_5\": 78,\n",
    "    \"2D_7.5\": 163,\n",
    "    \"2D_10\": 355,\n",
    "    \"2D_15\": 812,\n",
    "    \"2D_20\": 1475,\n",
    "    \"2D_30\": 3193,\n",
    "    \"2D_40\": 6046,\n",
    "    \"2D_60\": 13802,\n",
    "    \"2D_80\": 24562,\n",
    "}\n",
    "\n",
    "chance_level = [get_chance_ap(split, set_type=\"val\") for split in sample_counts.keys()]\n",
    "\n",
    "\n",
    "exps = [i for exp in experiment_ids for i in exp]\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(exps)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(exps, itertools.count()))\n",
    "df_plot_x = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict)).copy()\n",
    "df_plot_x[\"x\"] = df_plot_x[\"tags\"].apply(lambda x: sample_counts[x[0]])\n",
    "\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(4.75,4))\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    data=df_plot_x, \n",
    "    x=\"x\",\n",
    "    y=metric,\n",
    "    palette=palette,\n",
    "    hue_order=order,\n",
    "    style_order=order,\n",
    "    style=\"Model+Features\",\n",
    "    hue=\"Model+Features\",\n",
    "    errorbar=errorbar,\n",
    "    err_style=\"bars\",\n",
    "    dashes=dashes,\n",
    "    linewidth=linewidth,\n",
    "    markers=[\"o\", \"^\", \"s\", \"<\", \"p\"],\n",
    ")\n",
    "\n",
    "sns.lineplot(x=sample_counts.values(), y=chance_level, c=\"black\", label=\"Chance level\")\n",
    "\n",
    "ax.set_xlabel(\"Training data size\")\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.xaxis.set_tick_params(labelrotation=0)\n",
    "ax.set_ylim((0.53, 0.85))\n",
    "ax.set_xticks(\n",
    "    [32*2**n for n in range(10)], \n",
    "    [f\"{32*2**n}\" for n in range(10)],\n",
    ")\n",
    "\n",
    "ax.legend(loc=\"upper left\", title=None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_2D_restricted-data_models_{metric.replace('/', '_')}.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_2D_restricted-data_models_{metric.replace('/', '_')}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"test/avgPrecision_macro\"\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [   \n",
    "        [  # _2\n",
    "            \"JG1246\",  # D-MPNN+CGR\n",
    "            \"JG1245\",  # XGB+FP+RDKit\n",
    "            \"JG1244\",  # FFN+OHE\n",
    "            \"JG1247\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1295\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _5\n",
    "            \"JG1254\",  # D-MPNN+CGR\n",
    "            \"JG1253\",  # XGB+FP+RDKit\n",
    "            \"JG1252\",  # FFN+OHE\n",
    "            \"JG1255\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1294\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _7.5\n",
    "            \"JG1270\",  # D-MPNN+CGR\n",
    "            \"JG1269\",  # XGB+FP+RDKit\n",
    "            \"JG1268\",  # FFN+OHE\n",
    "            \"JG1271\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1293\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _10\n",
    "            \"JG1238\",  # D-MPNN+CGR\n",
    "            \"JG1237\",  # XGB+FP+RDKit\n",
    "            \"JG1236\",  # FFN+OHE\n",
    "            \"JG1239\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1292\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _15\n",
    "            \"JG1205\",  # D-MPNN+CGR\n",
    "            \"JG1199\",  # XGB+FP+RDKit\n",
    "            \"JG1193\",  # FFN+OHE\n",
    "            \"JG1211\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1291\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _20\n",
    "            \"JG1204\",  # D-MPNN+CGR\n",
    "            \"JG1198\",  # XGB+FP+RDKit\n",
    "            \"JG1192\",  # FFN+OHE\n",
    "            \"JG1210\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1290\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _30\n",
    "            \"JG1203\",  # D-MPNN+CGR\n",
    "            \"JG1197\",  # XGB+FP+RDKit\n",
    "            \"JG1191\",  # FFN+OHE\n",
    "            \"JG1209\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1289\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _40\n",
    "            \"JG1202\",  # D-MPNN+CGR\n",
    "            \"JG1196\",  # XGB+FP+RDKit\n",
    "            \"JG1190\",  # FFN+OHE\n",
    "            \"JG1208\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1288\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _60\n",
    "            \"JG1201\",  # D-MPNN+CGR\n",
    "            \"JG1195\",  # XGB+FP+RDKit\n",
    "            \"JG1189\",  # FFN+OHE\n",
    "            \"JG1207\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1287\",  # XGB+FP\n",
    "    ],\n",
    "        # note: we do not use the results from _80 due to high variance (too small test set)\n",
    "        [  # _80\n",
    "            \"JG1200\",  # D-MPNN+CGR\n",
    "            \"JG1194\",  # XGB+FP+RDKit\n",
    "            \"JG1188\",  # FFN+OHE\n",
    "            \"JG1206\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1286\",  # XGB+FP\n",
    "    ],\n",
    "]\n",
    "\n",
    "sample_counts = {  # mean number of training samples for each split \n",
    "    \"2D_2\": 38,\n",
    "    \"2D_5\": 78,\n",
    "    \"2D_7.5\": 163,\n",
    "    \"2D_10\": 355,\n",
    "    \"2D_15\": 812,\n",
    "    \"2D_20\": 1475,\n",
    "    \"2D_30\": 3193,\n",
    "    \"2D_40\": 6046,\n",
    "    \"2D_60\": 13802,\n",
    "    \"2D_80\": 24562,\n",
    "}\n",
    "\n",
    "chance_level = [get_chance_ap(split) for split in sample_counts.keys()]\n",
    "\n",
    "exps = [i for exp in experiment_ids for i in exp]\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(exps)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(exps, itertools.count()))\n",
    "df_plot_x = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict)).copy()\n",
    "df_plot_x[\"x\"] = df_plot_x[\"tags\"].apply(lambda x: sample_counts[x[0]])\n",
    "\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(4.75,4))\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    data=df_plot_x, \n",
    "    x=\"x\",\n",
    "    y=metric,\n",
    "    palette=palette,\n",
    "    hue_order=order,\n",
    "    style_order=order,\n",
    "    style=\"Model+Features\",\n",
    "    hue=\"Model+Features\",\n",
    "    errorbar=errorbar,\n",
    "    err_style=\"bars\",\n",
    "    dashes=dashes,\n",
    "    linewidth=linewidth,\n",
    "    markers=[\"o\", \"^\", \"s\", \"<\", \"p\"],\n",
    ")\n",
    "\n",
    "sns.lineplot(x=sample_counts.values(), y=chance_level, c=\"black\", label=\"Chance level\")\n",
    "\n",
    "ax.set_xlabel(\"Training data size\")\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.xaxis.set_tick_params(labelrotation=0)\n",
    "ax.set_ylim((0.5, 0.85))\n",
    "ax.set_xticks(\n",
    "    [32*2**n for n in range(10)], \n",
    "    [f\"{32*2**n}\" for n in range(10)],\n",
    ")\n",
    "ax.legend(loc=\"upper left\", title=None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_2D_restricted-data_models_{metric.replace('/', '_')}.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_2D_restricted-data_models_{metric.replace('/', '_')}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scaled using chance level for better comparability across sizes\n",
    "# i.e. we divide by the chance level so that the chance result is now 0 and best is still 1\n",
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"test/avgPrecision_macro\"\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [   \n",
    "        [  # _2\n",
    "            \"JG1246\",  # D-MPNN+CGR\n",
    "            \"JG1245\",  # XGB+FP+RDKit\n",
    "            \"JG1244\",  # FFN+OHE\n",
    "            \"JG1247\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1295\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _5\n",
    "            \"JG1254\",  # D-MPNN+CGR\n",
    "            \"JG1253\",  # XGB+FP+RDKit\n",
    "            \"JG1252\",  # FFN+OHE\n",
    "            \"JG1255\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1294\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _7.5\n",
    "            \"JG1270\",  # D-MPNN+CGR\n",
    "            \"JG1269\",  # XGB+FP+RDKit\n",
    "            \"JG1268\",  # FFN+OHE\n",
    "            \"JG1271\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1293\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _10\n",
    "            \"JG1238\",  # D-MPNN+CGR\n",
    "            \"JG1237\",  # XGB+FP+RDKit\n",
    "            \"JG1236\",  # FFN+OHE\n",
    "            \"JG1239\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1292\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _15\n",
    "            \"JG1205\",  # D-MPNN+CGR\n",
    "            \"JG1199\",  # XGB+FP+RDKit\n",
    "            \"JG1193\",  # FFN+OHE\n",
    "            \"JG1211\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1291\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _20\n",
    "            \"JG1204\",  # D-MPNN+CGR\n",
    "            \"JG1198\",  # XGB+FP+RDKit\n",
    "            \"JG1192\",  # FFN+OHE\n",
    "            \"JG1210\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1290\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _30\n",
    "            \"JG1203\",  # D-MPNN+CGR\n",
    "            \"JG1197\",  # XGB+FP+RDKit\n",
    "            \"JG1191\",  # FFN+OHE\n",
    "            \"JG1209\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1289\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _40\n",
    "            \"JG1202\",  # D-MPNN+CGR\n",
    "            \"JG1196\",  # XGB+FP+RDKit\n",
    "            \"JG1190\",  # FFN+OHE\n",
    "            \"JG1208\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1288\",  # XGB+FP\n",
    "    ],\n",
    "        [  # _60\n",
    "            \"JG1201\",  # D-MPNN+CGR\n",
    "            \"JG1195\",  # XGB+FP+RDKit\n",
    "            \"JG1189\",  # FFN+OHE\n",
    "            \"JG1207\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1287\",  # XGB+FP\n",
    "    ],\n",
    "        # note: we do not use the results from _80 due to high variance (too small test set)\n",
    "        [  # _80\n",
    "            \"JG1200\",  # D-MPNN+CGR\n",
    "            \"JG1194\",  # XGB+FP+RDKit\n",
    "            \"JG1188\",  # FFN+OHE\n",
    "            \"JG1206\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1286\",  # XGB+FP\n",
    "    ],\n",
    "]\n",
    "\n",
    "sample_counts = {  # mean number of training samples for each split \n",
    "    \"2D_2\": 38,\n",
    "    \"2D_5\": 78,\n",
    "    \"2D_7.5\": 163,\n",
    "    \"2D_10\": 355,\n",
    "    \"2D_15\": 812,\n",
    "    \"2D_20\": 1475,\n",
    "    \"2D_30\": 3193,\n",
    "    \"2D_40\": 6046,\n",
    "    \"2D_60\": 13802,\n",
    "    \"2D_80\": 24562,\n",
    "}\n",
    "\n",
    "exps = [i for exp in experiment_ids for i in exp]\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(exps)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(exps, itertools.count()))\n",
    "df_plot_x = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict)).copy()\n",
    "df_plot_x[\"x\"] = df_plot_x[\"tags\"].apply(lambda x: sample_counts[x[0]])\n",
    "\n",
    "# divide by chance level. Note that we obtain the chance level per individual fold so that the SEM still makes sense after scaling.\n",
    "chance_dict = {k: v for k, v in zip(sample_counts.keys(), chance_level)}\n",
    "df_plot_x[f\"{metric}_scaled\"] = (df_plot_x[metric] - df_plot_x.apply(lambda x: get_chance_ap(x[\"tags\"][0], x[\"fold\"], \"test\"), axis=1)) / (1 - df_plot_x.apply(lambda x: get_chance_ap(x[\"tags\"][0], x[\"fold\"], \"test\"), axis=1))\n",
    "\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(4.75,4))\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    data=df_plot_x, \n",
    "    x=\"x\",\n",
    "    y=f\"{metric}_scaled\",\n",
    "    palette=palette,\n",
    "    hue_order=order,\n",
    "    style_order=order,\n",
    "    style=\"Model+Features\",\n",
    "    hue=\"Model+Features\",\n",
    "    errorbar=errorbar,\n",
    "    err_style=\"bars\",\n",
    "    dashes=dashes,\n",
    "    linewidth=linewidth,\n",
    "    markers=[\"o\", \"^\", \"s\", \"<\", \"p\"],\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Training data size\")\n",
    "ax.set_ylabel(\"AUPRC (relative improvement over chance)\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.xaxis.set_tick_params(labelrotation=0)\n",
    "ax.set_ylim((0., 0.85))\n",
    "ax.set_xticks(\n",
    "    [32*2**n for n in range(11)], \n",
    "    [f\"{32*2**n}\" for n in range(11)],\n",
    ")\n",
    "ax.legend(loc=\"upper left\", title=None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_2D_restricted-data_models_{metric.replace('/', '_')}_relative.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_2D_restricted-data_models_{metric.replace('/', '_')}_relative.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D restricted data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"val/avgPrecision_macro\"\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "        [  # _10\n",
    "            \"JG1242\",  # D-MPNN+CGR\n",
    "            \"JG1241\",  # XGB+FP+RDKit\n",
    "            \"JG1240\",  # FFN+OHE\n",
    "            \"JG1243\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1285\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _15\n",
    "            \"JG1274\",  # D-MPNN+CGR\n",
    "            \"JG1273\",  # XGB+FP+RDKit\n",
    "            \"JG1272\",  # FFN+OHE\n",
    "            \"JG1275\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1284\",  # XGB/FP\n",
    "            \n",
    "    ],\n",
    "        [  # _20\n",
    "            \"JG1250\",  # D-MPNN+CGR\n",
    "            \"JG1249\",  # XGB+FP+RDKit\n",
    "            \"JG1248\",  # FFN+OHE\n",
    "            \"JG1251\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1283\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _25\n",
    "            \"JG1262\",  # D-MPNN+CGR\n",
    "            \"JG1261\",  # XGB+FP+RDKit\n",
    "            \"JG1260\",  # FFN+OHE\n",
    "            \"JG1263\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1282\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _30\n",
    "            \"JG1229\",  # D-MPNN+CGR\n",
    "            \"JG1223\",  # XGB+FP+RDKit\n",
    "            \"JG1217\",  # FFN+OHE\n",
    "            \"JG1235\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1281\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _34\n",
    "            \"JG1228\",  # D-MPNN+CGR\n",
    "            \"JG1222\",  # XGB+FP+RDKit\n",
    "            \"JG1216\",  # FFN+OHE\n",
    "            \"JG1234\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1280\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _40\n",
    "            \"JG1227\",  # D-MPNN+CGR\n",
    "            \"JG1221\",  # XGB+FP+RDKit\n",
    "            \"JG1215\",  # FFN+OHE\n",
    "            \"JG1233\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1279\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _50\n",
    "            \"JG1226\",  # D-MPNN+CGR\n",
    "            \"JG1220\",  # XGB+FP+RDKit\n",
    "            \"JG1214\",  # FFN+OHE\n",
    "            \"JG1232\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1278\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _60\n",
    "            \"JG1225\",  # D-MPNN+CGR\n",
    "            \"JG1219\",  # XGB+FP+RDKit\n",
    "            \"JG1213\",  # FFN+OHE\n",
    "            \"JG1231\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1277\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _70\n",
    "            \"JG1224\",  # D-MPNN+CGR\n",
    "            \"JG1218\",  # XGB+FP+RDKit\n",
    "            \"JG1212\",  # FFN+OHE\n",
    "            \"JG1230\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1276\",  # XGB/FP\n",
    "    ],\n",
    "]\n",
    "\n",
    "sample_counts = {  # mean number of training samples for each split \n",
    "    \"3D_10\": 32,\n",
    "    \"3D_15\": 112,\n",
    "    \"3D_20\": 273,\n",
    "    \"3D_25\": 529,\n",
    "    \"3D_30\": 901,\n",
    "    \"3D_34\": 1313,\n",
    "    \"3D_40\": 2399,\n",
    "    \"3D_50\": 4747,\n",
    "    \"3D_60\": 7948,\n",
    "    \"3D_70\": 12975,\n",
    "}\n",
    "\n",
    "chance_level = [0.554, 0.555, 0.548, 0.557, 0.562, 0.565, 0.581, 0.551, 0.578, 0.570]  # same order as sample_counts\n",
    "\n",
    "\n",
    "exps = [i for exp in experiment_ids for i in exp]\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(exps)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(exps, itertools.count()))\n",
    "df_plot_x = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict)).copy()\n",
    "df_plot_x[\"x\"] = df_plot_x[\"tags\"].apply(lambda x: sample_counts[x[0]])\n",
    "\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(4.75,4))\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    data=df_plot_x, \n",
    "    x=\"x\",\n",
    "    y=metric,\n",
    "    palette=palette,\n",
    "    style=\"Model+Features\",\n",
    "    hue=\"Model+Features\",\n",
    "    hue_order=order,\n",
    "    style_order=order,\n",
    "    errorbar=errorbar,\n",
    "    err_style=\"bars\",\n",
    "    dashes=dashes,\n",
    "    linewidth=linewidth,\n",
    "    markers=[\"o\", \"^\", \"s\", \"<\", \"p\"],\n",
    ")\n",
    "\n",
    "sns.lineplot(x=sample_counts.values(), y=chance_level, c=\"black\", label=\"Chance level\")\n",
    "\n",
    "ax.set_xlabel(\"Training data size\")\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.xaxis.set_tick_params(labelrotation=0)\n",
    "ax.set_ylim((0.5, 0.9))\n",
    "ax.set_xticks(\n",
    "    [32*2**n for n in range(10)], \n",
    "    [f\"{32*2**n}\" for n in range(10)],\n",
    ")\n",
    "ax.legend(loc=\"upper left\", title=None)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_3D_restricted-data_models_{metric.replace('/', '_')}.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_3D_restricted-data_models_{metric.replace('/', '_')}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"test/avgPrecision_macro\"\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "        [  # _10\n",
    "            \"JG1242\",  # D-MPNN+CGR\n",
    "            \"JG1241\",  # XGB+FP+RDKit\n",
    "            \"JG1240\",  # FFN+OHE\n",
    "            \"JG1243\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1285\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _15\n",
    "            \"JG1274\",  # D-MPNN+CGR\n",
    "            \"JG1273\",  # XGB+FP+RDKit\n",
    "            \"JG1272\",  # FFN+OHE\n",
    "            \"JG1275\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1284\",  # XGB/FP\n",
    "            \n",
    "    ],\n",
    "        [  # _20\n",
    "            \"JG1250\",  # D-MPNN+CGR\n",
    "            \"JG1249\",  # XGB+FP+RDKit\n",
    "            \"JG1248\",  # FFN+OHE\n",
    "            \"JG1251\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1283\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _25\n",
    "            \"JG1262\",  # D-MPNN+CGR\n",
    "            \"JG1261\",  # XGB+FP+RDKit\n",
    "            \"JG1260\",  # FFN+OHE\n",
    "            \"JG1263\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1282\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _30\n",
    "            \"JG1229\",  # D-MPNN+CGR\n",
    "            \"JG1223\",  # XGB+FP+RDKit\n",
    "            \"JG1217\",  # FFN+OHE\n",
    "            \"JG1235\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1281\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _34\n",
    "            \"JG1228\",  # D-MPNN+CGR\n",
    "            \"JG1222\",  # XGB+FP+RDKit\n",
    "            \"JG1216\",  # FFN+OHE\n",
    "            \"JG1234\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1280\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _40\n",
    "            \"JG1227\",  # D-MPNN+CGR\n",
    "            \"JG1221\",  # XGB+FP+RDKit\n",
    "            \"JG1215\",  # FFN+OHE\n",
    "            \"JG1233\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1279\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _50\n",
    "            \"JG1226\",  # D-MPNN+CGR\n",
    "            \"JG1220\",  # XGB+FP+RDKit\n",
    "            \"JG1214\",  # FFN+OHE\n",
    "            \"JG1232\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1278\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _60\n",
    "            \"JG1225\",  # D-MPNN+CGR\n",
    "            \"JG1219\",  # XGB+FP+RDKit\n",
    "            \"JG1213\",  # FFN+OHE\n",
    "            \"JG1231\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1277\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _70\n",
    "            \"JG1224\",  # D-MPNN+CGR\n",
    "            \"JG1218\",  # XGB+FP+RDKit\n",
    "            \"JG1212\",  # FFN+OHE\n",
    "            \"JG1230\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1276\",  # XGB/FP\n",
    "    ],\n",
    "]\n",
    "\n",
    "sample_counts = {  # mean number of training samples for each split \n",
    "    \"3D_10\": 32,\n",
    "    \"3D_15\": 112,\n",
    "    \"3D_20\": 273,\n",
    "    \"3D_25\": 529,\n",
    "    \"3D_30\": 901,\n",
    "    \"3D_34\": 1313,\n",
    "    \"3D_40\": 2399,\n",
    "    \"3D_50\": 4747,\n",
    "    \"3D_60\": 7948,\n",
    "    \"3D_70\": 12975,\n",
    "}\n",
    "\n",
    "chance_level = [0.553, 0.554, 0.554, 0.546, 0.564, 0.543, 0.543, 0.548, 0.534, 0.519]  # same order as sample_counts\n",
    "\n",
    "\n",
    "exps = [i for exp in experiment_ids for i in exp]\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(exps)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(exps, itertools.count()))\n",
    "df_plot_x = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict)).copy()\n",
    "df_plot_x[\"x\"] = df_plot_x[\"tags\"].apply(lambda x: sample_counts[x[0]])\n",
    "\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(4.75,4))\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    data=df_plot_x, \n",
    "    x=\"x\",\n",
    "    y=metric,\n",
    "    palette=palette,\n",
    "    hue_order=order,\n",
    "    style_order=order,\n",
    "    style=\"Model+Features\",\n",
    "    hue=\"Model+Features\",\n",
    "    dashes=dashes,\n",
    "    errorbar=errorbar,\n",
    "    err_style=\"bars\",\n",
    "    linewidth=linewidth,\n",
    "    markers=[\"o\", \"^\", \"s\", \"<\", \"p\"],\n",
    ")\n",
    "\n",
    "sns.lineplot(x=sample_counts.values(), y=chance_level, c=\"black\", label=\"Chance level\")\n",
    "\n",
    "ax.set_xlabel(\"Training data size\")\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.xaxis.set_tick_params(labelrotation=0)\n",
    "ax.set_ylim((0.5, 0.85))\n",
    "ax.set_xticks(\n",
    "    [32*2**n for n in range(10)], \n",
    "    [f\"{32*2**n}\" for n in range(10)],\n",
    ")\n",
    "ax.legend(loc=\"upper left\", title=None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_3D_restricted-data_models_{metric.replace('/', '_')}.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_3D_restricted-data_models_{metric.replace('/', '_')}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repetition of the last plot, but with number of seen building blocks on the x axis instead of training samples\n",
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"test/avgPrecision_macro\"\n",
    "\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "        [  # _10\n",
    "            \"JG1242\",  # D-MPNN+CGR\n",
    "            \"JG1241\",  # XGB+FP+RDKit\n",
    "            \"JG1240\",  # FFN+OHE\n",
    "            \"JG1243\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1285\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _15\n",
    "            \"JG1274\",  # D-MPNN+CGR\n",
    "            \"JG1273\",  # XGB+FP+RDKit\n",
    "            \"JG1272\",  # FFN+OHE\n",
    "            \"JG1275\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1284\",  # XGB/FP\n",
    "            \n",
    "    ],\n",
    "        [  # _20\n",
    "            \"JG1250\",  # D-MPNN+CGR\n",
    "            \"JG1249\",  # XGB+FP+RDKit\n",
    "            \"JG1248\",  # FFN+OHE\n",
    "            \"JG1251\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1283\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _25\n",
    "            \"JG1262\",  # D-MPNN+CGR\n",
    "            \"JG1261\",  # XGB+FP+RDKit\n",
    "            \"JG1260\",  # FFN+OHE\n",
    "            \"JG1263\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1282\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _30\n",
    "            \"JG1229\",  # D-MPNN+CGR\n",
    "            \"JG1223\",  # XGB+FP+RDKit\n",
    "            \"JG1217\",  # FFN+OHE\n",
    "            \"JG1235\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1281\",  # XGB/FP\n",
    "    ],\n",
    "   #     [  # _34  # here we leave this one out as it is very close to _40 on the X-axis\n",
    "   #         \"JG1228\",  # D-MPNN+CGR\n",
    "   #         \"JG1222\",  # XGB+FP+RDKit\n",
    "   #         \"JG1216\",  # FFN+OHE\n",
    "   #         \"JG1234\",  # D-MPNN+CGR+RDKit\n",
    "   #         \"JG1280\",  # XGB/FP\n",
    "   # ],\n",
    "        [  # _40\n",
    "            \"JG1227\",  # D-MPNN+CGR\n",
    "            \"JG1221\",  # XGB+FP+RDKit\n",
    "            \"JG1215\",  # FFN+OHE\n",
    "            \"JG1233\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1279\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _50\n",
    "            \"JG1226\",  # D-MPNN+CGR\n",
    "            \"JG1220\",  # XGB+FP+RDKit\n",
    "            \"JG1214\",  # FFN+OHE\n",
    "            \"JG1232\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1278\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _60\n",
    "            \"JG1225\",  # D-MPNN+CGR\n",
    "            \"JG1219\",  # XGB+FP+RDKit\n",
    "            \"JG1213\",  # FFN+OHE\n",
    "            \"JG1231\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1277\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _70\n",
    "            \"JG1224\",  # D-MPNN+CGR\n",
    "            \"JG1218\",  # XGB+FP+RDKit\n",
    "            \"JG1212\",  # FFN+OHE\n",
    "            \"JG1230\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1276\",  # XGB/FP\n",
    "    ],\n",
    "]\n",
    "\n",
    "sample_counts = {  # mean number of unique building blocks in training split (sum of all three types)\n",
    "    \"3D_10\": 15.8,\n",
    "    \"3D_15\": 25.9,\n",
    "    \"3D_20\": 34.3,\n",
    "    \"3D_25\": 42.9,\n",
    "    \"3D_30\": 51.3,\n",
    "    \"3D_34\": 68.3,\n",
    "    \"3D_40\": 69.8,\n",
    "    \"3D_50\": 88.6,\n",
    "    \"3D_60\": 104.4,\n",
    "    \"3D_70\": 123.4,\n",
    "}\n",
    "\n",
    "chance_level = [0.553, 0.554, 0.554, 0.546, 0.564, 0.543, 0.543, 0.548, 0.534, 0.519]  # same order as sample_counts\n",
    "\n",
    "\n",
    "exps = [i for exp in experiment_ids for i in exp]\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(exps)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(exps, itertools.count()))\n",
    "df_plot_x = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict)).copy()\n",
    "df_plot_x[\"x\"] = df_plot_x[\"tags\"].apply(lambda x: sample_counts[x[0]])\n",
    "\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(4.75,4))\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    data=df_plot_x, \n",
    "    x=\"x\",\n",
    "    y=metric,\n",
    "    palette=palette,\n",
    "    hue_order=order,\n",
    "    style_order=order,\n",
    "    style=\"Model+Features\",\n",
    "    hue=\"Model+Features\",\n",
    "    dashes=dashes,\n",
    "    errorbar=errorbar,\n",
    "    err_style=\"bars\",\n",
    "    linewidth=linewidth,\n",
    "    markers=[\"o\", \"^\", \"s\", \"<\", \"p\"],\n",
    ")\n",
    "\n",
    "sns.lineplot(x=sample_counts.values(), y=chance_level, c=\"black\", label=\"Chance level\")\n",
    "\n",
    "ax.set_xlabel(\"Training data unique building blocks\")\n",
    "ax.set_ylabel(\"AUPRC\")\n",
    "#ax.set_xscale(\"log\")\n",
    "ax.xaxis.set_tick_params(labelrotation=0)\n",
    "ax.set_ylim((0.5, 0.85))\n",
    "#ax.set_xticks(\n",
    "#    [32*2**n for n in range(10)], \n",
    "#    [f\"{32*2**n}\" for n in range(10)],\n",
    "#)\n",
    "ax.legend(loc=\"center right\", title=None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_3D_restricted-data_models_{metric.replace('/', '_')}.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_3D_restricted-data_models_{metric.replace('/', '_')}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scaled using chance level for better comparability across sizes\n",
    "# i.e. we divide by the chance level so that the chance result is now 0 and best is still 1\n",
    "# choose the metric, i.e. accuracy or AUROC\n",
    "metric = \"test/avgPrecision_macro\"\n",
    "\n",
    "# choose the data to plot\n",
    "# choose the data to plot\n",
    "experiment_ids = [\n",
    "        [  # _10\n",
    "            \"JG1242\",  # D-MPNN+CGR\n",
    "            \"JG1241\",  # XGB+FP+RDKit\n",
    "            \"JG1240\",  # FFN+OHE\n",
    "            \"JG1243\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1285\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _15\n",
    "            \"JG1274\",  # D-MPNN+CGR\n",
    "            \"JG1273\",  # XGB+FP+RDKit\n",
    "            \"JG1272\",  # FFN+OHE\n",
    "            \"JG1275\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1284\",  # XGB/FP\n",
    "            \n",
    "    ],\n",
    "        [  # _20\n",
    "            \"JG1250\",  # D-MPNN+CGR\n",
    "            \"JG1249\",  # XGB+FP+RDKit\n",
    "            \"JG1248\",  # FFN+OHE\n",
    "            \"JG1251\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1283\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _25\n",
    "            \"JG1262\",  # D-MPNN+CGR\n",
    "            \"JG1261\",  # XGB+FP+RDKit\n",
    "            \"JG1260\",  # FFN+OHE\n",
    "            \"JG1263\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1282\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _30\n",
    "            \"JG1229\",  # D-MPNN+CGR\n",
    "            \"JG1223\",  # XGB+FP+RDKit\n",
    "            \"JG1217\",  # FFN+OHE\n",
    "            \"JG1235\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1281\",  # XGB/FP\n",
    "    ],\n",
    "   #     [  # _34  # here we leave this one out as it is very close to _40 on the X-axis\n",
    "   #         \"JG1228\",  # D-MPNN+CGR\n",
    "   #         \"JG1222\",  # XGB+FP+RDKit\n",
    "   #         \"JG1216\",  # FFN+OHE\n",
    "   #         \"JG1234\",  # D-MPNN+CGR+RDKit\n",
    "   #         \"JG1280\",  # XGB/FP\n",
    "   # ],\n",
    "        [  # _40\n",
    "            \"JG1227\",  # D-MPNN+CGR\n",
    "            \"JG1221\",  # XGB+FP+RDKit\n",
    "            \"JG1215\",  # FFN+OHE\n",
    "            \"JG1233\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1279\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _50\n",
    "            \"JG1226\",  # D-MPNN+CGR\n",
    "            \"JG1220\",  # XGB+FP+RDKit\n",
    "            \"JG1214\",  # FFN+OHE\n",
    "            \"JG1232\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1278\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _60\n",
    "            \"JG1225\",  # D-MPNN+CGR\n",
    "            \"JG1219\",  # XGB+FP+RDKit\n",
    "            \"JG1213\",  # FFN+OHE\n",
    "            \"JG1231\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1277\",  # XGB/FP\n",
    "    ],\n",
    "        [  # _70\n",
    "            \"JG1224\",  # D-MPNN+CGR\n",
    "            \"JG1218\",  # XGB+FP+RDKit\n",
    "            \"JG1212\",  # FFN+OHE\n",
    "            \"JG1230\",  # D-MPNN+CGR+RDKit\n",
    "            \"JG1276\",  # XGB/FP\n",
    "    ],\n",
    "]\n",
    "\n",
    "sample_counts = {  # mean number of training samples for each split \n",
    "    \"3D_10\": 32,\n",
    "    \"3D_15\": 112,\n",
    "    \"3D_20\": 273,\n",
    "    \"3D_25\": 529,\n",
    "    \"3D_30\": 901,\n",
    "    \"3D_34\": 1313,\n",
    "    \"3D_40\": 2399,\n",
    "    \"3D_50\": 4747,\n",
    "    \"3D_60\": 7948,\n",
    "    \"3D_70\": 12975,\n",
    "}\n",
    "\n",
    "exps = [i for exp in experiment_ids for i in exp]\n",
    "# filter the data\n",
    "df_plot = df_all.loc[df_all['experiment_id'].isin(exps)]\n",
    "\n",
    "# sort the values\n",
    "sort_dict = dict(zip(exps, itertools.count()))\n",
    "df_plot_x = df_plot.sort_values(by=\"experiment_id\", kind=\"mergesort\", key=lambda x: x.map(sort_dict)).copy()\n",
    "df_plot_x[\"x\"] = df_plot_x[\"tags\"].apply(lambda x: sample_counts[x[0]])\n",
    "\n",
    "# divide by chance level. Note that we obtain the chance level per individual fold so that the SEM still makes sense after scaling.\n",
    "chance_dict = {k: v for k, v in zip(sample_counts.keys(), chance_level)}\n",
    "df_plot_x[f\"{metric}_scaled\"] = (df_plot_x[metric] - df_plot_x.apply(lambda x: get_chance_ap(x[\"tags\"][0], x[\"fold\"], \"test\"), axis=1)) / (1 - df_plot_x.apply(lambda x: get_chance_ap(x[\"tags\"][0], x[\"fold\"], \"test\"), axis=1))\n",
    "\n",
    "# set plot\n",
    "fig, ax = plt.subplots(figsize=(4.75,4))\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    data=df_plot_x, \n",
    "    x=\"x\",\n",
    "    y=f\"{metric}_scaled\",\n",
    "    palette=palette,\n",
    "    hue_order=order,\n",
    "    style_order=order,\n",
    "    style=\"Model+Features\",\n",
    "    hue=\"Model+Features\",\n",
    "    errorbar=errorbar,\n",
    "    err_style=\"bars\",\n",
    "    dashes=dashes,\n",
    "    linewidth=linewidth,\n",
    "    markers=[\"o\", \"^\", \"s\", \"<\", \"p\"],\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Training data size\")\n",
    "ax.set_ylabel(\"AUPRC (relative improvement over chance)\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.xaxis.set_tick_params(labelrotation=0)\n",
    "ax.set_ylim((0., 0.85))\n",
    "ax.set_xticks(\n",
    "    [32*2**n for n in range(11)], \n",
    "    [f\"{32*2**n}\" for n in range(11)],\n",
    ")\n",
    "ax.legend(loc=\"upper left\", title=None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / f\"metrics_3D_restricted-data_models_{metric.replace('/', '_')}_relative.svg\", format=\"svg\")\n",
    "fig.savefig(analysis_dir / f\"metrics_3D_restricted-data_models_{metric.replace('/', '_')}_relative.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
