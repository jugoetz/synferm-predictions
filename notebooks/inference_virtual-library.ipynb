{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Inference using the Synthetic Fermentation models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In principle, this notebook works the same as `inference.ipynb`, but we keep it separate for reproducibility of the virtual library predictions and also because here we will do some specific steps like I/O directly to the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import statistics\n",
    "import sys\n",
    "import sqlite3\n",
    "sys.path.append(str(pathlib.Path(\"__file__\").absolute().parents[1]))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.util.definitions import TRAINED_MODEL_DIR, LOG_DIR, DATA_ROOT\n",
    "from src.model.classifier import load_trained_model\n",
    "from src.data.dataloader import SynFermDataset, collate_fn\n",
    "from reaction_generator import SFReactionGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to the best models\n",
    "model_0D_name = \"2024-01-04-085409_305115_fold0\"\n",
    "model_0D = TRAINED_MODEL_DIR / model_0D_name / \"last-epoch72-val_loss0.19.ckpt\"  # FFN\n",
    "# path to the OneHotEncoder state for model_0D\n",
    "ohe_state_dict = LOG_DIR / \"OHE_state_dict_ohlvinnXkSzSXBJi.json\"\n",
    "assert model_0D.is_file()\n",
    "assert ohe_state_dict.is_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "To use the notebook on your products, change `raw_dir` to the directory that your CSV file containing SMILES is in. Then change `filename_base` to the filename of your csv file without the `.csv` suffix. If you do not want to use all the SMILES in your file (e.g. because some are not valid SLAP products), suppy a `valid_idx_file`. You can set the value to `None` if you want to use all SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import product SMILES\n",
    "raw_dir = DATA_ROOT\n",
    "dbname = \"50k_project.db\"\n",
    "con = sqlite3.connect(DATA_ROOT / dbname)\n",
    "# n.b. we just select everything now, later filter for things that the 0D model cannot handle.\n",
    "# Here we don't care whether the reaction has been seen before, we can merge with the reaction data later\n",
    "res = con.execute(\"SELECT id, long_name, SMILES FROM virtuallibrary WHERE type = 'A';\").fetchall()\n",
    "df = pd.DataFrame(res, columns=[\"vl_id\", \"long_name\", \"product_A_smiles\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all these were actually in the training data for the model we use\n",
    "dfs = [pd.read_csv(TRAINED_MODEL_DIR / model_0D_name / f\"train_{bb}.csv\") for bb in [\"initiators\", \"monomers\", \"terminators\"]]\n",
    "used_building_blocks = pd.concat(dfs)\n",
    "used_building_blocks    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused building blocks will not be recognized by the one-hot encoder, so we filter for that\n",
    "df = df.loc[df[\"long_name\"].str.split(\"+\").apply(lambda x: all([i.strip() in used_building_blocks[\"long\"].values for i in x]))]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = SFReactionGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df[0:100][\"product_A_smiles\"].apply(lambda x: gen.get_reaction_smiles(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to generate the reactionSMILES. This will take a moment \n",
    "# (to be precise I expect it to take a bit over 2h. Of course one could optimize or parallelize but I'm going home now so who cares)\n",
    "df[\"reaction_smiles_atom_mapped\"] = df[\"product_A_smiles\"].apply(lambda x: gen.get_reaction_smiles(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this\n",
    "df.to_csv(DATA_ROOT / \"virtual-library_reactionSMILES.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate data set. This will also take a moment\n",
    "data = SynFermDataset(\n",
    "    name=\"virtual-library_reactionSMILES.csv\",\n",
    "    raw_dir=DATA_ROOT,\n",
    "    reaction=True,\n",
    "    global_features=[\"OHE\", ],\n",
    "    global_featurizer_state_dict_path=ohe_state_dict,\n",
    "    graph_type=\"bond_edges\",\n",
    "    featurizers=\"custom\",\n",
    "    smiles_columns=[\"reaction_smiles_atom_mapped\"],\n",
    "    label_columns=None,\n",
    "    task=\"multilabel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the predictions\n",
    "\n",
    "# load the trained model \n",
    "model_0D = load_trained_model(\"FFN\", model_0D)\n",
    "model_0D.eval()\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", logger=False, max_epochs=-1)\n",
    "# prepare data\n",
    "dl = DataLoader(data, collate_fn=collate_fn, num_workers=0)\n",
    "# predict\n",
    "probabilities_0D = torch.sigmoid(torch.concat(trainer.predict(model_0D, dl)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_0D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load decision thresholds\n",
    "with open(LOG_DIR / \"thresholds\" / f\"{model_0D_name}.txt\", \"r\") as f:\n",
    "    thresholds = [float(i) for i in f.readlines()]\n",
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the thresholds\n",
    "preds = torch.stack([torch.where(probabilities_0D[:, i] > thresholds[i], 1, 0) for i in range(3)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine with data\n",
    "df[[\"prob_A\", \"prob_B\", \"prob_C\"]] = probabilities_0D\n",
    "df[[\"pred_A\", \"pred_B\", \"pred_C\"]] = preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize our predictions\n",
    "df[\"pred_A\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"prob_A\"].plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write df to output file\n",
    "df.to_csv(DATA_ROOT / f\"virtual-library_predictions_2023-12-20.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_ids = defaultdict(dict)\n",
    "longnames = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mapping between vl_ids and longnames from db\n",
    "res = con.execute(\"SELECT id, long_name, type FROM virtuallibrary\").fetchall()\n",
    "for row in res:\n",
    "    vl_ids[row[1]][row[2]] = row[0]\n",
    "    if row[2] == \"A\":\n",
    "        longnames[row[0]] = row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to db\n",
    "cur = con.cursor()\n",
    "for i, row in df[[\"vl_id\", \"pred_A\", \"pred_B\", \"pred_C\"]].iterrows():\n",
    "    # get vl_id for B and C\n",
    "    other_ids = vl_ids[longnames[int(row[\"vl_id\"])]]\n",
    "    cur.execute(\"INSERT INTO virtuallibrary_predictions (vl_id, binary_outcome, binary_model) VALUES (?, ?, ?);\",\n",
    "               (row[\"vl_id\"].item(), row[\"pred_A\"].item(), model_0D_name))\n",
    "    cur.execute(\"INSERT INTO virtuallibrary_predictions (vl_id, binary_outcome, binary_model) VALUES (?, ?, ?);\",\n",
    "               (other_ids[\"B\"], row[\"pred_B\"].item(), model_0D_name))\n",
    "    cur.execute(\"INSERT INTO virtuallibrary_predictions (vl_id, binary_outcome, binary_model) VALUES (?, ?, ?);\",\n",
    "               (other_ids[\"C\"], row[\"pred_C\"].item(), model_0D_name))\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
