{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Analyze decision threshold\n",
    "We notice that some models always predict 1 for binary_A. We try modifiying the decision threshold and see the impact on validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "sys.path.append(str(pathlib.Path().absolute().parent))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, accuracy_score, balanced_accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, fbeta_score, PrecisionRecallDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "from src.util.definitions import PRED_DIR, DATA_ROOT, LOG_DIR\n",
    "from src.util.io import read_predictions\n",
    "from ghost import optimize_threshold_from_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api(timeout=59)\n",
    "\n",
    "runs = api.runs(\"jugoetz/synferm-predictions\", filters={\"group\": \"2023-12-20-202602_330364\"})  # best 0D model JG1309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get run info\n",
    "config_list = [run.config for run in runs]\n",
    "summary_list = [run.summary._json_dict for run in runs]\n",
    "name_list = [run.name for run in runs]\n",
    "\n",
    "name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train predictions\n",
    "preds = [read_predictions(n, \"train\") for n in name_list]\n",
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine with ground truth\n",
    "df_true = pd.read_csv(DATA_ROOT / \"synferm_dataset_2023-12-20_39486records.csv\")\n",
    "comb = [pred.merge(df_true[[\"binary_A\", \"binary_B\", \"binary_C\"]], left_index=True, right_index=True) for pred in preds]\n",
    "comb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate thresholded metrics over many thresholds\n",
    "# and obtain best threshold with GHOST with Cohen's Kappa method\n",
    "bal_acc = []\n",
    "f_beta = []\n",
    "x = np.arange(0, 1.0001, 0.01)\n",
    "y_bal_acc = np.empty((101, 9))\n",
    "y_f_beta = np.empty((101, 9))\n",
    "best_thresh = np.empty(9)\n",
    "for fold in range(9):\n",
    "    for i, threshold in enumerate(x):\n",
    "        y_bal_acc[i, fold] = balanced_accuracy_score(comb[fold][\"binary_A\"], (comb[fold][\"pred_0\"] > threshold))\n",
    "        y_f_beta[i, fold] = fbeta_score(comb[fold][\"binary_A\"], (comb[fold][\"pred_0\"] > threshold), beta=0.1)\n",
    "    best_thresh[fold] = optimize_threshold_from_predictions(comb[fold][\"binary_A\"], comb[fold][\"pred_0\"], thresholds=x, ThOpt_metrics=\"Kappa\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain best threshold with GHOST with AUROC method\n",
    "best_thresh_roc = np.empty(9)\n",
    "for fold in range(9):\n",
    "    best_thresh_roc[fold] = optimize_threshold_from_predictions(comb[fold][\"binary_A\"], comb[fold][\"pred_0\"], thresholds=x, ThOpt_metrics=\"ROC\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresh_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot best thresholds from GHOST with metric value by threshold\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "for i, fold in enumerate(name_list):\n",
    "    line = ax.plot(x, y_bal_acc[:, i], label=fold[-1])\n",
    "    ax.vlines(x=best_thresh[i], ymin=0, ymax=y_bal_acc[int((best_thresh[i] * 100).round()), i], ls=\":\", colors=line[0].get_color(), lw=1.2)\n",
    "    ax.vlines(x=best_thresh_roc[i], ymin=0, ymax=y_bal_acc[int((best_thresh_roc[i] * 100).round()), i], ls=\"--\", colors=line[0].get_color(), lw=1.2)\n",
    "\n",
    "ax.set_xlabel(\"Decision boundary\")\n",
    "ax.set_ylabel(\"Balanced accuracy\")\n",
    "ax.set_xlim(0,1)\n",
    "ax.set_ylim(0.4,1)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Maximize balanced accuracy\n",
    "\n",
    "Simple way to pick the decision threshold is to calculate the balanced accuracy over different thresholds and pick the one that maximizes it.\n",
    "\n",
    "Note:\n",
    "- Since this is average recall per class, for our problem, this heavily tilts the predictions to minimize false negatives. That also means, recall for the positive class will take a heavy hit\n",
    "- Picking the maximum can be a bit unstable if the balanced_accuracy/threshold curve is not so smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try maximizing balanced accuracy on all folds\n",
    "for fold in range(9):\n",
    "    threshold = np.argmax(y_bal_acc[:, fold]) / 100\n",
    "    y_true = comb[fold][\"binary_A\"]\n",
    "    y_prob = comb[fold][\"pred_0\"]\n",
    "    y_pred = (y_prob > threshold)\n",
    "    print(\"balanced accuracy:\", f\"{balanced_accuracy_score(y_true, y_pred):.2f}\")\n",
    "    print(\"recall:\", f\"{recall_score(y_true, y_pred):.2f}\")\n",
    "    print(\"precision:\", f\"{precision_score(y_true, y_pred):.2f}\")\n",
    "    print(\"f_0.5 score:\", f\"{fbeta_score(y_true, y_pred, beta=0.5):.2f}\")\n",
    "    print()\n",
    "\n",
    "# plot the PRC\n",
    "PrecisionRecallDisplay.from_predictions(y_true, y_prob, pos_label=1, drop_intermediate=True, plot_chance_level=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### GHOST\n",
    "The Rinicker lab has published GHOST (1) for decision threshold picking.\n",
    "On a high level this:\n",
    "- Takes startified samples from the training set (default: without replacement)\n",
    "- Determines metric (default: Cohen's kappa) for all decision thresholds\n",
    "- Calculates the median metric over stratified samples for all decision thresholds\n",
    "- Returns the threshold with highest median metric\n",
    "\n",
    "(1) Esposito, C.; A. Landrum, G.; Schneider, N.; Stiefl, N.; Riniker, S. GHOST: Adjusting the Decision Threshold to Handle Imbalanced Data in Machine Learning. Journal of Chemical Information and Modeling 2021, 61 (6), 2623–2640. https://doi.org/10.1021/acs.jcim.1c00160."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate GHOST on all folds (training data)\n",
    "for i in range(9):\n",
    "    threshold = best_thresh[i]\n",
    "    y_true = comb[i][\"binary_A\"]\n",
    "    y_prob = comb[i][\"pred_0\"]\n",
    "    y_pred = (y_prob > threshold)\n",
    "    print(\"balanced accuracy:\", f\"{balanced_accuracy_score(y_true, y_pred):.2f}\")\n",
    "    print(\"recall:\", f\"{recall_score(y_true, y_pred):.2f}\")\n",
    "    print(\"precision:\", f\"{precision_score(y_true, y_pred):.2f}\")\n",
    "    print(\"f_0.5 score:\", f\"{fbeta_score(y_true, y_pred, beta=0.5):.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at validation set metrics\n",
    "# read val predictions\n",
    "preds_val = [read_predictions(n, \"val\") for n in name_list]\n",
    "preds_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine with ground truth\n",
    "comb_val = [pred.merge(df_true[[\"binary_A\", \"binary_B\", \"binary_C\"]], left_index=True, right_index=True) for pred in preds_val]\n",
    "comb_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold obtained by GHOST with Kappa\n",
    "acc, bal_acc, recall, precision, fbeta = [], [], [], [], []\n",
    "\n",
    "for fold in range(9):\n",
    "    threshold = best_thresh[fold]\n",
    "    y_true = comb_val[fold][\"binary_A\"]\n",
    "    y_prob = comb_val[fold][\"pred_0\"]\n",
    "    y_pred = (y_prob > threshold)\n",
    "    acc.append(accuracy_score(y_true, y_pred))\n",
    "    bal_acc.append(balanced_accuracy_score(y_true, y_pred))\n",
    "    recall.append(recall_score(y_true, y_pred))\n",
    "    precision.append(precision_score(y_true, y_pred))\n",
    "    fbeta.append(fbeta_score(y_true, y_pred, beta=0.5))\n",
    "\n",
    "print(\"accuracy:\", f\"{np.array(acc).mean():.2f}±{np.array(acc).std():.2f}\")\n",
    "print(\"balanced accuracy:\", f\"{np.array(bal_acc).mean():.2f}±{np.array(bal_acc).std():.2f}\")\n",
    "print(\"recall:\", f\"{np.array(recall).mean():.2f}±{np.array(recall).std():.2f}\")\n",
    "print(\"precision:\", f\"{np.array(precision).mean():.2f}±{np.array(precision).std():.2f}\")\n",
    "print(\"f_0.5 score:\", f\"{np.array(fbeta).mean():.2f}±{np.array(fbeta).std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold obtained by GHOST with ROC\n",
    "acc, bal_acc, recall, precision, fbeta = [], [], [], [], []\n",
    "\n",
    "for fold in range(9):\n",
    "    threshold = best_thresh_roc[fold]\n",
    "    y_true = comb_val[fold][\"binary_A\"]\n",
    "    y_prob = comb_val[fold][\"pred_0\"]\n",
    "    y_pred = (y_prob > threshold)\n",
    "    acc.append(accuracy_score(y_true, y_pred))\n",
    "    bal_acc.append(balanced_accuracy_score(y_true, y_pred))\n",
    "    recall.append(recall_score(y_true, y_pred))\n",
    "    precision.append(precision_score(y_true, y_pred))\n",
    "    fbeta.append(fbeta_score(y_true, y_pred, beta=0.5))\n",
    "\n",
    "print(\"accuracy:\", f\"{np.array(acc).mean():.2f}±{np.array(acc).std():.2f}\")\n",
    "print(\"balanced accuracy:\", f\"{np.array(bal_acc).mean():.2f}±{np.array(bal_acc).std():.2f}\")\n",
    "print(\"recall:\", f\"{np.array(recall).mean():.2f}±{np.array(recall).std():.2f}\")\n",
    "print(\"precision:\", f\"{np.array(precision).mean():.2f}±{np.array(precision).std():.2f}\")\n",
    "print(\"f_0.5 score:\", f\"{np.array(fbeta).mean():.2f}±{np.array(fbeta).std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tresh_bal_acc =  np.argmax(y_bal_acc, axis=0) / 100\n",
    "best_tresh_bal_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold obtained by maximizing balanced accuracy\n",
    "\n",
    "acc, bal_acc, recall, precision, fbeta = [], [], [], [], []\n",
    "\n",
    "for fold in range(9):\n",
    "    threshold = np.argmax(y_bal_acc[:, fold]) / 100\n",
    "    y_true = comb_val[fold][\"binary_A\"]\n",
    "    y_prob = comb_val[fold][\"pred_0\"]\n",
    "    y_pred = (y_prob > threshold)\n",
    "    acc.append(accuracy_score(y_true, y_pred))\n",
    "    bal_acc.append(balanced_accuracy_score(y_true, y_pred))\n",
    "    recall.append(recall_score(y_true, y_pred))\n",
    "    precision.append(precision_score(y_true, y_pred))\n",
    "    fbeta.append(fbeta_score(y_true, y_pred, beta=0.5))\n",
    "\n",
    "print(\"accuracy:\", f\"{np.array(acc).mean():.2f}±{np.array(acc).std():.2f}\")\n",
    "print(\"balanced accuracy:\", f\"{np.array(bal_acc).mean():.2f}±{np.array(bal_acc).std():.2f}\")\n",
    "print(\"recall:\", f\"{np.array(recall).mean():.2f}±{np.array(recall).std():.2f}\")\n",
    "print(\"precision:\", f\"{np.array(precision).mean():.2f}±{np.array(precision).std():.2f}\")\n",
    "print(\"f_0.5 score:\", f\"{np.array(fbeta).mean():.2f}±{np.array(fbeta).std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed threshold at 0.5\n",
    "\n",
    "acc, bal_acc, recall, precision, fbeta = [], [], [], [], []\n",
    "\n",
    "for fold in range(9):\n",
    "    threshold = 0.5\n",
    "    y_true = comb_val[fold][\"binary_A\"]\n",
    "    y_prob = comb_val[fold][\"pred_0\"]\n",
    "    y_pred = (y_prob > threshold)\n",
    "    acc.append(accuracy_score(y_true, y_pred))\n",
    "    bal_acc.append(balanced_accuracy_score(y_true, y_pred))\n",
    "    recall.append(recall_score(y_true, y_pred))\n",
    "    precision.append(precision_score(y_true, y_pred))\n",
    "    fbeta.append(fbeta_score(y_true, y_pred, beta=0.5))\n",
    "\n",
    "print(\"accuracy:\", f\"{np.array(acc).mean():.2f}±{np.array(acc).std():.2f}\")\n",
    "print(\"balanced accuracy:\", f\"{np.array(bal_acc).mean():.2f}±{np.array(bal_acc).std():.2f}\")\n",
    "print(\"recall:\", f\"{np.array(recall).mean():.2f}±{np.array(recall).std():.2f}\")\n",
    "print(\"precision:\", f\"{np.array(precision).mean():.2f}±{np.array(precision).std():.2f}\")\n",
    "print(\"f_0.5 score:\", f\"{np.array(fbeta).mean():.2f}±{np.array(fbeta).std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold equal to training data distribution\n",
    "\n",
    "acc, bal_acc, recall, precision, fbeta = [], [], [], [], []\n",
    "\n",
    "for fold in range(9):\n",
    "    threshold = comb[fold][\"binary_A\"].mean()\n",
    "    y_true = comb_val[fold][\"binary_A\"]\n",
    "    y_prob = comb_val[fold][\"pred_0\"]\n",
    "    y_pred = (y_prob > threshold)\n",
    "    acc.append(accuracy_score(y_true, y_pred))\n",
    "    bal_acc.append(balanced_accuracy_score(y_true, y_pred))\n",
    "    recall.append(recall_score(y_true, y_pred))\n",
    "    precision.append(precision_score(y_true, y_pred))\n",
    "    fbeta.append(fbeta_score(y_true, y_pred, beta=0.5))\n",
    "\n",
    "print(\"accuracy:\", f\"{np.array(acc).mean():.2f}±{np.array(acc).std():.2f}\")\n",
    "print(\"balanced accuracy:\", f\"{np.array(bal_acc).mean():.2f}±{np.array(bal_acc).std():.2f}\")\n",
    "print(\"recall:\", f\"{np.array(recall).mean():.2f}±{np.array(recall).std():.2f}\")\n",
    "print(\"precision:\", f\"{np.array(precision).mean():.2f}±{np.array(precision).std():.2f}\")\n",
    "print(\"f_0.5 score:\", f\"{np.array(fbeta).mean():.2f}±{np.array(fbeta).std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Using GHOST with Cohen's Kappa gives the best results.\n",
    "We will use the thresholds obtained this way for our final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best thresholds for this model\n",
    "for thresh, name in zip(best_thresh, name_list):\n",
    "    with open(LOG_DIR / \"thresholds\" / f\"{name}.txt\", \"w\") as f:\n",
    "        f.write(f\"{thresh}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Faster way to pick decision treshold\n",
    "Given a run_group, identify and save all the decision thresholds using GHOST with Cohen's kappa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decision_thresholds(run_group: str) -> None:\n",
    "    \"\"\"\n",
    "    Given a run group, identify and save all the decision thresholds using GHOST with Cohen's kappa.\n",
    "    Best thresholds are saved to a txt file with 3 lines.\n",
    "    The 1st line is the threshold for target binary_A, the 2nd for binary_B, and the 3rd for binary_C.\n",
    "    If the run_group contains multiple runs, one txt file is generated per run.\n",
    "    \n",
    "    Args: \n",
    "        run_group (str): Name of a run_group present in the wandb database.\n",
    "    Returns:\n",
    "        None\n",
    "    Raises:\n",
    "        ValueError: Raised if no runs are found in WandB for the run_group\n",
    "        RuntimeError: Raised if the predictions for a run are not found on disk.\n",
    "    \"\"\"\n",
    "    # get runs from wandb\n",
    "    api = wandb.Api(timeout=59)\n",
    "    runs = api.runs(\"jugoetz/synferm-predictions\", filters={\"group\": run_group})\n",
    "    name_list = [run.name for run in runs]\n",
    "    if len(name_list) == 0:\n",
    "        raise ValueError(f\"No runs found for run_group {run_group}.\")\n",
    "        \n",
    "    # read train predictions\n",
    "    try:\n",
    "        preds = [read_predictions(n, \"train\") for n in name_list]\n",
    "    except FileNotFoundError as e:\n",
    "        raise RuntimeError(f\"Did not find all predictions for runs {name_list}. Exception leading to this: {e}\")\n",
    "    \n",
    "    # combine predictions with ground truth\n",
    "    df_true = pd.read_csv(DATA_ROOT / \"synferm_dataset_2023-12-20_39486records.csv\")\n",
    "    comb = [pred.merge(df_true[[\"binary_A\", \"binary_B\", \"binary_C\"]], left_index=True, right_index=True) for pred in preds]\n",
    "    \n",
    "    # obtain best threshold with GHOST with Cohen's Kappa method\n",
    "    print(f\"Running GHOST for {len(name_list)} runs. This may take a while...\")\n",
    "    best_thresh = np.empty((len(name_list), 3))\n",
    "    thresholds = np.arange(0, 1.0001, 0.01)\n",
    "    for run_idx in range(len(name_list)):\n",
    "        for i, target in enumerate(\"ABC\"):\n",
    "            best_thresh[run_idx, i] = optimize_threshold_from_predictions(\n",
    "                comb[run_idx][f\"binary_{target}\"], \n",
    "                comb[run_idx][f\"pred_{i}\"], \n",
    "                thresholds=thresholds, \n",
    "                ThOpt_metrics=\"Kappa\",\n",
    "                random_seed=42, # we seed for reproducible results\n",
    "            )\n",
    "    \n",
    "    # write thresholds to disk\n",
    "    for threshs, name in zip(best_thresh, name_list):\n",
    "        with open(LOG_DIR / \"thresholds\" / f\"{name}.txt\", \"w\") as f:\n",
    "            for thresh in threshs:\n",
    "                f.write(f\"{thresh:.2f}\\n\")\n",
    "    print(best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverse_class_distribution(run_group: str) -> None:\n",
    "    \"\"\"\n",
    "    An alternative is to set the decision boundary to the class distribution.\n",
    "    Note that this is a quite extreme measure and is interesting mostly as a reference point.\n",
    "    Here we only need to know the split, but for convenience we can get this from the run group (through train indices)\n",
    "    Best thresholds are saved to a txt file with 3 lines.\n",
    "    The 1st line is the threshold for target binary_A, the 2nd for binary_B, and the 3rd for binary_C.\n",
    "    If the run_group contains multiple runs, one txt file is generated per run.\n",
    "    \n",
    "    Args: \n",
    "        run_group (str): Name of a run_group present in the wandb database.\n",
    "    Returns:\n",
    "        None\n",
    "    Raises:\n",
    "        ValueError: Raised if no runs are found in WandB for the run_group\n",
    "        RuntimeError: Raised if the predictions for a run are not found on disk.\n",
    "    \"\"\"\n",
    "    # get runs from wandb\n",
    "    api = wandb.Api(timeout=59)\n",
    "    runs = api.runs(\"jugoetz/synferm-predictions\", filters={\"group\": run_group})\n",
    "    name_list = [run.name for run in runs]\n",
    "    if len(name_list) == 0:\n",
    "        raise ValueError(f\"No runs found for run_group {run_group}.\")\n",
    "        \n",
    "    # read train predictions\n",
    "    try:\n",
    "        preds = [read_predictions(n, \"train\") for n in name_list]\n",
    "    except FileNotFoundError as e:\n",
    "        raise RuntimeError(f\"Did not find all predictions for runs {name_list}. Exception leading to this: {e}\")\n",
    "    \n",
    "    # combine predictions with ground truth\n",
    "    df_true = pd.read_csv(DATA_ROOT / \"synferm_dataset_2023-12-20_39486records.csv\")\n",
    "    comb = [pred.merge(df_true[[\"binary_A\", \"binary_B\", \"binary_C\"]], left_index=True, right_index=True) for pred in preds]\n",
    "    \n",
    "    # obtain class distributions\n",
    "    best_thresh = np.empty((len(name_list), 3))\n",
    "    for run_idx in range(len(name_list)):\n",
    "        for i, target in enumerate(\"ABC\"):\n",
    "            best_thresh[run_idx, i] = comb[run_idx][f\"binary_{target}\"].mean()\n",
    "                \n",
    "    \n",
    "    # write thresholds to disk\n",
    "    for threshs, name in zip(best_thresh, name_list):\n",
    "        with open(LOG_DIR / \"thresholds\" / f\"{name}.txt\", \"w\") as f:\n",
    "            for thresh in threshs:\n",
    "                f.write(f\"{thresh:.2f}\\n\")\n",
    "    print(best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best OD model from validation (JG1309, FFN/OHE)\n",
    "get_decision_thresholds(\"2023-12-20-202602_330364\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the OD production model (JG1349, FFN/OHE)\n",
    "get_decision_thresholds(\"2024-01-30-112912_514212\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best XGB/FP models (JG1486, 1D)\n",
    "get_decision_thresholds(\"2024-01-23-063840_864375\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the XGB/FP modelfor Euan on 27 folds (JG1526, 1D)\n",
    "get_decision_thresholds(\"2024-02-23-134822_777158\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best XGB/FP models (JG1495, 2D)\n",
    "get_decision_thresholds(\"2024-01-25-192032_503662\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best XGB/FP models (JG1504, 3D)\n",
    "get_decision_thresholds(\"2024-01-26-161936_145583\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Thresholded metrics for test scores of best models\n",
    "(i.e. we evaluate the models that were selected based on validation performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the thresholded metrics for these models\n",
    "# 0D\n",
    "api = wandb.Api(timeout=59)\n",
    "runs = api.runs(\"jugoetz/synferm-predictions\", filters={\"group\": \"2023-12-20-202602_330364\"})\n",
    "\n",
    "# get run info\n",
    "config_list = [run.config for run in runs]\n",
    "summary_list = [run.summary._json_dict for run in runs]\n",
    "name_list = [run.name for run in runs]\n",
    "\n",
    "# read val predictions\n",
    "preds = [read_predictions(n, \"test\") for n in name_list]\n",
    "# combine with ground truth\n",
    "df_true = pd.read_csv(DATA_ROOT / \"synferm_dataset_2023-12-20_39486records.csv\")\n",
    "comb = [pred.merge(df_true[[\"binary_A\", \"binary_B\", \"binary_C\"]], left_index=True, right_index=True) for pred in preds]\n",
    "\n",
    "# load decision thresholds\n",
    "threshs = []\n",
    "for name in name_list:\n",
    "    with open(LOG_DIR / \"thresholds\" / f\"{name}.txt\", \"r\") as f:\n",
    "        threshs.append([float(i) for i in f.readlines()])\n",
    "threshs = np.array(threshs)\n",
    "\n",
    "# get truth\n",
    "fold_true = [comb[i][[\"binary_A\", \"binary_B\", \"binary_C\"]].to_numpy() for i in range(len(name_list))]\n",
    "\n",
    "# get preds by applying the thresholds\n",
    "fold_preds = []\n",
    "for fold_i in range(len(name_list)):\n",
    "    fold_preds.append(np.stack([np.where(preds[fold_i].to_numpy()[:, i] > threshs[fold_i, i], 1, 0) for i in range(3)], axis=1))\n",
    "\n",
    "# evaluate\n",
    "acc, bal_acc, recall, precision, fbeta = np.zeros((len(name_list), 3)), np.zeros((len(name_list), 3)), np.zeros((len(name_list), 3)), np.zeros((len(name_list), 3)), np.zeros((len(name_list), 3))\n",
    "\n",
    "for fold in range(len(name_list)):\n",
    "    for target in range(3):\n",
    "        threshold = threshs[fold]\n",
    "        y_true = fold_true[fold][:, target]\n",
    "        y_pred = fold_preds[fold][:, target]\n",
    "        acc[fold, target] = accuracy_score(y_true, y_pred)\n",
    "        bal_acc[fold, target] = balanced_accuracy_score(y_true, y_pred)\n",
    "        recall[fold, target] = recall_score(y_true, y_pred)\n",
    "        precision[fold, target] = precision_score(y_true, y_pred)\n",
    "        fbeta[fold, target] = fbeta_score(y_true, y_pred, beta=0.5)\n",
    "        print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "for i, name in enumerate(name_list):\n",
    "    records.append({\"name\": name, \"split\": \"0D\", \"metric\": \"accuracy\", \"target\": \"macro\", \"value\": acc[i].mean()})\n",
    "    records.append({\"name\": name, \"split\": \"0D\", \"metric\": \"recall\", \"target\": \"macro\", \"value\": recall[i].mean()})\n",
    "    records.append({\"name\": name, \"split\": \"0D\", \"metric\": \"precision\", \"target\": \"macro\", \"value\": precision[i].mean()})\n",
    "    records.append({\"name\": name, \"split\": \"0D\", \"metric\": \"accuracy\", \"target\": \"A\", \"value\": acc[i, 0]})\n",
    "    records.append({\"name\": name, \"split\": \"0D\", \"metric\": \"recall\", \"target\": \"A\", \"value\": recall[i, 0]})\n",
    "    records.append({\"name\": name, \"split\": \"0D\", \"metric\": \"precision\", \"target\": \"A\", \"value\": precision[i, 0]})\n",
    "\n",
    "\n",
    "print(\"Mean macro accuracy:\", f\"{acc.mean():.2f}±{acc.mean(axis=1).std():.3f}\")\n",
    "print(\"Mean macro recall:\", f\"{recall.mean():.2f}±{recall.mean(axis=1).std():.3f}\")\n",
    "print(\"Mean macro precision:\", f\"{precision.mean():.2f}±{precision.mean(axis=1).std():.3f}\")\n",
    "#print(\"Mean macro balanced accuracy:\", f\"{bal_acc.mean():.2f}±{bal_acc.mean(axis=1).std():.2f}\")\n",
    "#print(\"Mean macro f_0.5 score:\", f\"{fbeta.mean():.2f}±{fbeta.mean(axis=1).std():.2f}\")\n",
    "\n",
    "print(\"Mean target_A accuracy:\", f\"{acc[:, 0].mean():.2f}±{acc[:, 0].std():.3f}\")\n",
    "print(\"Mean target_A recall:\", f\"{recall[:, 0].mean():.2f}±{recall[:, 0].std():.3f}\")\n",
    "print(\"Mean target_A precision:\", f\"{precision[:, 0].mean():.2f}±{precision[:, 0].std():.3f}\")\n",
    "#print(\"Mean target_A balanced accuracy:\", f\"{bal_acc[:, 0].mean():.2f}±{bal_acc[:, 0].std():.2f}\")\n",
    "#print(\"Mean target_A f_0.5 score:\", f\"{fbeta[:, 0].mean():.2f}±{fbeta[:, 0].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the thresholded metrics for these models\n",
    "# 1D\n",
    "api = wandb.Api()\n",
    "runs = api.runs(\"jugoetz/synferm-predictions\", filters={\"group\": \"2024-01-23-063840_864375\"})\n",
    "\n",
    "# get run info\n",
    "config_list = [run.config for run in runs]\n",
    "summary_list = [run.summary._json_dict for run in runs]\n",
    "name_list = [run.name for run in runs]\n",
    "\n",
    "# read val predictions\n",
    "preds = [read_predictions(n, \"test\") for n in name_list]\n",
    "# combine with ground truth\n",
    "df_true = pd.read_csv(DATA_ROOT / \"synferm_dataset_2023-12-20_39486records.csv\")\n",
    "comb = [pred.merge(df_true[[\"binary_A\", \"binary_B\", \"binary_C\"]], left_index=True, right_index=True) for pred in preds]\n",
    "\n",
    "# load decision thresholds\n",
    "threshs = []\n",
    "for name in name_list:\n",
    "    with open(LOG_DIR / \"thresholds\" / f\"{name}.txt\", \"r\") as f:\n",
    "        threshs.append([float(i) for i in f.readlines()])\n",
    "threshs = np.array(threshs)\n",
    "\n",
    "# get truth\n",
    "fold_true = [comb[i][[\"binary_A\", \"binary_B\", \"binary_C\"]].to_numpy() for i in range(len(name_list))]\n",
    "\n",
    "# get preds by applying the thresholds\n",
    "fold_preds = []\n",
    "for fold_i in range(len(name_list)):\n",
    "    fold_preds.append(np.stack([np.where(preds[fold_i].to_numpy()[:, i] > threshs[fold_i, i], 1, 0) for i in range(3)], axis=1))\n",
    "\n",
    "# evaluate\n",
    "acc, bal_acc, recall, precision, fbeta = np.zeros((len(name_list), 3)), np.zeros((len(name_list), 3)), np.zeros((len(name_list), 3)), np.zeros((len(name_list), 3)), np.zeros((len(name_list), 3))\n",
    "\n",
    "for fold in range(len(name_list)):\n",
    "    for target in range(3):\n",
    "        threshold = threshs[fold]\n",
    "        y_true = fold_true[fold][:, target]\n",
    "        y_pred = fold_preds[fold][:, target]\n",
    "        acc[fold, target] = accuracy_score(y_true, y_pred)\n",
    "        bal_acc[fold, target] = balanced_accuracy_score(y_true, y_pred)\n",
    "        recall[fold, target] = recall_score(y_true, y_pred)\n",
    "        precision[fold, target] = precision_score(y_true, y_pred)\n",
    "        fbeta[fold, target] = fbeta_score(y_true, y_pred, beta=0.5)\n",
    "\n",
    "for i, name in enumerate(name_list):\n",
    "    records.append({\"name\": name, \"split\": \"1D\", \"metric\": \"accuracy\", \"target\": \"macro\", \"value\": acc[i].mean()})\n",
    "    records.append({\"name\": name, \"split\": \"1D\", \"metric\": \"recall\", \"target\": \"macro\", \"value\": recall[i].mean()})\n",
    "    records.append({\"name\": name, \"split\": \"1D\", \"metric\": \"precision\", \"target\": \"macro\", \"value\": precision[i].mean()})\n",
    "    records.append({\"name\": name, \"split\": \"1D\", \"metric\": \"accuracy\", \"target\": \"A\", \"value\": acc[i, 0]})\n",
    "    records.append({\"name\": name, \"split\": \"1D\", \"metric\": \"recall\", \"target\": \"A\", \"value\": recall[i, 0]})\n",
    "    records.append({\"name\": name, \"split\": \"1D\", \"metric\": \"precision\", \"target\": \"A\", \"value\": precision[i, 0]})\n",
    "\n",
    "\n",
    "print(\"Mean macro accuracy:\", f\"{acc.mean():.2f}±{acc.mean(axis=1).std():.3f}\")\n",
    "print(\"Mean macro recall:\", f\"{recall.mean():.2f}±{recall.mean(axis=1).std():.3f}\")\n",
    "print(\"Mean macro precision:\", f\"{precision.mean():.2f}±{precision.mean(axis=1).std():.3f}\")\n",
    "print(\"Mean macro balanced accuracy:\", f\"{bal_acc.mean():.2f}±{bal_acc.mean(axis=1).std():.2f}\")\n",
    "#print(\"Mean macro f_0.5 score:\", f\"{fbeta.mean():.2f}±{fbeta.mean(axis=1).std():.2f}\")\n",
    "\n",
    "print(\"Mean target_A accuracy:\", f\"{acc[:, 0].mean():.2f}±{acc[:, 0].std():.3f}\")\n",
    "print(\"Mean target_A recall:\", f\"{recall[:, 0].mean():.2f}±{recall[:, 0].std():.3f}\")\n",
    "print(\"Mean target_A precision:\", f\"{precision[:, 0].mean():.2f}±{precision[:, 0].std():.3f}\")\n",
    "print(\"Mean target_A balanced accuracy:\", f\"{bal_acc[:, 0].mean():.2f}±{bal_acc[:, 0].std():.2f}\")\n",
    "#print(\"Mean target_A f_0.5 score:\", f\"{fbeta[:, 0].mean():.2f}±{fbeta[:, 0].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the thresholded metrics for these models\n",
    "# 2D\n",
    "api = wandb.Api()\n",
    "runs = api.runs(\"jugoetz/synferm-predictions\", filters={\"group\": \"2024-01-25-192032_503662\"})\n",
    "\n",
    "# get run info\n",
    "config_list = [run.config for run in runs]\n",
    "summary_list = [run.summary._json_dict for run in runs]\n",
    "name_list = [run.name for run in runs]\n",
    "\n",
    "# read val predictions\n",
    "preds = [read_predictions(n, \"test\") for n in name_list]\n",
    "# combine with ground truth\n",
    "df_true = pd.read_csv(DATA_ROOT / \"synferm_dataset_2023-12-20_39486records.csv\")\n",
    "comb = [pred.merge(df_true[[\"binary_A\", \"binary_B\", \"binary_C\"]], left_index=True, right_index=True) for pred in preds]\n",
    "\n",
    "# load decision thresholds\n",
    "threshs = []\n",
    "for name in name_list:\n",
    "    with open(LOG_DIR / \"thresholds\" / f\"{name}.txt\", \"r\") as f:\n",
    "        threshs.append([float(i) for i in f.readlines()])\n",
    "threshs = np.array(threshs)\n",
    "\n",
    "# get truth\n",
    "fold_true = [comb[i][[\"binary_A\", \"binary_B\", \"binary_C\"]].to_numpy() for i in range(len(name_list))]\n",
    "\n",
    "# get preds by applying the thresholds\n",
    "fold_preds = []\n",
    "for fold_i in range(len(name_list)):\n",
    "    fold_preds.append(np.stack([np.where(preds[fold_i].to_numpy()[:, i] > threshs[fold_i, i], 1, 0) for i in range(3)], axis=1))\n",
    "\n",
    "# evaluate\n",
    "acc, bal_acc, recall, precision, fbeta = np.zeros((len(name_list), 3)), np.zeros((len(name_list), 3)), np.zeros((len(name_list), 3)), np.zeros((len(name_list), 3)), np.zeros((len(name_list), 3))\n",
    "\n",
    "for fold in range(len(name_list)):\n",
    "    for target in range(3):\n",
    "        threshold = threshs[fold]\n",
    "        y_true = fold_true[fold][:, target]\n",
    "        y_pred = fold_preds[fold][:, target]\n",
    "        acc[fold, target] = accuracy_score(y_true, y_pred)\n",
    "        bal_acc[fold, target] = balanced_accuracy_score(y_true, y_pred)\n",
    "        recall[fold, target] = recall_score(y_true, y_pred)\n",
    "        precision[fold, target] = precision_score(y_true, y_pred)\n",
    "        fbeta[fold, target] = fbeta_score(y_true, y_pred, beta=0.5)\n",
    "\n",
    "for i, name in enumerate(name_list):\n",
    "    records.append({\"name\": name, \"split\": \"2D\", \"metric\": \"accuracy\", \"target\": \"macro\", \"value\": acc[i].mean()})\n",
    "    records.append({\"name\": name, \"split\": \"2D\", \"metric\": \"recall\", \"target\": \"macro\", \"value\": recall[i].mean()})\n",
    "    records.append({\"name\": name, \"split\": \"2D\", \"metric\": \"precision\", \"target\": \"macro\", \"value\": precision[i].mean()})\n",
    "    records.append({\"name\": name, \"split\": \"2D\", \"metric\": \"accuracy\", \"target\": \"A\", \"value\": acc[i, 0]})\n",
    "    records.append({\"name\": name, \"split\": \"2D\", \"metric\": \"recall\", \"target\": \"A\", \"value\": recall[i, 0]})\n",
    "    records.append({\"name\": name, \"split\": \"2D\", \"metric\": \"precision\", \"target\": \"A\", \"value\": precision[i, 0]})\n",
    "\n",
    "\n",
    "print(\"Mean macro accuracy:\", f\"{acc.mean():.2f}±{acc.mean(axis=1).std():.3f}\")\n",
    "print(\"Mean macro recall:\", f\"{recall.mean():.2f}±{recall.mean(axis=1).std():.3f}\")\n",
    "print(\"Mean macro precision:\", f\"{precision.mean():.2f}±{precision.mean(axis=1).std():.3f}\")\n",
    "print(\"Mean macro balanced accuracy:\", f\"{bal_acc.mean():.2f}±{bal_acc.mean(axis=1).std():.2f}\")\n",
    "#print(\"Mean macro f_0.5 score:\", f\"{fbeta.mean():.2f}±{fbeta.mean(axis=1).std():.2f}\")\n",
    "\n",
    "print(\"Mean target_A accuracy:\", f\"{acc[:, 0].mean():.2f}±{acc[:, 0].std():.3f}\")\n",
    "print(\"Mean target_A recall:\", f\"{recall[:, 0].mean():.2f}±{recall[:, 0].std():.3f}\")\n",
    "print(\"Mean target_A precision:\", f\"{precision[:, 0].mean():.2f}±{precision[:, 0].std():.3f}\")\n",
    "print(\"Mean target_A balanced accuracy:\", f\"{bal_acc[:, 0].mean():.2f}±{bal_acc[:, 0].std():.2f}\")\n",
    "#print(\"Mean target_A f_0.5 score:\", f\"{fbeta[:, 0].mean():.2f}±{fbeta[:, 0].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the thresholded metrics for these models\n",
    "# 3D\n",
    "api = wandb.Api()\n",
    "runs = api.runs(\"jugoetz/synferm-predictions\", filters={\"group\": \"2024-01-26-161936_145583\"})\n",
    "\n",
    "# get run info\n",
    "config_list = [run.config for run in runs]\n",
    "summary_list = [run.summary._json_dict for run in runs]\n",
    "name_list = [run.name for run in runs]\n",
    "\n",
    "# read val predictions\n",
    "preds = [read_predictions(n, \"test\") for n in name_list]\n",
    "# combine with ground truth\n",
    "df_true = pd.read_csv(DATA_ROOT / \"synferm_dataset_2023-12-20_39486records.csv\")\n",
    "comb = [pred.merge(df_true[[\"binary_A\", \"binary_B\", \"binary_C\"]], left_index=True, right_index=True) for pred in preds]\n",
    "\n",
    "# load decision thresholds\n",
    "threshs = []\n",
    "for name in name_list:\n",
    "    with open(LOG_DIR / \"thresholds\" / f\"{name}.txt\", \"r\") as f:\n",
    "        threshs.append([float(i) for i in f.readlines()])\n",
    "threshs = np.array(threshs)\n",
    "\n",
    "# get truth\n",
    "fold_true = [comb[i][[\"binary_A\", \"binary_B\", \"binary_C\"]].to_numpy() for i in range(len(name_list))]\n",
    "\n",
    "# get preds by applying the thresholds\n",
    "fold_preds = []\n",
    "for fold_i in range(len(name_list)):\n",
    "    fold_preds.append(np.stack([np.where(preds[fold_i].to_numpy()[:, i] > threshs[fold_i, i], 1, 0) for i in range(3)], axis=1))\n",
    "\n",
    "# evaluate\n",
    "acc, bal_acc, recall, precision, fbeta = np.zeros((len(name_list), 3)), np.zeros((len(name_list), 3)), np.zeros((len(name_list), 3)), np.zeros((len(name_list), 3)), np.zeros((len(name_list), 3))\n",
    "\n",
    "for fold in range(len(name_list)):\n",
    "    for target in range(3):\n",
    "        threshold = threshs[fold]\n",
    "        y_true = fold_true[fold][:, target]\n",
    "        y_pred = fold_preds[fold][:, target]\n",
    "        acc[fold, target] = accuracy_score(y_true, y_pred)\n",
    "        bal_acc[fold, target] = balanced_accuracy_score(y_true, y_pred)\n",
    "        recall[fold, target] = recall_score(y_true, y_pred)\n",
    "        precision[fold, target] = precision_score(y_true, y_pred)\n",
    "        fbeta[fold, target] = fbeta_score(y_true, y_pred, beta=0.5)\n",
    "        if target == 0:\n",
    "            print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "for i, name in enumerate(name_list):\n",
    "    records.append({\"name\": name, \"split\": \"3D\", \"metric\": \"accuracy\", \"target\": \"macro\", \"value\": acc[i].mean()})\n",
    "    records.append({\"name\": name, \"split\": \"3D\", \"metric\": \"recall\", \"target\": \"macro\", \"value\": recall[i].mean()})\n",
    "    records.append({\"name\": name, \"split\": \"3D\", \"metric\": \"precision\", \"target\": \"macro\", \"value\": precision[i].mean()})\n",
    "    records.append({\"name\": name, \"split\": \"3D\", \"metric\": \"accuracy\", \"target\": \"A\", \"value\": acc[i, 0]})\n",
    "    records.append({\"name\": name, \"split\": \"3D\", \"metric\": \"recall\", \"target\": \"A\", \"value\": recall[i, 0]})\n",
    "    records.append({\"name\": name, \"split\": \"3D\", \"metric\": \"precision\", \"target\": \"A\", \"value\": precision[i, 0]})\n",
    "\n",
    "\n",
    "print(\"Mean macro accuracy:\", f\"{acc.mean():.2f}±{acc.mean(axis=1).std():.3f}\")\n",
    "print(\"Mean macro recall:\", f\"{recall.mean():.2f}±{recall.mean(axis=1).std():.3f}\")\n",
    "print(\"Mean macro precision:\", f\"{precision.mean():.2f}±{precision.mean(axis=1).std():.3f}\")\n",
    "print(\"Mean macro balanced accuracy:\", f\"{bal_acc.mean():.2f}±{bal_acc.mean(axis=1).std():.2f}\")\n",
    "#print(\"Mean macro f_0.5 score:\", f\"{fbeta.mean():.2f}±{fbeta.mean(axis=1).std():.2f}\")\n",
    "\n",
    "print(\"Mean target_A accuracy:\", f\"{acc[:, 0].mean():.2f}±{acc[:, 0].std():.3f}\")\n",
    "print(\"Mean target_A recall:\", f\"{recall[:, 0].mean():.2f}±{recall[:, 0].std():.3f}\")\n",
    "print(\"Mean target_A precision:\", f\"{precision[:, 0].mean():.2f}±{precision[:, 0].std():.3f}\")\n",
    "print(\"Mean target_A balanced accuracy:\", f\"{bal_acc[:, 0].mean():.2f}±{bal_acc[:, 0].std():.2f}\")\n",
    "#print(\"Mean target_A f_0.5 score:\", f\"{fbeta[:, 0].mean():.2f}±{fbeta[:, 0].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(records)\n",
    "df[\"metric\"] = df[\"metric\"].apply(str.capitalize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib\n",
    "#matplotlib.use('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "sns.set_theme(context=\"paper\", \n",
    "              style=\"white\", \n",
    "              font_scale=1, #0.7,\n",
    "              rc={\"savefig.transparent\": True, \n",
    "                  \"axes.grid\": False, \n",
    "                  \"axes.spines.bottom\": True,\n",
    "                  \"axes.spines.left\": False,\n",
    "                  \"axes.spines.right\": False,\n",
    "                  \"axes.spines.top\": False,\n",
    "                  \"font.family\":'sans-serif',\n",
    "                  \"font.sans-serif\":[\"Helvetica\", \"Arial\"],\n",
    "                  \"xtick.major.pad\": 0.0,\n",
    "                  \"xtick.minor.pad\": 0.0,\n",
    "                  \"ytick.major.pad\": 0.0,\n",
    "                  \"ytick.minor.pad\": 0.0,\n",
    "                  \"axes.labelweight\": \"bold\",\n",
    "                  \"axes.labelpad\": 2.5,  # standard is 4.0\n",
    "                  \"axes.xmargin\": .05,\n",
    "                 }, \n",
    "             )\n",
    "\n",
    "# more settings for all plots\n",
    "errorbar = \"se\"  # standard error of the mean\n",
    "errwidth = .9\n",
    "errcolor = \"black\"\n",
    "capsize = .1  # size of the end of the errorbar\n",
    "linewidth = 1.  # width of the outline of barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette([\"#5760bb\", \"#bd57d5\", \"#87ba70\", \"#c6c150\"])\n",
    "palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dir where we will save plots\n",
    "analysis_dir = pathlib.Path(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4.75,4))\n",
    "sns.barplot(df.loc[df.target == \"macro\"], \n",
    "            x=\"split\", \n",
    "            y=\"value\", \n",
    "            hue=\"metric\",\n",
    "            palette=palette,\n",
    "            errorbar=errorbar, \n",
    "            errcolor=errcolor,\n",
    "            lw=linewidth,\n",
    "            capsize=capsize / 4,\n",
    "            errwidth=1.5,\n",
    "            alpha=.99\n",
    "           )\n",
    "\n",
    "# Set the color of each bar manually\n",
    "for i, bar in enumerate(ax.patches):\n",
    "    bar.set_facecolor(palette[i % 4])\n",
    "    if i in range(4, 8):\n",
    "        bar.set_hatch(\"//////\")\n",
    "    if i in range(8, 12):\n",
    "        bar.set_hatch(\"\\\\\\\\\\\\\\\\\\\\\\\\\")  # need more to escape backslashes\n",
    "\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(\"Metric\")\n",
    "ax.set_ylim((0, 1))\n",
    "\n",
    "ax.legend(loc=\"lower left\", title=None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / \"best-model_metrics_2023-12-20_test-macro.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4.75,4))\n",
    "sns.barplot(df.loc[df.target == \"A\"], \n",
    "            x=\"split\", \n",
    "            y=\"value\", \n",
    "            hue=\"metric\",\n",
    "            palette=palette,\n",
    "            errorbar=errorbar, \n",
    "            errcolor=errcolor,\n",
    "            lw=linewidth,\n",
    "            capsize=capsize / 4,\n",
    "            errwidth=1.5,\n",
    "           )\n",
    "\n",
    "# Set the color of each bar manually\n",
    "for i, bar in enumerate(ax.patches):\n",
    "    bar.set_facecolor(palette[i % 4])\n",
    "    if i in range(4, 8):\n",
    "        bar.set_hatch(\"//////\")\n",
    "    if i in range(8, 12):\n",
    "        bar.set_hatch(\"\\\\\\\\\\\\\\\\\\\\\\\\\")  # need more to escape backslashes\n",
    "\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(\"Metric\")\n",
    "ax.set_ylim((0, 1))\n",
    "\n",
    "ax.legend(loc=\"lower left\", title=None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis_dir / \"best-model_metrics_2023-12-20_test-A.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
