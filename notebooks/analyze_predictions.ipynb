{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Analyze predictions\n",
    "\n",
    "Initially, the ML train loop did not calculate some metrics we are now interested in.\n",
    "But since we saved the predictions, it should be easy to obtain the metrics.\n",
    "We add them back to wandb, for easier analysis there.\n",
    "- Add AUROC, balanced accuracy, and - for multilabel tasks - some averages over all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "sys.path.append(str(pathlib.Path().absolute().parent))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, accuracy_score, balanced_accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "from src.util.definitions import PRED_DIR, DATA_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = pd.read_csv(DATA_ROOT / \"synferm_dataset_2023-09-05_40018records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "runs = api.runs(\"jugoetz/synferm-predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_list, config_list, name_list = [], [], []\n",
    "for run in runs:\n",
    "    # .summary contains output keys/values for\n",
    "    # metrics such as accuracy.\n",
    "    #  We call ._json_dict to omit large files\n",
    "    summary_list.append(run.summary._json_dict)\n",
    "\n",
    "    # .config contains the hyperparameters.\n",
    "    #  We remove special values that start with _.\n",
    "    config_list.append(\n",
    "        {k: v for k,v in run.config.items()\n",
    "         if not k.startswith('_')})\n",
    "\n",
    "    # .name is the human-readable name of the run.\n",
    "    name_list.append(run.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list.index(\"2023-09-26-132606_155550_fold8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit([\"A\", \"B\", \"C\", \"no_product\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in runs[9:-1]:\n",
    "    # we want to calculate additional metrics\n",
    "    run_type = run.config[\"training\"][\"task\"]\n",
    "    run_target_names = run.config[\"target_names\"]\n",
    "    run_name = run.name\n",
    "    # first we check if predicted values are available\n",
    "    val_pred_path = PRED_DIR / run_name / \"val_preds_last.csv\"\n",
    "    test_pred_path = PRED_DIR / run_name / \"test_preds_last.csv\"\n",
    "    metrics = {}\n",
    "    for name, file in zip([\"val\", \"test\"], [val_pred_path, test_pred_path]):\n",
    "        if file.is_file():\n",
    "            # import predictions and combine with ground truth\n",
    "            df = pd.read_csv(file, index_col=\"idx\").merge(df_true, how=\"left\", left_index=True, right_index=True)\n",
    "        \n",
    "            # extract predictions\n",
    "            if run_type in [\"multilabel\", \"binary\"]:\n",
    "                y_prob = df[[f\"pred_{i}\" for i in range(len(run_target_names))]].to_numpy()\n",
    "                y_hat = (y_prob > 0.5).astype(np.int_)\n",
    "                y_true = df[run_target_names].to_numpy()\n",
    "            elif run_type == \"multiclass\":\n",
    "                y_prob = df[[f\"pred_{i}\" for i in range(len(le.classes_))]].to_numpy()\n",
    "                y_hat = np.argmax(y_prob, axis=1)\n",
    "                y_true = le.transform(df[\"major_A-C\"].to_numpy())\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected run_type\")\n",
    "        \n",
    "            # calculate additional metrics\n",
    "            if run_type == \"multilabel\":\n",
    "                auroc_avg = roc_auc_score(y_true, y_prob, average=\"macro\")\n",
    "                #conf_mat = multilabel_confusion_matrix(y_true, y_hat)\n",
    "                recall_avg = recall_score(y_true, y_hat, average=\"macro\")\n",
    "                precision_avg = precision_score(y_true, y_hat, average=\"macro\")\n",
    "                f1_avg = f1_score(y_true, y_hat, average=\"micro\")\n",
    "                \n",
    "                # compute balanced accuracy per label\n",
    "                balanced_acc = []\n",
    "                for i, t in enumerate(run_target_names):\n",
    "                    balanced_acc.append(balanced_accuracy_score(y_true[:,i], y_hat[:,i], adjusted=False))\n",
    "        \n",
    "                metrics.update({\n",
    "                            f\"{name}/balanced_accuracy_macro\": np.mean(balanced_acc), \n",
    "                            f\"{name}/auroc_macro\": auroc_avg, \n",
    "                            f\"{name}/recall_macro\": recall_avg, \n",
    "                            f\"{name}/precision_macro\": precision_avg,\n",
    "                            f\"{name}/f1_micro\": f1_avg \n",
    "                            })\n",
    "                metrics.update({f\"{name}/balanced_accuracy_target_{t}\": v for t, v in zip(run_target_names, balanced_acc)})\n",
    "            elif run_type == \"binary\":\n",
    "                # calculate additional metrics\n",
    "                auroc = roc_auc_score(y_true, y_prob, average=None)\n",
    "                balanced_acc = balanced_accuracy_score(y_true, y_hat, adjusted=False)\n",
    "                metrics.update({\n",
    "                            f\"{name}/balanced_accuracy\": balanced_acc, \n",
    "                            f\"{name}/auroc\": auroc \n",
    "                            })\n",
    "            elif run_type == \"multiclass\":\n",
    "                # calculate additional metrics\n",
    "                balanced_acc = balanced_accuracy_score(y_true, y_hat, adjusted=False)\n",
    "                auroc_macro = roc_auc_score(y_true, y_prob, average=\"macro\", multi_class=\"ovo\")  # one-v-one + macro average is insensitive to class imbalance\n",
    "                metrics.update({\n",
    "                    f\"{name}/balanced_accuracy\": balanced_acc, \n",
    "                    f\"{name}/auroc_macro_ovo\": auroc_macro\n",
    "                    })\n",
    "        else:\n",
    "            print(f\"{name} predictions not found for {run_name}\")\n",
    "    # add new metrics to wandb run\n",
    "    for k, v in metrics.items():\n",
    "        run.summary[k] = v\n",
    "    # update wandb\n",
    "    run.summary.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = api.runs(\"jugoetz/synferm-predictions\", filters={\"display_name\": {\"$regex\": \"2023-09-26-114651_772025_fold*\"}})\n",
    "\n",
    "for run in runs:\n",
    "    print(run.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "for metric_name in runs[0].summary.keys():\n",
    "    if not metric_name.startswith(\"_\"):\n",
    "        metrics[metric_name] = np.array([run.summary[metric_name] for run in runs])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_name in [\n",
    "    \"val/balanced_accuracy_macro\",\n",
    "    \"test/accuracy_target_binary_A\",\n",
    "    \"test/accuracy_target_binary_B\",\n",
    "    \"test/accuracy_target_binary_C\",\n",
    "    \"test/balanced_accuracy_macro\",\n",
    "    \"test/balanced_accuracy_target_binary_A\",\n",
    "    \"test/balanced_accuracy_target_binary_B\",\n",
    "    \"test/balanced_accuracy_target_binary_C\",\n",
    "    \"test/precision_macro\",\n",
    "    \"test/precision_target_binary_A\",\n",
    "    \"test/precision_target_binary_B\",\n",
    "    \"test/precision_target_binary_C\",\n",
    "    \"test/recall_macro\",\n",
    "    \"test/recall_target_binary_A\",\n",
    "    \"test/recall_target_binary_B\",\n",
    "    \"test/recall_target_binary_C\",\n",
    "]:\n",
    "    print(f'{metric_name}: {metrics[metric_name].mean():.4f}±{metrics[metric_name].std():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
