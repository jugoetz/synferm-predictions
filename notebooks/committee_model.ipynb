{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e00d24-7eac-424d-af8f-b7db7c1dde0f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Try committee model\n",
    "\n",
    "We found that generally the following three models perform with insignificant differences:\n",
    "- D-MPNN/CGR\n",
    "- XGB/FP\n",
    "- LogReg/FP\n",
    "\n",
    "If we were to use these as a committee (i.e. averaging probabilities from all three predictions), do we get better predictions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87dc94cb-9107-45f6-a94e-2e10ffc8adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "sys.path.append(str(pathlib.Path(\"__file__\").absolute().parents[1]))\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "from src.util.definitions import PRED_DIR, DATA_ROOT\n",
    "from utils import get_runs_as_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ec07c75-d20c-4a22-a311-76b7a6149adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = pd.read_csv(DATA_ROOT / \"synferm_dataset_2023-09-05_40018records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccfe90dc-3e1f-4b17-a6f9-61de7056877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_list, config_list, tag_list, name_list  = get_runs_as_list(filters={\"jobType\": \"hparam_best\"}\n",
    "                                                                   )\n",
    "df_all = pd.json_normalize(config_list).merge(pd.json_normalize(summary_list), left_index=True, right_index=True)\n",
    "df_all[\"tags\"] = tag_list\n",
    "df_all[\"run_id\"] = name_list\n",
    "df_all[\"run_group\"] = [s.rsplit(\"_\", maxsplit=1)[0] for s in name_list]\n",
    "df_all[\"Model+Features\"] = df_all[\"name\"] + \"/\" + df_all[\"decoder.global_features\"].str.join(\"+\").str.replace(\"None\", \"CGR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90837f9a-d05f-4e0c-a272-a83d32833004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0D',) --> {'JG1128', 'JG1109', 'JG1117', 'JG1160', 'JG1106', 'JG1100', 'JG1116', 'JG1135', 'JG1115', 'JG1131'}\n",
      "('0D_1.25',) --> {'JG1147', 'JG1141', 'JG1159', 'JG1153'}\n",
      "('0D_10',) --> {'JG1138', 'JG1150', 'JG1156', 'JG1144'}\n",
      "('0D_2.5',) --> {'JG1140', 'JG1152', 'JG1146', 'JG1158'}\n",
      "('0D_20',) --> {'JG1143', 'JG1137', 'JG1149', 'JG1155'}\n",
      "('0D_40',) --> {'JG1148', 'JG1136', 'JG1142', 'JG1154'}\n",
      "('0D_5',) --> {'JG1145', 'JG1151', 'JG1157', 'JG1139'}\n",
      "('1D',) --> {'JG1123', 'JG1118', 'JG1125', 'JG1104', 'JG1129', 'JG1126', 'JG1121', 'JG1101', 'JG1132'}\n",
      "('2D',) --> {'JG1119', 'JG1122', 'JG1102', 'JG1112', 'JG1105', 'JG1127', 'JG1124', 'JG1130', 'JG1133'}\n",
      "('3D',) --> {'JG1111', 'JG1134', 'JG1103', 'JG1113', 'JG1120', 'JG1107', 'JG1108', 'JG1110', 'JG1114'}\n"
     ]
    }
   ],
   "source": [
    "# check available experiments by split\n",
    "for tag, row in df_all.groupby(\"tags\")[[\"experiment_id\"]].agg(set).iterrows():\n",
    "    print(tag, \"-->\", row[\"experiment_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af68f68f-ba61-44b0-93d6-ad86f104353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we look at the 1D split first\n",
    "exp_ids = [\"JG1101\", \"JG1129\", \"JG1125\"]  # D-MPNN/CGR, XGB/FP, FFN/FP for 1D (LogReg/FP will be JG1186 when trained)\n",
    "#exp_ids = [\"JG1102\", \"JG1130\", \"JG1112\"]  # D-MPNN/CGR, XGB/FP, LogReg/FP for 2D\n",
    "#exp_ids = [\"JG1103\", \"JG1111\", \"JG1108\"]  # D-MPNN/CGR, XGB/FP, LogReg/FP for 3D\n",
    "df_exps = df_all.loc[df_all.experiment_id.isin(exp_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da5c1287-12eb-4015-9403-d417c47c3c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val predictions not found for 2023-10-25-135320_203870_fold8 (JG1129)\n",
      "test predictions not found for 2023-10-25-135320_203870_fold8 (JG1129)\n",
      "val predictions not found for 2023-10-25-135320_203870_fold7 (JG1129)\n",
      "test predictions not found for 2023-10-25-135320_203870_fold7 (JG1129)\n",
      "val predictions not found for 2023-10-25-135320_203870_fold6 (JG1129)\n",
      "test predictions not found for 2023-10-25-135320_203870_fold6 (JG1129)\n",
      "val predictions not found for 2023-10-25-135320_203870_fold5 (JG1129)\n",
      "test predictions not found for 2023-10-25-135320_203870_fold5 (JG1129)\n",
      "val predictions not found for 2023-10-25-135320_203870_fold4 (JG1129)\n",
      "test predictions not found for 2023-10-25-135320_203870_fold4 (JG1129)\n",
      "val predictions not found for 2023-10-25-135320_203870_fold3 (JG1129)\n",
      "test predictions not found for 2023-10-25-135320_203870_fold3 (JG1129)\n",
      "val predictions not found for 2023-10-25-135320_203870_fold2 (JG1129)\n",
      "test predictions not found for 2023-10-25-135320_203870_fold2 (JG1129)\n",
      "val predictions not found for 2023-10-25-135320_203870_fold1 (JG1129)\n",
      "test predictions not found for 2023-10-25-135320_203870_fold1 (JG1129)\n",
      "val predictions not found for 2023-10-25-135320_203870_fold0 (JG1129)\n",
      "test predictions not found for 2023-10-25-135320_203870_fold0 (JG1129)\n"
     ]
    }
   ],
   "source": [
    "avg_precision = []\n",
    "for i, exp in df_exps.iterrows():\n",
    "    # first we check if predicted values are available\n",
    "    val_pred_path = PRED_DIR / exp.run_id / \"val_preds_last.csv\"\n",
    "    test_pred_path = PRED_DIR / exp.run_id / \"test_preds_last.csv\"\n",
    "    \n",
    "    for name, file in zip([\"val\", \"test\"], [val_pred_path, test_pred_path]):\n",
    "        if file.is_file():\n",
    "            # import predictions and combine with ground truth\n",
    "            df = pd.read_csv(file, index_col=\"idx\").merge(df_true, how=\"left\", left_index=True, right_index=True)\n",
    "            \n",
    "            # extract predictions\n",
    "            if exp[\"training.task\"] in [\"multilabel\", \"binary\"]:\n",
    "                y_prob = df[[f\"pred_{i}\" for i in range(len(exp[\"target_names\"]))]].to_numpy()\n",
    "                y_hat = (y_prob > 0.5).astype(np.int_)\n",
    "                y_true = df[exp[\"target_names\"]].to_numpy()\n",
    "            elif exp[\"training.task\"] == \"multiclass\":\n",
    "                y_prob = df[[f\"pred_{i}\" for i in range(len(le.classes_))]].to_numpy()\n",
    "                y_hat = np.argmax(y_prob, axis=1)\n",
    "                y_true = le.transform(df[\"major_A-C\"].to_numpy())\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected run_type\")\n",
    "        \n",
    "            # calculate additional metrics\n",
    "            if exp[\"training.task\"] == \"multilabel\":\n",
    "                avg_precision.append(average_precision_score(y_true, y_prob, average=\"macro\") )       \n",
    "            elif exp[\"training.task\"] == \"binary\":\n",
    "                # currently not needed\n",
    "                ...\n",
    "            elif exp[\"training.task\"] == \"multiclass\":\n",
    "                # currently not needed\n",
    "                ...\n",
    "        else:\n",
    "            print(f\"{name} predictions not found for {exp.run_id} ({exp.experiment_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "512e5713-d179-4df3-8d33-3f86301c8027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8606858092918435,\n",
       " 0.939935396961079,\n",
       " 0.9426246803901227,\n",
       " 0.6814280094921755,\n",
       " 0.8718325828935735,\n",
       " 0.7668198187203904,\n",
       " 0.9078859310546591,\n",
       " 0.8893935999766921,\n",
       " 0.8986881863030569,\n",
       " 0.8705959829509929,\n",
       " 0.899278411641179,\n",
       " 0.9166458771759424,\n",
       " 0.819170328677186,\n",
       " 0.84844715283895,\n",
       " 0.9238185780887608,\n",
       " 0.8505691027622345,\n",
       " 0.8236051543044508,\n",
       " 0.9009267951065478,\n",
       " 0.8431993605491148,\n",
       " 0.9322388093134286,\n",
       " 0.9295661740412372,\n",
       " 0.7930076255136319,\n",
       " 0.898223033144368,\n",
       " 0.7435044059802235,\n",
       " 0.8845574774689853,\n",
       " 0.8964382073648984,\n",
       " 0.8873453813268656,\n",
       " 0.8638725342304555,\n",
       " 0.9078190376000951,\n",
       " 0.9165069250661829,\n",
       " 0.8677211437984745,\n",
       " 0.8590571179849403,\n",
       " 0.9309577431735795,\n",
       " 0.8803839873246893,\n",
       " 0.8617059920625434,\n",
       " 0.8892900297074432]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e643fc47-b59f-4454-b099-15b10a06dbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
