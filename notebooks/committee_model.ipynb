{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Try committee model\n",
    "\n",
    "We found that generally the following three models perform with insignificant differences:\n",
    "- D-MPNN/CGR\n",
    "- XGB/FP\n",
    "- LogReg/FP\n",
    "\n",
    "If we were to use these as a committee (i.e. averaging probabilities from all three predictions), can we get better predictions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "sys.path.append(str(pathlib.Path().absolute().parent))\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import wilcoxon\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "from src.util.definitions import PRED_DIR, DATA_ROOT\n",
    "from utils import get_runs_as_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = pd.read_csv(DATA_ROOT / \"synferm_dataset_2023-09-05_40018records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_list, config_list, tag_list, name_list  = get_runs_as_list(filters={\"jobType\": \"hparam_best\"}\n",
    "                                                                   )\n",
    "df_all = pd.json_normalize(config_list).merge(pd.json_normalize(summary_list), left_index=True, right_index=True)\n",
    "df_all[\"tags\"] = tag_list\n",
    "df_all[\"run_id\"] = name_list\n",
    "df_all[\"run_group\"] = [s.rsplit(\"_\", maxsplit=1)[0] for s in name_list]\n",
    "df_all[\"Model+Features\"] = df_all[\"name\"] + \"/\" + df_all[\"decoder.global_features\"].str.join(\"+\").str.replace(\"None\", \"CGR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check available experiments by split\n",
    "for tag, row in df_all.groupby(\"tags\")[[\"experiment_id\"]].agg(set).iterrows():\n",
    "    print(tag, \"-->\", row[\"experiment_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_ids = [\"JG1101\", \"JG1129\", \"JG1186\"]  # D-MPNN/CGR, XGB/FP, LogReg/FP for 1D\n",
    "df_exps = df_all.loc[df_all.experiment_id.isin(exp_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_avg_precision = []\n",
    "for fold in range(9):\n",
    "    val_preds = []\n",
    "    test_preds = []\n",
    "    # obtain all experiments for that fold\n",
    "    for _, exp in df_exps.loc[df_exps[\"run_id\"].str[-1] == str(fold)].iterrows():\n",
    "        # first we check if predicted values are available\n",
    "        val_pred_path = PRED_DIR / exp.run_id / \"val_preds_last.csv\"\n",
    "        test_pred_path = PRED_DIR / exp.run_id / \"test_preds_last.csv\"\n",
    "    \n",
    "        for name, file, preds in zip([\"val\", \"test\"], [val_pred_path, test_pred_path], [val_preds, test_preds]):\n",
    "            if file.is_file():\n",
    "                # import predictions\n",
    "                df = pd.read_csv(file, index_col=\"idx\")\n",
    "                preds.append(df)\n",
    "            else:\n",
    "                print(f\"{name} predictions not found for {exp.run_id} ({exp.experiment_id})\")\n",
    "    # merge all the predictions and the ground truth\n",
    "    val = pd.concat(val_preds, keys=[\"modelA\", \"modelB\", \"modelC\",], axis=1).merge(pd.concat([df_true], axis=1, keys=[\"true\"]), how=\"left\", left_index=True, right_index=True)\n",
    "    test = pd.concat(test_preds, keys=[\"modelA\", \"modelB\", \"modelC\",], axis=1).merge(pd.concat([df_true], axis=1, keys=[\"true\"]), how=\"left\", left_index=True, right_index=True)\n",
    "    \n",
    "    \n",
    "    # add the committee predictions\n",
    "    for i in range(3):\n",
    "        val[\"committee\", f\"pred_{i}\"] = (val[\"modelA\", f\"pred_{i}\"] + val[\"modelB\", f\"pred_{i}\"] + val[\"modelC\", f\"pred_{i}\"]) / 3  # simply take the mean\n",
    "        test[\"committee\", f\"pred_{i}\"] = (test[\"modelA\", f\"pred_{i}\"] + test[\"modelB\", f\"pred_{i}\"] + test[\"modelC\", f\"pred_{i}\"]) / 3  # simply take the mean\n",
    "    \n",
    "    \n",
    "    # calculate metrics for the committee model\n",
    "    \n",
    "    # extract predictions\n",
    "    y_prob = val[\"committee\"].to_numpy()\n",
    "    y_hat = (y_prob > 0.5).astype(np.int_)\n",
    "    y_true = val[\"true\"][[\"binary_A\", \"binary_B\", \"binary_C\"]].to_numpy()\n",
    "    \n",
    "    # calculate metric\n",
    "    val_avg_precision.append(average_precision_score(y_true, y_prob, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check mean and std for committee model on val set\n",
    "print(\"Committee model:\")\n",
    "print(np.mean(val_avg_precision), np.std(val_avg_precision), sep=\"±\")\n",
    "print()\n",
    "\n",
    "# check mean and std for constituent models\n",
    "print(\"Constituent models:\")\n",
    "print(df_exps.groupby([\"Model+Features\"])[\"val/avgPrecision_macro\"].aggregate([np.mean, np.std]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the committee model significantly different from the individual models?\n",
    "for model in df_exps[\"Model+Features\"].drop_duplicates():\n",
    "    metrics_model = df_exps.loc[df_exps[\"Model+Features\"] == model].sort_values(by=\"run_id\")[\"val/avgPrecision_macro\"].to_numpy()\n",
    "    print(model, \":\", wilcoxon(val_avg_precision, metrics_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Conclusion 1D\n",
    "The committee model is has the highest mean score with second lowest std.\n",
    "It is significantly better than LogReg/FP, but not better than the other two models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 2D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_ids = [\"JG1102\", \"JG1130\", \"JG1112\"]  # D-MPNN/CGR, XGB/FP, LogReg/FP for 2D\n",
    "df_exps = df_all.loc[df_all.experiment_id.isin(exp_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_avg_precision = []\n",
    "for fold in range(9):\n",
    "    val_preds = []\n",
    "    test_preds = []\n",
    "    # obtain all experiments for that fold\n",
    "    for _, exp in df_exps.loc[df_exps[\"run_id\"].str[-1] == str(fold)].iterrows():\n",
    "        # first we check if predicted values are available\n",
    "        val_pred_path = PRED_DIR / exp.run_id / \"val_preds_last.csv\"\n",
    "        test_pred_path = PRED_DIR / exp.run_id / \"test_preds_last.csv\"\n",
    "    \n",
    "        for name, file, preds in zip([\"val\", \"test\"], [val_pred_path, test_pred_path], [val_preds, test_preds]):\n",
    "            if file.is_file():\n",
    "                # import predictions\n",
    "                df = pd.read_csv(file, index_col=\"idx\")\n",
    "                preds.append(df)\n",
    "            else:\n",
    "                print(f\"{name} predictions not found for {exp.run_id} ({exp.experiment_id})\")\n",
    "    # merge all the predictions and the ground truth\n",
    "    val = pd.concat(val_preds, keys=[\"modelA\", \"modelB\", \"modelC\",], axis=1).merge(pd.concat([df_true], axis=1, keys=[\"true\"]), how=\"left\", left_index=True, right_index=True)\n",
    "    test = pd.concat(test_preds, keys=[\"modelA\", \"modelB\", \"modelC\",], axis=1).merge(pd.concat([df_true], axis=1, keys=[\"true\"]), how=\"left\", left_index=True, right_index=True)\n",
    "    \n",
    "    \n",
    "    # add the committee predictions\n",
    "    for i in range(3):\n",
    "        val[\"committee\", f\"pred_{i}\"] = (val[\"modelA\", f\"pred_{i}\"] + val[\"modelB\", f\"pred_{i}\"] + val[\"modelC\", f\"pred_{i}\"]) / 3  # simply take the mean\n",
    "        test[\"committee\", f\"pred_{i}\"] = (test[\"modelA\", f\"pred_{i}\"] + test[\"modelB\", f\"pred_{i}\"] + test[\"modelC\", f\"pred_{i}\"]) / 3  # simply take the mean\n",
    "    \n",
    "    \n",
    "    # calculate metrics for the committee model\n",
    "    \n",
    "    # extract predictions\n",
    "    y_prob = val[\"committee\"].to_numpy()\n",
    "    y_hat = (y_prob > 0.5).astype(np.int_)\n",
    "    y_true = val[\"true\"][[\"binary_A\", \"binary_B\", \"binary_C\"]].to_numpy()\n",
    "    \n",
    "    # calculate metric\n",
    "    val_avg_precision.append(average_precision_score(y_true, y_prob, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check mean and std for committee model on val set\n",
    "print(\"Committee model:\")\n",
    "print(np.mean(val_avg_precision), np.std(val_avg_precision), sep=\"±\")\n",
    "print()\n",
    "\n",
    "# check mean and std for constituent models\n",
    "print(\"Constituent models:\")\n",
    "print(df_exps.groupby([\"Model+Features\"])[\"val/avgPrecision_macro\"].aggregate([np.mean, np.std]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the committee model significantly different from the individual models?\n",
    "for model in df_exps[\"Model+Features\"].drop_duplicates():\n",
    "    metrics_model = df_exps.loc[df_exps[\"Model+Features\"] == model].sort_values(by=\"run_id\")[\"val/avgPrecision_macro\"].to_numpy()\n",
    "    print(model, \":\", wilcoxon(val_avg_precision, metrics_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Conclusion 2D\n",
    "The committee model has the highest mean score and the lowest std.\n",
    "It is significantly better than the LogReg/FP model, but not different from the other two models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 3D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_ids = [\"JG1103\", \"JG1111\", \"JG1108\"]  # D-MPNN/CGR, XGB/FP, LogReg/FP for 3D\n",
    "df_exps = df_all.loc[df_all.experiment_id.isin(exp_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_avg_precision = []\n",
    "for fold in range(9):\n",
    "    val_preds = []\n",
    "    test_preds = []\n",
    "    # obtain all experiments for that fold\n",
    "    for _, exp in df_exps.loc[df_exps[\"run_id\"].str[-1] == str(fold)].iterrows():\n",
    "        # first we check if predicted values are available\n",
    "        val_pred_path = PRED_DIR / exp.run_id / \"val_preds_last.csv\"\n",
    "        test_pred_path = PRED_DIR / exp.run_id / \"test_preds_last.csv\"\n",
    "    \n",
    "        for name, file, preds in zip([\"val\", \"test\"], [val_pred_path, test_pred_path], [val_preds, test_preds]):\n",
    "            if file.is_file():\n",
    "                # import predictions\n",
    "                df = pd.read_csv(file, index_col=\"idx\")\n",
    "                preds.append(df)\n",
    "            else:\n",
    "                print(f\"{name} predictions not found for {exp.run_id} ({exp.experiment_id})\")\n",
    "    # merge all the predictions and the ground truth\n",
    "    val = pd.concat(val_preds, keys=[\"modelA\", \"modelB\", \"modelC\",], axis=1).merge(pd.concat([df_true], axis=1, keys=[\"true\"]), how=\"left\", left_index=True, right_index=True)\n",
    "    test = pd.concat(test_preds, keys=[\"modelA\", \"modelB\", \"modelC\",], axis=1).merge(pd.concat([df_true], axis=1, keys=[\"true\"]), how=\"left\", left_index=True, right_index=True)\n",
    "    \n",
    "    \n",
    "    # add the committee predictions\n",
    "    for i in range(3):\n",
    "        val[\"committee\", f\"pred_{i}\"] = (val[\"modelA\", f\"pred_{i}\"] + val[\"modelB\", f\"pred_{i}\"] + val[\"modelC\", f\"pred_{i}\"]) / 3  # simply take the mean\n",
    "        test[\"committee\", f\"pred_{i}\"] = (test[\"modelA\", f\"pred_{i}\"] + test[\"modelB\", f\"pred_{i}\"] + test[\"modelC\", f\"pred_{i}\"]) / 3  # simply take the mean\n",
    "    \n",
    "    \n",
    "    # calculate metrics for the committee model\n",
    "    \n",
    "    # extract predictions\n",
    "    y_prob = val[\"committee\"].to_numpy()\n",
    "    y_hat = (y_prob > 0.5).astype(np.int_)\n",
    "    y_true = val[\"true\"][[\"binary_A\", \"binary_B\", \"binary_C\"]].to_numpy()\n",
    "    \n",
    "    # calculate metric\n",
    "    val_avg_precision.append(average_precision_score(y_true, y_prob, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check mean and std for committee model on val set\n",
    "print(\"Committee model:\")\n",
    "print(np.mean(val_avg_precision), np.std(val_avg_precision), sep=\"±\")\n",
    "print()\n",
    "\n",
    "# check mean and std for constituent models\n",
    "print(\"Constituent models:\")\n",
    "print(df_exps.groupby([\"Model+Features\"])[\"val/avgPrecision_macro\"].aggregate([np.mean, np.std]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the committee model significantly different from the individual models?\n",
    "for model in df_exps[\"Model+Features\"].drop_duplicates():\n",
    "    metrics_model = df_exps.loc[df_exps[\"Model+Features\"] == model].sort_values(by=\"run_id\")[\"val/avgPrecision_macro\"].to_numpy()\n",
    "    print(model, \":\", wilcoxon(val_avg_precision, metrics_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Conclusion 3D\n",
    "The committee model has the highest mean score but the second worst standard deviation.\n",
    "It is significantly better than D-MPNN/CGR and LogReg/FP. It is not different from XGB/FP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Several things can be seen her (some corroborate findings from other experiments):\n",
    "\n",
    "- The committee model is equal or better than all of its constitutuents\n",
    "- The committee model is better than SOME of its constituents\n",
    "- XGB is the most reliable model across all different situations\n",
    "- D-MPNN model is not suitable for the 3D problem due to lack of data\n",
    "- In 1D and 2D situations, where lots of data is available, XGB and D-MPNN outperform the simpler Logistic Regression.\n",
    "  It is not fully clear how much this is due to XGB and D-MPNN profiting from larger number of samples vs. abusing combinatorial information.\n",
    "\n",
    "### So should we use a committee model?\n",
    "- Performance-wise the answer is clearly yes.\n",
    "- On the flipside the committee model is more expensive to use in inference and more complex, adding possible points of failure\n",
    "- The committee model never significantly outperforms XGB/FP alone, using only XGB/FP seems to be the logical compromise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
