{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e00d24-7eac-424d-af8f-b7db7c1dde0f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Try committee model\n",
    "\n",
    "We found that generally the following three models perform with insignificant differences:\n",
    "- D-MPNN/CGR\n",
    "- XGB/FP\n",
    "- LogReg/FP\n",
    "\n",
    "If we were to use these as a committee (i.e. averaging probabilities from all three predictions), do we get better predictions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87dc94cb-9107-45f6-a94e-2e10ffc8adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "sys.path.append(str(pathlib.Path(\"__file__\").absolute().parents[1]))\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import wilcoxon\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "from src.util.definitions import PRED_DIR, DATA_ROOT\n",
    "from utils import get_runs_as_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ec07c75-d20c-4a22-a311-76b7a6149adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = pd.read_csv(DATA_ROOT / \"synferm_dataset_2023-09-05_40018records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccfe90dc-3e1f-4b17-a6f9-61de7056877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_list, config_list, tag_list, name_list  = get_runs_as_list(filters={\"jobType\": \"hparam_best\"}\n",
    "                                                                   )\n",
    "df_all = pd.json_normalize(config_list).merge(pd.json_normalize(summary_list), left_index=True, right_index=True)\n",
    "df_all[\"tags\"] = tag_list\n",
    "df_all[\"run_id\"] = name_list\n",
    "df_all[\"run_group\"] = [s.rsplit(\"_\", maxsplit=1)[0] for s in name_list]\n",
    "df_all[\"Model+Features\"] = df_all[\"name\"] + \"/\" + df_all[\"decoder.global_features\"].str.join(\"+\").str.replace(\"None\", \"CGR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90837f9a-d05f-4e0c-a272-a83d32833004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0D',) --> {'JG1160', 'JG1116', 'JG1117', 'JG1106', 'JG1131', 'JG1100', 'JG1128', 'JG1135', 'JG1115', 'JG1109', 'JG1185'}\n",
      "('0D_1.25',) --> {'JG1153', 'JG1141', 'JG1159', 'JG1147'}\n",
      "('0D_10',) --> {'JG1144', 'JG1138', 'JG1150', 'JG1156'}\n",
      "('0D_2.5',) --> {'JG1146', 'JG1140', 'JG1158', 'JG1152'}\n",
      "('0D_20',) --> {'JG1149', 'JG1137', 'JG1155', 'JG1143'}\n",
      "('0D_40',) --> {'JG1136', 'JG1142', 'JG1154', 'JG1148'}\n",
      "('0D_5',) --> {'JG1157', 'JG1139', 'JG1151', 'JG1145'}\n",
      "('1D',) --> {'JG1125', 'JG1118', 'JG1101', 'JG1123', 'JG1104', 'JG1129', 'JG1132', 'JG1121', 'JG1186', 'JG1126'}\n",
      "('1D_10',) --> {'JG1164', 'JG1182', 'JG1170', 'JG1176'}\n",
      "('1D_2.5',) --> {'JG1184', 'JG1178', 'JG1166', 'JG1172'}\n",
      "('1D_20',) --> {'JG1175', 'JG1169', 'JG1163', 'JG1181'}\n",
      "('1D_40',) --> {'JG1162', 'JG1174', 'JG1180', 'JG1168'}\n",
      "('1D_5',) --> {'JG1177', 'JG1165', 'JG1171', 'JG1183'}\n",
      "('1D_80',) --> {'JG1167', 'JG1173', 'JG1179', 'JG1161'}\n",
      "('2D',) --> {'JG1124', 'JG1112', 'JG1130', 'JG1133', 'JG1105', 'JG1119', 'JG1102', 'JG1122', 'JG1127'}\n",
      "('3D',) --> {'JG1134', 'JG1111', 'JG1110', 'JG1113', 'JG1108', 'JG1107', 'JG1103', 'JG1114', 'JG1120'}\n"
     ]
    }
   ],
   "source": [
    "# check available experiments by split\n",
    "for tag, row in df_all.groupby(\"tags\")[[\"experiment_id\"]].agg(set).iterrows():\n",
    "    print(tag, \"-->\", row[\"experiment_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35154496-7965-46e8-ac6c-ba256d309e24",
   "metadata": {},
   "source": [
    "## 1D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "af68f68f-ba61-44b0-93d6-ad86f104353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_ids = [\"JG1101\", \"JG1129\", \"JG1186\"]  # D-MPNN/CGR, XGB/FP, LogReg/FP for 1D\n",
    "df_exps = df_all.loc[df_all.experiment_id.isin(exp_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "764c4baa-e948-4efc-8cc1-491c3f36c7c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_avg_precision = []\n",
    "for fold in range(9):\n",
    "    val_preds = []\n",
    "    test_preds = []\n",
    "    # obtain all experiments for that fold\n",
    "    for _, exp in df_exps.loc[df_exps[\"run_id\"].str[-1] == str(fold)].iterrows():\n",
    "        # first we check if predicted values are available\n",
    "        val_pred_path = PRED_DIR / exp.run_id / \"val_preds_last.csv\"\n",
    "        test_pred_path = PRED_DIR / exp.run_id / \"test_preds_last.csv\"\n",
    "    \n",
    "        for name, file, preds in zip([\"val\", \"test\"], [val_pred_path, test_pred_path], [val_preds, test_preds]):\n",
    "            if file.is_file():\n",
    "                # import predictions\n",
    "                df = pd.read_csv(file, index_col=\"idx\")\n",
    "                preds.append(df)\n",
    "            else:\n",
    "                print(f\"{name} predictions not found for {exp.run_id} ({exp.experiment_id})\")\n",
    "    # merge all the predictions and the ground truth\n",
    "    val = pd.concat(val_preds, keys=[\"modelA\", \"modelB\", \"modelC\",], axis=1).merge(pd.concat([df_true], axis=1, keys=[\"true\"]), how=\"left\", left_index=True, right_index=True)\n",
    "    test = pd.concat(test_preds, keys=[\"modelA\", \"modelB\", \"modelC\",], axis=1).merge(pd.concat([df_true], axis=1, keys=[\"true\"]), how=\"left\", left_index=True, right_index=True)\n",
    "    \n",
    "    \n",
    "    # add the committee predictions\n",
    "    for i in range(3):\n",
    "        val[\"committee\", f\"pred_{i}\"] = (val[\"modelA\", f\"pred_{i}\"] + val[\"modelB\", f\"pred_{i}\"] + val[\"modelC\", f\"pred_{i}\"]) / 3  # simply take the mean\n",
    "        test[\"committee\", f\"pred_{i}\"] = (test[\"modelA\", f\"pred_{i}\"] + test[\"modelB\", f\"pred_{i}\"] + test[\"modelC\", f\"pred_{i}\"]) / 3  # simply take the mean\n",
    "    \n",
    "    \n",
    "    # calculate metrics for the committee model\n",
    "    \n",
    "    # extract predictions\n",
    "    y_prob = val[\"committee\"].to_numpy()\n",
    "    y_hat = (y_prob > 0.5).astype(np.int_)\n",
    "    y_true = val[\"true\"][[\"binary_A\", \"binary_B\", \"binary_C\"]].to_numpy()\n",
    "    \n",
    "    # calculate metric\n",
    "    val_avg_precision.append(average_precision_score(y_true, y_prob, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0569391c-529c-4b75-8a0f-acdca1343903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Committee model:\n",
      "0.8919505789953232±0.03353670577237679\n",
      "\n",
      "Constituent models:\n",
      "                           mean       std\n",
      "Model+Features                           \n",
      "D-MPNN/CGR             0.890784  0.030425\n",
      "LogisticRegression/FP  0.871912  0.043093\n",
      "XGB/FP                 0.883578  0.042024\n"
     ]
    }
   ],
   "source": [
    "# check mean and std for committee model on val set\n",
    "print(\"Committee model:\")\n",
    "print(np.mean(val_avg_precision), np.std(val_avg_precision), sep=\"±\")\n",
    "print()\n",
    "\n",
    "# check mean and std for constituent models\n",
    "print(\"Constituent models:\")\n",
    "print(df_exps.groupby([\"Model+Features\"])[\"val/avgPrecision_macro\"].aggregate([np.mean, np.std]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5b7774cc-524d-4a7d-8ac0-bd3968ac0fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB/FP : WilcoxonResult(statistic=11.0, pvalue=0.203125)\n",
      "LogisticRegression/FP : WilcoxonResult(statistic=0.0, pvalue=0.00390625)\n",
      "D-MPNN/CGR : WilcoxonResult(statistic=22.0, pvalue=1.0)\n"
     ]
    }
   ],
   "source": [
    "# is the committee model significantly different from the individual models?\n",
    "for model in df_exps[\"Model+Features\"].drop_duplicates():\n",
    "    metrics_model = df_exps.loc[df_exps[\"Model+Features\"] == model].sort_values(by=\"run_id\")[\"val/avgPrecision_macro\"].to_numpy()\n",
    "    print(model, \":\", wilcoxon(val_avg_precision, metrics_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc32162-6988-43e5-9f47-005349c745be",
   "metadata": {},
   "source": [
    "### Conclusion 1D\n",
    "The committee model is has the highest mean score with second lowest std.\n",
    "It is significantly better than LogReg/FP, but not better than the other two models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81285b-39f1-41e4-b454-c2cda217e28a",
   "metadata": {},
   "source": [
    "## 2D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4bf28259-acd0-4c47-9b53-5dedf5988daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_ids = [\"JG1102\", \"JG1130\", \"JG1112\"]  # D-MPNN/CGR, XGB/FP, LogReg/FP for 2D\n",
    "df_exps = df_all.loc[df_all.experiment_id.isin(exp_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "74b05316-178b-44f4-baa3-56bd05cd13bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_avg_precision = []\n",
    "for fold in range(9):\n",
    "    val_preds = []\n",
    "    test_preds = []\n",
    "    # obtain all experiments for that fold\n",
    "    for _, exp in df_exps.loc[df_exps[\"run_id\"].str[-1] == str(fold)].iterrows():\n",
    "        # first we check if predicted values are available\n",
    "        val_pred_path = PRED_DIR / exp.run_id / \"val_preds_last.csv\"\n",
    "        test_pred_path = PRED_DIR / exp.run_id / \"test_preds_last.csv\"\n",
    "    \n",
    "        for name, file, preds in zip([\"val\", \"test\"], [val_pred_path, test_pred_path], [val_preds, test_preds]):\n",
    "            if file.is_file():\n",
    "                # import predictions\n",
    "                df = pd.read_csv(file, index_col=\"idx\")\n",
    "                preds.append(df)\n",
    "            else:\n",
    "                print(f\"{name} predictions not found for {exp.run_id} ({exp.experiment_id})\")\n",
    "    # merge all the predictions and the ground truth\n",
    "    val = pd.concat(val_preds, keys=[\"modelA\", \"modelB\", \"modelC\",], axis=1).merge(pd.concat([df_true], axis=1, keys=[\"true\"]), how=\"left\", left_index=True, right_index=True)\n",
    "    test = pd.concat(test_preds, keys=[\"modelA\", \"modelB\", \"modelC\",], axis=1).merge(pd.concat([df_true], axis=1, keys=[\"true\"]), how=\"left\", left_index=True, right_index=True)\n",
    "    \n",
    "    \n",
    "    # add the committee predictions\n",
    "    for i in range(3):\n",
    "        val[\"committee\", f\"pred_{i}\"] = (val[\"modelA\", f\"pred_{i}\"] + val[\"modelB\", f\"pred_{i}\"] + val[\"modelC\", f\"pred_{i}\"]) / 3  # simply take the mean\n",
    "        test[\"committee\", f\"pred_{i}\"] = (test[\"modelA\", f\"pred_{i}\"] + test[\"modelB\", f\"pred_{i}\"] + test[\"modelC\", f\"pred_{i}\"]) / 3  # simply take the mean\n",
    "    \n",
    "    \n",
    "    # calculate metrics for the committee model\n",
    "    \n",
    "    # extract predictions\n",
    "    y_prob = val[\"committee\"].to_numpy()\n",
    "    y_hat = (y_prob > 0.5).astype(np.int_)\n",
    "    y_true = val[\"true\"][[\"binary_A\", \"binary_B\", \"binary_C\"]].to_numpy()\n",
    "    \n",
    "    # calculate metric\n",
    "    val_avg_precision.append(average_precision_score(y_true, y_prob, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "45440745-4e36-4693-bc6f-e12922c5e73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Committee model:\n",
      "0.809958986512768±0.11051314722498096\n",
      "\n",
      "Constituent models:\n",
      "                           mean       std\n",
      "Model+Features                           \n",
      "D-MPNN/CGR             0.809626  0.124337\n",
      "LogisticRegression/FP  0.774621  0.122331\n",
      "XGB/FP                 0.790416  0.112848\n"
     ]
    }
   ],
   "source": [
    "# check mean and std for committee model on val set\n",
    "print(\"Committee model:\")\n",
    "print(np.mean(val_avg_precision), np.std(val_avg_precision), sep=\"±\")\n",
    "print()\n",
    "\n",
    "# check mean and std for constituent models\n",
    "print(\"Constituent models:\")\n",
    "print(df_exps.groupby([\"Model+Features\"])[\"val/avgPrecision_macro\"].aggregate([np.mean, np.std]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3d3d49bf-f6d6-4f12-b7da-4b45c133b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression/FP : WilcoxonResult(statistic=0.0, pvalue=0.00390625)\n",
      "XGB/FP : WilcoxonResult(statistic=7.0, pvalue=0.07421875)\n",
      "D-MPNN/CGR : WilcoxonResult(statistic=22.0, pvalue=1.0)\n"
     ]
    }
   ],
   "source": [
    "# is the committee model significantly different from the individual models?\n",
    "for model in df_exps[\"Model+Features\"].drop_duplicates():\n",
    "    metrics_model = df_exps.loc[df_exps[\"Model+Features\"] == model].sort_values(by=\"run_id\")[\"val/avgPrecision_macro\"].to_numpy()\n",
    "    print(model, \":\", wilcoxon(val_avg_precision, metrics_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b2d062-7fdf-4e25-9a22-f2d01f035e28",
   "metadata": {},
   "source": [
    "### Conclusion 2D\n",
    "The committee model has the highest mean score and the lowest std.\n",
    "It is significantly better than the LogReg/FP model, but not different from the other two models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439ffd22-86ce-4fbb-87a3-9f0e591f6888",
   "metadata": {},
   "source": [
    "## 3D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "51b0c429-15bb-42b1-9001-76eb6d84c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_ids = [\"JG1103\", \"JG1111\", \"JG1108\"]  # D-MPNN/CGR, XGB/FP, LogReg/FP for 3D\n",
    "df_exps = df_all.loc[df_all.experiment_id.isin(exp_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b24803d0-8478-435c-8817-e3e007360daf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_avg_precision = []\n",
    "for fold in range(9):\n",
    "    val_preds = []\n",
    "    test_preds = []\n",
    "    # obtain all experiments for that fold\n",
    "    for _, exp in df_exps.loc[df_exps[\"run_id\"].str[-1] == str(fold)].iterrows():\n",
    "        # first we check if predicted values are available\n",
    "        val_pred_path = PRED_DIR / exp.run_id / \"val_preds_last.csv\"\n",
    "        test_pred_path = PRED_DIR / exp.run_id / \"test_preds_last.csv\"\n",
    "    \n",
    "        for name, file, preds in zip([\"val\", \"test\"], [val_pred_path, test_pred_path], [val_preds, test_preds]):\n",
    "            if file.is_file():\n",
    "                # import predictions\n",
    "                df = pd.read_csv(file, index_col=\"idx\")\n",
    "                preds.append(df)\n",
    "            else:\n",
    "                print(f\"{name} predictions not found for {exp.run_id} ({exp.experiment_id})\")\n",
    "    # merge all the predictions and the ground truth\n",
    "    val = pd.concat(val_preds, keys=[\"modelA\", \"modelB\", \"modelC\",], axis=1).merge(pd.concat([df_true], axis=1, keys=[\"true\"]), how=\"left\", left_index=True, right_index=True)\n",
    "    test = pd.concat(test_preds, keys=[\"modelA\", \"modelB\", \"modelC\",], axis=1).merge(pd.concat([df_true], axis=1, keys=[\"true\"]), how=\"left\", left_index=True, right_index=True)\n",
    "    \n",
    "    \n",
    "    # add the committee predictions\n",
    "    for i in range(3):\n",
    "        val[\"committee\", f\"pred_{i}\"] = (val[\"modelA\", f\"pred_{i}\"] + val[\"modelB\", f\"pred_{i}\"] + val[\"modelC\", f\"pred_{i}\"]) / 3  # simply take the mean\n",
    "        test[\"committee\", f\"pred_{i}\"] = (test[\"modelA\", f\"pred_{i}\"] + test[\"modelB\", f\"pred_{i}\"] + test[\"modelC\", f\"pred_{i}\"]) / 3  # simply take the mean\n",
    "    \n",
    "    \n",
    "    # calculate metrics for the committee model\n",
    "    \n",
    "    # extract predictions\n",
    "    y_prob = val[\"committee\"].to_numpy()\n",
    "    y_hat = (y_prob > 0.5).astype(np.int_)\n",
    "    y_true = val[\"true\"][[\"binary_A\", \"binary_B\", \"binary_C\"]].to_numpy()\n",
    "    \n",
    "    # calculate metric\n",
    "    val_avg_precision.append(average_precision_score(y_true, y_prob, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "93d446ad-619a-4489-b9d8-8f1c786418c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Committee model:\n",
      "0.8064186998158501±0.04332848404326717\n",
      "\n",
      "Constituent models:\n",
      "                           mean       std\n",
      "Model+Features                           \n",
      "D-MPNN/CGR             0.769681  0.056595\n",
      "LogisticRegression/FP  0.794498  0.042385\n",
      "XGB/FP                 0.800110  0.041297\n"
     ]
    }
   ],
   "source": [
    "# check mean and std for committee model on val set\n",
    "print(\"Committee model:\")\n",
    "print(np.mean(val_avg_precision), np.std(val_avg_precision), sep=\"±\")\n",
    "print()\n",
    "\n",
    "# check mean and std for constituent models\n",
    "print(\"Constituent models:\")\n",
    "print(df_exps.groupby([\"Model+Features\"])[\"val/avgPrecision_macro\"].aggregate([np.mean, np.std]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b8fd8034-10f9-4424-a0d7-0e0a1cb57f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression/FP : WilcoxonResult(statistic=5.0, pvalue=0.0390625)\n",
      "XGB/FP : WilcoxonResult(statistic=11.0, pvalue=0.203125)\n",
      "D-MPNN/CGR : WilcoxonResult(statistic=1.0, pvalue=0.0078125)\n"
     ]
    }
   ],
   "source": [
    "# is the committee model significantly different from the individual models?\n",
    "for model in df_exps[\"Model+Features\"].drop_duplicates():\n",
    "    metrics_model = df_exps.loc[df_exps[\"Model+Features\"] == model].sort_values(by=\"run_id\")[\"val/avgPrecision_macro\"].to_numpy()\n",
    "    print(model, \":\", wilcoxon(val_avg_precision, metrics_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb22351-b5c5-4aff-820b-6a26e776b772",
   "metadata": {},
   "source": [
    "### Conclusion 3D\n",
    "The committee model has the highest mean score but the second worst standard deviation.\n",
    "It is significantly better than D-MPNN/CGR and LogReg/FP. It is not different from XGB/FP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d37bd95-76c9-4da8-86a7-13402b1e74b5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Several things can be seen her (some corroborate findings from other experiments):\n",
    "\n",
    "- The committee model is equal or better than all of its constitutuents\n",
    "- The committee model is better than SOME of its constituents\n",
    "- XGB is the most reliable model across all different situations\n",
    "- D-MPNN model is not suitable for the 3D problem due to lack of data\n",
    "- In 1D and 2D situations, where lots of data is available, XGB and D-MPNN outperform the simpler Logistic Regression.\n",
    "  It is not fully clear how much this is due to XGB and D-MPNN profiting from larger number of samples vs. abusing combinatorial information.\n",
    "\n",
    "### So should we use a committee model?\n",
    "- Performance-wise the answer is clearly yes.\n",
    "- On the flipside the committee model is more expensive to use in inference and more complex, adding possible points of failure\n",
    "- The committee model never significantly outperforms XGB/FP alone, using only XGB/FP seems to be the logical compromise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbcb2b2-aaea-4bf2-91ce-849a5560916a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
