{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Inference using the Synthetic Fermentation models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Predict reaction outcome for Synthetic Fermentation products.\n",
    "\n",
    "The input to this are SMILES of the desired product(s).\n",
    "Inputs can be supplied directly as a csv file with one column named \"smiles\" and arbitrary additional columns.\n",
    "\n",
    "The output is:\n",
    "- the reactionSMILES that leads to this product using the SLAP platform\n",
    "- a classification of whether the reaction is expected to work\n",
    "\n",
    "The output is written to a new csv file containing all columns from the input file, and six new columns: `rxn_smiles`, `rxn_prediction`, `rxn_confidence`.\n",
    "\n",
    "Predictions are given as `0` (meaning no reaction expected) or `1` (meaning successful reaction expected). \n",
    "If the reaction was in the acquired data set, the known outcome is returned instead.\n",
    "\n",
    "Confidence is given as an integer in the range `0-4`, with `0` indicating the highest confidence.\n",
    "Confidence is determined based on the complexity of the prediction problem using the following mapping:\n",
    "- `0`: known reaction\n",
    "- `1`: all three reactants known in other reactions\n",
    "- `2`: exactly one reactant known in other reactions\n",
    "- `3`: exactly two reactants known in other reactions\n",
    "- `4`: none of the reactants known in other reactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import statistics\n",
    "import sys\n",
    "sys.path.append(str(pathlib.Path().absolute().parent))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.util.definitions import TRAINED_MODEL_DIR, LOG_DIR, DATA_ROOT\n",
    "from src.model.classifier import load_trained_model\n",
    "from src.data.dataloader import SynFermDataset, collate_fn\n",
    "from reaction_generator import SFReactionGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_smiles(\n",
    "    raw_dir: pathlib.Path, filename: str, valid_idx_file: pathlib.Path = None\n",
    "):\n",
    "    \"\"\"Import smiles from a csv file and filter by indices supplied in a second csv file\"\"\"\n",
    "    smiles_df = pd.read_csv(raw_dir / filename)\n",
    "    if valid_idx_file is None:\n",
    "        return smiles_df\n",
    "    else:\n",
    "        indices_arr = pd.read_csv(valid_idx_file)[\"index\"].to_numpy()\n",
    "        return smiles_df.loc[indices_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to the best models\n",
    "model_0D_name = \"2023-11-20-175433_236136_fold0\"\n",
    "model_0D = LOG_DIR / \"checkpoints\" / model_0D_name / \"last-epoch38-val_loss0.20.ckpt\"  # FFN\n",
    "#model_1D = TRAINED_MODEL_DIR / \"2023-03-06-112027_188465\" / \"best.ckpt\"  # D-MPNN\n",
    "#model_2D = TRAINED_MODEL_DIR / \"2023-03-06-112721_778803\" / \"best.ckpt\"  # D-MPNN\n",
    "#model_3D\n",
    "# path to the OneHotEncoder state for model_0D\n",
    "ohe_state_dict = LOG_DIR / \"OHE_state_dict_KcEovvzIEafcIYUJ.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "To use the notebook on your products, change `raw_dir` to the directory that your CSV file containing SMILES is in. Then change `filename_base` to the filename of your csv file without the `.csv` suffix. If you do not want to use all the SMILES in your file (e.g. because some are not valid SLAP products), suppy a `valid_idx_file`. You can set the value to `None` if you want to use all SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import product SMILES and generate reactionSMILES. This will take some time.\n",
    "raw_dir = DATA_ROOT  # <-- change me\n",
    "filename = \"synferm_dataset_2023-09-05_40018records.csv\"  # <-- change me\n",
    "# remove the .csv extension AND any other extensions behind it (e.g. remove .csv.bz2 or csv.gz)\n",
    "filename_base = filename.split(\".csv\")[0]\n",
    "valid_idx_file = \"../data/splits/synferm_dataset_2023-09-05_0D_split_final-retrain/fold0_val.csv\"  # <-- change me or set me to None\n",
    "df = import_smiles(raw_dir, filename, valid_idx_file=valid_idx_file)\n",
    "#data = SLAPProductDataset(smiles=df[\"smiles\"].values.tolist())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "df.to_csv(DATA_ROOT / \"inference_test.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the moment we assume everything is 0D data\n",
    "data = SynFermDataset(\n",
    "    name=\"inference_test.csv\",\n",
    "    raw_dir=DATA_ROOT,\n",
    "    reaction=True,\n",
    "    global_features=[\"OHE\", ],\n",
    "    global_featurizer_state_dict_path=ohe_state_dict,\n",
    "    graph_type=\"bond_edges\",\n",
    "    featurizers=\"custom\",\n",
    "    smiles_columns=[\"reaction_smiles_atom_mapped\"],\n",
    "    label_columns=None,\n",
    "    task=\"multilabel\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the predictions\n",
    "\n",
    "# load the trained model \n",
    "model_0D = load_trained_model(\"FFN\", model_0D)\n",
    "model_0D.eval()\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", logger=False, max_epochs=-1)\n",
    "# prepare data\n",
    "dl = DataLoader(data, collate_fn=collate_fn, num_workers=0)\n",
    "# predict\n",
    "probabilities_0D = torch.sigmoid(torch.concat(trainer.predict(model_0D, dl)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_0D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load decision thresholds\n",
    "with open(LOG_DIR / \"thresholds\" / f\"{model_0D_name}.txt\", \"r\") as f:\n",
    "    thresholds = [float(i) for i in f.readlines()]\n",
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the thresholds\n",
    "preds = torch.stack([torch.where(probabilities_0D[:, i] > thresholds[i], 1, 0) for i in range(3)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine with data\n",
    "df[[\"prob_A\", \"prob_B\", \"prob_C\"]] = probabilities_0D\n",
    "df[[\"pred_A\", \"pred_B\", \"pred_C\"]] = preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check accuracy\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, recall_score, precision_score, fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL: check that we still obtain the same metrics\n",
    "y_true = df[\"binary_A\"]\n",
    "y_pred = df[\"pred_A\"]\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "fbeta = fbeta_score(y_true, y_pred, beta=0.5)\n",
    "\n",
    "print(\"accuracy:\", f\"{acc:.2f}\")\n",
    "print(\"balanced accuracy:\", f\"{bal_acc:.2f}\")\n",
    "print(\"recall:\", f\"{recall:.2f}\")\n",
    "print(\"precision:\", f\"{precision:.2f}\")\n",
    "print(\"f_0.5 score:\", f\"{fbeta:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble outputs\n",
    "predictions = np.full(len(data.reactions), np.nan, dtype=float)\n",
    "\n",
    "predictions[data.idx_known] = [statistics.mean(data.known_outcomes[i]) for i in data.idx_known]  # for known reaction we add the average reaction outcome\n",
    "try:\n",
    "    predictions[data.idx_0D] = predictions_0D\n",
    "except NameError:\n",
    "    pass\n",
    "try:\n",
    "    predictions[data.idx_1D_slap] = predictions_1D_slap\n",
    "except NameError:\n",
    "    pass\n",
    "try:\n",
    "    predictions[data.idx_1D_aldehyde] = predictions_1D_aldehyde\n",
    "except NameError:\n",
    "    pass\n",
    "try:\n",
    "    predictions[data.idx_2D] = predictions_2D\n",
    "except NameError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we have not predicted for anything\n",
    "# this should be only the reactions in data.invalid_idxs\n",
    "rxn_idxs_no_pred = np.argwhere(np.isnan(predictions)).flatten()\n",
    "\n",
    "rxn_idxs_invalid = [data.product_idxs.index(i) for i in data.invalid_idxs]\n",
    "\n",
    "assert set(rxn_idxs_no_pred) == set(rxn_idxs_invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain individual new columns for output df\n",
    "df[\"rxn1_smiles\"] = [data.reactions[i] for i in arr[:,0]]\n",
    "\n",
    "df[\"rxn1_predictions\"] = [predictions[i] for i in arr[:,0]]\n",
    "\n",
    "df[\"rxn1_confidence\"] = [rxn_problem_types[i] for i in arr[:,0]]\n",
    "\n",
    "df[\"rxn2_smiles\"] = [reactions_augmented[i] for i in arr[:,1]]\n",
    "\n",
    "df[\"rxn2_predictions\"] = [predictions_augmented[i] for i in arr[:,1]]\n",
    "\n",
    "df[\"rxn2_confidence\"] = [rxn_problem_types_augmented[i] for i in arr[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataset statistics for control to log file (+ optionally print)\n",
    "verbose = True\n",
    "log_output = f\"\"\"\\\n",
    "{len(data.reactions)} reactions generated from {len(data.smiles)} input SMILES\n",
    "Known reactions: {(sum(x is not None for x in data.known_outcomes))}\n",
    "0D reactions: 0, thereof 0 predicted positive\n",
    "1D reactions with unknown aldehyde: {len(data.dataset_1D_aldehyde)}, thereof {np.count_nonzero(predictions_1D_aldehyde)} predicted positive\n",
    "1D reactions with unknown SLAP reagent: {len(data.dataset_1D_slap)}, thereof {np.count_nonzero(predictions_1D_slap)} predicted positive\n",
    "2D reactions: {len(data.dataset_2D)}, thereof {np.count_nonzero(predictions_2D)} predicted positive\n",
    "\"\"\"\n",
    "\n",
    "with open(raw_dir / f\"{filename_base}_reaction_prediction.log\", \"w\") as file:\n",
    "    file.write(log_output)\n",
    "if verbose:\n",
    "    print(log_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write df to output file\n",
    "df.to_csv(raw_dir / f\"{filename_base}_reaction_prediction.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
